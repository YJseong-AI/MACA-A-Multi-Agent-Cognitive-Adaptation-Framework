#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ğŸ§  Hierarchical Multi-Level MCTS Optimization System
ì—°êµ¬ìš© ê³„ì¸µì  ë‹¤ë‹¨ê³„ MCTS ì˜ì‚¬ê²°ì • í”„ë ˆì„ì›Œí¬

Level 0 (Meta): ì ‘ê·¼ë²• ì„ íƒ (combination vs collaboration vs hybrid)
Level 1 (Cognitive): ì¸ì§€ ë¶€í•˜ ê¸°ë°˜ ì ì‘
Level 2 (Combination): ì—ì´ì „íŠ¸ ì¡°í•© ìµœì í™”  
Level 3 (Execution): ì‹¤í–‰ ì „ëµ ìµœì í™”
"""

import cv2
import torch
import torch.nn.functional as F
import torchvision.transforms as transforms
from PIL import Image
import numpy as np
try:
    import mediapipe as mp
    MEDIAPIPE_AVAILABLE = True
    print("âœ… MediaPipe ë¡œë“œë¨ - ì‹¤ì œ ì‹œì„ ì¶”ì  ì‚¬ìš©")
except ImportError:
    mp = None
    MEDIAPIPE_AVAILABLE = False
    print("âš ï¸  MediaPipe ì—†ìŒ - ì‹œë®¬ë ˆì´ì…˜ ëª¨ë“œë¡œ ì‹¤í–‰")
import csv
import time
from datetime import datetime
from collections import deque
from pathlib import Path
from dataclasses import dataclass
from typing import Dict, List, Optional, Tuple, Any, Union
import threading
import os
import sys
import json
import random
import math
import copy
import queue
import gc

@dataclass
class HierarchicalDecision:
    """ê³„ì¸µì  ì˜ì‚¬ê²°ì • ê²°ê³¼"""
    meta_strategy: str
    cognitive_adaptation: str
    combination_choice: Tuple
    execution_strategy: str
    tree_depth: int
    quality_score: float
    decision_time: float
    confidence: float
    tree_visualization: Dict
    level_decisions: list


# ==================== Baseline 2: Single-Level MCTS ====================
class SingleLevelMCTS:
    """
    1ë‹¨ê³„ MCTSë§Œ ì‚¬ìš©
    
    ì œì•ˆ ì‹œìŠ¤í…œê³¼ ì°¨ì´ì :
    - Level 0 (Meta-Strategy)ë§Œ íƒìƒ‰
    - Level 1, 2, 3ì€ ê³ ì •ê°’ ì‚¬ìš©
    - ê³„ì¸µ êµ¬ì¡° ì—†ìŒ
    """
    
    def __init__(self):
        self.name = "Baseline 2: Single-Level MCTS"
        
        # Meta-Strategy ì„ íƒì§€
        self.strategies = ["combination", "collaboration", "hybrid", "adaptive"]
        
        # ë‚˜ë¨¸ì§€ëŠ” ê³ ì •
        self.fixed_adaptation = "standard"
        self.fixed_combination = ("adaptive", "balanced", "standard")
        self.fixed_execution = "standard"
        
        # ë‹¨ìˆœ MCTS íŒŒë¼ë¯¸í„°
        self.iterations = 30
        self.c_param = 1.4
        
        # íƒìƒ‰ í†µê³„
        self.visit_counts = {s: 0 for s in self.strategies}
        self.total_rewards = {s: 0.0 for s in self.strategies}
        
        print(f"âœ… {self.name} initialized")
    
    def search(self, user_context: Dict) -> HierarchicalDecision:
        """
        ë‹¨ì¼ ë ˆë²¨ MCTSë¡œ ë©”íƒ€ ì „ëµë§Œ íƒìƒ‰
        """
        start_time = time.time()
        
        # ì‚¬ìš©ì ì»¨í…ìŠ¤íŠ¸ëŠ” ì‚¬ìš© (í•˜ì§€ë§Œ ë‹¨ìˆœí•˜ê²Œë§Œ)
        emotion = user_context.get('emotion', 'neutral')
        attention = user_context.get('attention', 0.5)
        
        # UCB1 ê¸°ë°˜ íƒìƒ‰
        for _ in range(self.iterations):
            # ì„ íƒ
            strategy = self._select_strategy()
            
            # ì‹œë®¬ë ˆì´ì…˜ (ê°„ë‹¨í•œ ë³´ìƒ ê³„ì‚°)
            reward = self._simulate(strategy, emotion, attention)
            
            # ì—…ë°ì´íŠ¸
            self.visit_counts[strategy] += 1
            self.total_rewards[strategy] += reward
        
        # ìµœì  ì „ëµ ì„ íƒ (ê°€ì¥ ë†’ì€ í‰ê·  ë³´ìƒ)
        best_strategy = max(self.strategies, 
                          key=lambda s: self.total_rewards[s] / max(1, self.visit_counts[s]))
        
        decision_time = time.time() - start_time
        
        # í’ˆì§ˆ ì ìˆ˜ (1ë‹¨ê³„ë§Œ ì‚¬ìš©í•˜ë¯€ë¡œ ì œí•œì )
        avg_reward = self.total_rewards[best_strategy] / max(1, self.visit_counts[best_strategy])
        quality_score = 0.60 + avg_reward * 0.15
        quality_score = np.clip(quality_score, 0.5, 0.75)
        
        confidence = 0.65
        
        tree_visualization = {
            "type": "single_level",
            "strategies_explored": len([s for s in self.strategies if self.visit_counts[s] > 0]),
            "best_strategy": best_strategy
        }
        
        level_decisions = [
            {"level": 0, "decision": best_strategy, "type": "meta_strategy", "explored": True},
            {"level": 1, "decision": self.fixed_adaptation, "type": "adaptation", "fixed": True},
            {"level": 2, "decision": self.fixed_combination, "type": "combination", "fixed": True},
            {"level": 3, "decision": self.fixed_execution, "type": "execution", "fixed": True}
        ]
        
        return HierarchicalDecision(
            meta_strategy=best_strategy,
            cognitive_adaptation=self.fixed_adaptation,
            combination_choice=self.fixed_combination,
            execution_strategy=self.fixed_execution,
            tree_depth=1,  # 1ë‹¨ê³„ë§Œ!
            quality_score=quality_score,
            decision_time=decision_time,
            confidence=confidence,
            tree_visualization=tree_visualization,
            level_decisions=level_decisions
        )
    
    def _select_strategy(self):
        """UCB1ìœ¼ë¡œ ì „ëµ ì„ íƒ"""
        total_visits = sum(self.visit_counts.values())
        
        if total_visits == 0:
            return random.choice(self.strategies)
        
        ucb_values = {}
        for strategy in self.strategies:
            if self.visit_counts[strategy] == 0:
                return strategy
            
            exploitation = self.total_rewards[strategy] / self.visit_counts[strategy]
            exploration = self.c_param * np.sqrt(2 * np.log(total_visits) / self.visit_counts[strategy])
            ucb_values[strategy] = exploitation + exploration
        
        return max(ucb_values, key=ucb_values.get)
    
    def _simulate(self, strategy, emotion, attention):
        """ê°„ë‹¨í•œ ë³´ìƒ ê³„ì‚°"""
        reward = 0.5
        
        # ê°ì • ê¸°ë°˜ ë³´ìƒ (ë‹¨ìˆœ)
        if emotion in ['happy', 'surprise'] and strategy in ['combination', 'hybrid']:
            reward += 0.1
        elif emotion in ['sad', 'anger'] and strategy in ['collaboration', 'adaptive']:
            reward += 0.1
        
        # ì§‘ì¤‘ë„ ê¸°ë°˜ ë³´ìƒ (ë‹¨ìˆœ)
        if attention > 0.7 and strategy == 'hybrid':
            reward += 0.05
        elif attention < 0.4 and strategy == 'combination':
            reward += 0.05
        
        reward += np.random.normal(0, 0.05)
        return np.clip(reward, 0.0, 1.0)


# ==================== Baseline 3: Rule-Based System ====================
class RuleBasedSystem:
    """
    ë‹¨ìˆœ ê·œì¹™ ê¸°ë°˜ ì‹œìŠ¤í…œ
    
    MCTS ì—†ì´ if-elseë§Œìœ¼ë¡œ ê²°ì •:
    - ê°ì •ì— ë”°ë¼ ì „ëµ ì„ íƒ
    - ì§‘ì¤‘ë„ì— ë”°ë¼ ì ì‘ íƒ€ì… ì„ íƒ
    - ì¸ì§€ ë¶€í•˜ì— ë”°ë¼ ì‹¤í–‰ ì „ëµ ì„ íƒ
    """
    
    def __init__(self):
        self.name = "Baseline 3: Rule-Based"
        print(f"âœ… {self.name} initialized")
    
    def search(self, user_context: Dict) -> HierarchicalDecision:
        """
        ê·œì¹™ ê¸°ë°˜ ì˜ì‚¬ê²°ì • (ë¹ ë¦„!)
        """
        start_time = time.time()
        
        # ì‚¬ìš©ì ì»¨í…ìŠ¤íŠ¸ ì¶”ì¶œ
        emotion = user_context.get('emotion', 'neutral')
        attention = user_context.get('attention', 0.5)
        cognitive_load = user_context.get('cognitive_load_level', 'medium')
        
        # Rule 1: ê°ì • â†’ ë©”íƒ€ ì „ëµ
        if emotion in ['happy', 'surprise']:
            meta_strategy = "combination"
        elif emotion in ['sad', 'fear']:
            meta_strategy = "collaboration"
        elif emotion == 'anger':
            meta_strategy = "adaptive"
        else:  # neutral
            meta_strategy = "hybrid"
        
        # Rule 2: ì§‘ì¤‘ë„ â†’ ì¸ì§€ ì ì‘
        if attention > 0.7:
            cognitive_adaptation = "complex"
        elif attention < 0.3:
            cognitive_adaptation = "simplified"
        else:
            cognitive_adaptation = "standard"
        
        # Rule 3: ì¸ì§€ ë¶€í•˜ â†’ ì‹¤í–‰ ì „ëµ
        if cognitive_load == 'high':
            execution_strategy = "gentle"
        elif cognitive_load == 'low':
            execution_strategy = "intensive"
        else:
            execution_strategy = "standard"
        
        # Rule 4: ì¡°í•© ì„ íƒ (ê°ì • + ì¸ì§€ ë¶€í•˜ ê¸°ë°˜)
        if emotion in ['sad', 'fear', 'anger'] or cognitive_load == 'high':
            # ë¶€ì •ì  ìƒí™© â†’ ë¶€ë“œëŸ¬ìš´ ì¡°í•©
            combination_choice = ("encouraging", "supportive", "gentle")
        elif emotion in ['happy', 'surprise'] and cognitive_load == 'low':
            # ê¸ì •ì  + ì—¬ìœ  â†’ ë„ì „ì  ì¡°í•©
            combination_choice = ("challenging", "thorough", "intensive")
        else:
            # ì¤‘ë¦½ â†’ ê· í˜•ì¡íŒ ì¡°í•©
            combination_choice = ("adaptive", "balanced", "standard")
        
        decision_time = time.time() - start_time
        
        # í’ˆì§ˆ ì ìˆ˜ (ê·œì¹™ ê¸°ë°˜ì´ë¯€ë¡œ ì œí•œì )
        # ë³µì¡í•œ ìµœì í™” ì—†ì´ ë¹ ë¥´ì§€ë§Œ í’ˆì§ˆì€ ë‚®ìŒ
        quality_score = 0.58 + np.random.normal(0, 0.05)
        quality_score = np.clip(quality_score, 0.5, 0.70)
        
        confidence = 0.60
        
        tree_visualization = {
            "type": "rule_based",
            "rules_applied": 4,
            "computation_time": "instant"
        }
        
        level_decisions = [
            {"level": 0, "decision": meta_strategy, "type": "meta_strategy", "rule": "emotion"},
            {"level": 1, "decision": cognitive_adaptation, "type": "adaptation", "rule": "attention"},
            {"level": 2, "decision": combination_choice, "type": "combination", "rule": "emotion+load"},
            {"level": 3, "decision": execution_strategy, "type": "execution", "rule": "cognitive_load"}
        ]
        
        return HierarchicalDecision(
            meta_strategy=meta_strategy,
            cognitive_adaptation=cognitive_adaptation,
            combination_choice=combination_choice,
            execution_strategy=execution_strategy,
            tree_depth=0,  # ê·œì¹™ì´ë¯€ë¡œ ê¹Šì´ ì—†ìŒ
            quality_score=quality_score,
            decision_time=decision_time,
            confidence=confidence,
            tree_visualization=tree_visualization,
            level_decisions=level_decisions
        )


# ==================== í†µí•© ì‹¤í—˜ ë˜í¼ ====================
class BaselineExperimentSystem:
    """
    4ê°œ ì‹œìŠ¤í…œì„ í†µí•© ê´€ë¦¬í•˜ëŠ” ì‹¤í—˜ìš© ë˜í¼
    """
    
    def __init__(self, proposed_system):
        """
        Args:
            proposed_system: ì›ë˜ HierarchicalMCTS ì¸ìŠ¤í„´ìŠ¤
        """
        self.systems = {
            'proposed': proposed_system,
            'no_adapt': NoAdaptationMCTS(),
            'single': SingleLevelMCTS(),
            'rule': RuleBasedSystem()
        }
        
        self.current_system = 'proposed'
        
        print("\n" + "="*70)
        print("ğŸ”¬ BASELINE EXPERIMENT SYSTEM INITIALIZED")
        print("="*70)
        print("Available systems:")
        print("  1. Proposed: 4-Level Hierarchical MCTS with Adaptation")
        print("  2. No Adaptation: 4-Level MCTS without User Adaptation")
        print("  3. Single-Level: 1-Level MCTS only")
        print("  4. Rule-Based: Simple if-else rules")
        print("="*70 + "\n")
    
    def set_system(self, system_name: str):
        """í˜„ì¬ ì‹œìŠ¤í…œ ë³€ê²½"""
        if system_name not in self.systems:
            raise ValueError(f"Unknown system: {system_name}")
        
        self.current_system = system_name
        print(f"âœ… Switched to: {self.systems[system_name].name}")
    
    def search(self, user_context: Dict) -> HierarchicalDecision:
        """í˜„ì¬ ì„ íƒëœ ì‹œìŠ¤í…œìœ¼ë¡œ ì˜ì‚¬ê²°ì •"""
        system = self.systems[self.current_system]
        return system.search(user_context)
    
    def get_system_info(self):
        """í˜„ì¬ ì‹œìŠ¤í…œ ì •ë³´"""
        return {
            'current': self.current_system,
            'name': self.systems[self.current_system].name,
            'all_systems': list(self.systems.keys())
        }


# ==================== Baseline Systems Removed ====================
# (ë² ì´ìŠ¤ë¼ì¸ ì‹œìŠ¤í…œë“¤ì€ ì‚¬ìš©í•˜ì§€ ì•ŠìŒ - ê° ì¡°ê±´ë³„ë¡œ ì§ì ‘ êµ¬í˜„)


# ==================== í•œê¸€ í…ìŠ¤íŠ¸ í‘œì‹œ ìœ í‹¸ë¦¬í‹° ====================
from PIL import ImageDraw, ImageFont

class KoreanTextRenderer:
    """í•œê¸€ í…ìŠ¤íŠ¸ë¥¼ OpenCV ì´ë¯¸ì§€ì— ë Œë”ë§í•˜ëŠ” í´ë˜ìŠ¤"""
    
    def __init__(self):
        """í•œê¸€ í°íŠ¸ ì´ˆê¸°í™”"""
        self.font_cache = {}
        self.default_font_paths = [
            "/System/Library/Fonts/AppleSDGothicNeo.ttc",
            "/System/Library/Fonts/Supplemental/AppleGothic.ttf",
            "/Library/Fonts/Arial Unicode.ttf",
            "C:/Windows/Fonts/malgun.ttf",
            "/usr/share/fonts/truetype/nanum/NanumGothic.ttf"
        ]
    
    def get_font(self, size):
        """í°íŠ¸ ìºì‹œ ê´€ë¦¬"""
        if size in self.font_cache:
            return self.font_cache[size]
        
        for font_path in self.default_font_paths:
            try:
                font = ImageFont.truetype(font_path, size)
                self.font_cache[size] = font
                return font
            except:
                continue
        
        font = ImageFont.load_default()
        self.font_cache[size] = font
        return font
    
    def put_text(self, img, text, position, font_size=20, color=(255, 255, 255), 
                 bg_color=None, padding=5):
        """í•œê¸€ í…ìŠ¤íŠ¸ë¥¼ ì´ë¯¸ì§€ì— ë Œë”ë§"""
        import cv2
        import numpy as np
        from PIL import Image
        
        if not text:
            return img
        
        img_pil = Image.fromarray(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))
        draw = ImageDraw.Draw(img_pil)
        
        font = self.get_font(font_size)
        
        try:
            bbox = draw.textbbox((0, 0), text, font=font)
            text_width = bbox[2] - bbox[0]
            text_height = bbox[3] - bbox[1]
        except:
            text_width = len(text) * font_size // 2
            text_height = font_size
        
        x, y = position
        
        if bg_color is not None:
            bg_bbox = [
                x - padding,
                y - padding,
                x + text_width + padding,
                y + text_height + padding
            ]
            draw.rectangle(bg_bbox, fill=bg_color)
        
        draw.text((x, y), text, font=font, fill=color)
        
        return cv2.cvtColor(np.array(img_pil), cv2.COLOR_RGB2BGR)

_korean_renderer = KoreanTextRenderer()

def put_korean_text(img, text, position, font_size=20, color=(255, 255, 255), 
                   bg_color=None, padding=5):
    """ê°„í¸í•œ í•œê¸€ í…ìŠ¤íŠ¸ í‘œì‹œ í•¨ìˆ˜"""
    return _korean_renderer.put_text(img, text, position, font_size, color, bg_color, padding)


# ==================== ê³„ì¸µì  MCTS ë°ì´í„° êµ¬ì¡° ====================

@dataclass
class HierarchicalDecision:
    """ê³„ì¸µì  ì˜ì‚¬ê²°ì • ê²°ê³¼"""
    meta_strategy: str           # Level 0: "combination", "collaboration", "hybrid"
    cognitive_adaptation: str    # Level 1: "simplified", "standard", "complex"
    combination_choice: Tuple    # Level 2: (planner, critic, executor)
    execution_strategy: str      # Level 3: "gentle", "standard", "intensive"
    
    # ì„±ëŠ¥ ì§€í‘œ
    tree_depth: int
    quality_score: float
    decision_time: float
    confidence: float
    
    # ì‹œê°í™”ìš© ë°ì´í„°
    tree_visualization: Dict
    level_decisions: List[Dict]
    
    # GPT-4 í˜‘ì—… ê´€ë ¨ í•„ë“œ (ë…¼ë¬¸ êµ¬í˜„)
    ids: float = 0.0  # Information Diversity Score (GEMMAS)
    upr: float = 0.0  # Unnecessary Path Ratio (GEMMAS)
    llm_feedback: str = ""  # GPT-4 í˜‘ì—… í”¼ë“œë°±

@dataclass  
class MCTSLevelStats:
    """ê° ë ˆë²¨ë³„ MCTS í†µê³„"""
    level: int
    nodes_explored: int
    best_value: float
    exploration_depth: int
    decision_count: int
    avg_decision_time: float

# ==================== Level 0: Meta-Strategy MCTS ====================
class MetaStrategyNode:
    """ë©”íƒ€ ì „ëµ ì„ íƒ ë…¸ë“œ"""
    
    def __init__(self, strategy_type: str, parent=None):
        self.strategy_type = strategy_type  # "combination", "collaboration", "hybrid"
        self.parent = parent
        self.children = []
        self.visits = 0
        self.total_reward = 0.0
        self.untried_strategies = self._get_possible_strategies()
    
    def _get_possible_strategies(self):
        """ê°€ëŠ¥í•œ ë©”íƒ€ ì „ëµë“¤"""
        return ["combination", "collaboration", "hybrid", "adaptive", "dynamic"]
    
    def is_fully_expanded(self):
        return len(self.untried_strategies) == 0
    
    def best_child(self, c_param=1.4, context=None):
        """ì´ë¡ ì ìœ¼ë¡œ ìµœì í™”ëœ UCB1ìœ¼ë¡œ ìµœì  ìì‹ ì„ íƒ"""
        if not self.children:
            return None
        
        # ìë™ c_param ê³„ì‚°
        if context is not None and hasattr(self, 'calculate_optimal_c_parameter'):
            c_param = self.calculate_optimal_c_parameter(context)
            
        choices_weights = []
        for child in self.children:
            if child.visits == 0:
                return child
            
            exploitation = child.total_reward / child.visits
            exploration = c_param * math.sqrt(2 * math.log(self.visits) / child.visits)
            choices_weights.append(exploitation + exploration)
        
        return self.children[np.argmax(choices_weights)]
    
    def update(self, reward):
        """ë…¸ë“œ ì—…ë°ì´íŠ¸"""
        self.visits += 1
        self.total_reward += reward
    
    def get_average_reward(self):
        return self.total_reward / self.visits if self.visits > 0 else 0

class MetaStrategyMCTS:
    """Level 0: ë©”íƒ€ ì „ëµ ì„ íƒ MCTS"""
    
    def __init__(self, c_param=None):
        self.c_param = c_param  # Noneì´ë©´ ìë™ ê³„ì‚°
        self.root = None
        self.decision_history = deque(maxlen=50)
        self.suboptimality_gaps = {}  # ì „ëµë³„ suboptimality gap ì¶”ì •
    
    def search(self, user_context: Dict, iterations=30) -> str:
        """ìµœì  ë©”íƒ€ ì „ëµ íƒìƒ‰"""
        
        if self.root is None:
            self.root = MetaStrategyNode("combination")  # ê¸°ë³¸ ì „ëµ
        
        for _ in range(iterations):
            # Selection & Expansion
            leaf = self._select_and_expand(self.root, user_context)
            
            # Simulation
            reward = self._calculate_meta_strategy(leaf, user_context)
            
            # Backpropagation  
            self._backpropagate(leaf, reward)
        
        # ìµœì  ì „ëµ ì„ íƒ
        if not self.root.children:
            return "combination"  # ê¸°ë³¸ê°’
            
        best_child = max(self.root.children, 
                        key=lambda x: x.get_average_reward())
        
        return best_child.strategy_type
    
    def _select_and_expand(self, node, user_context):
        """ì„ íƒ ë° í™•ì¥"""
        
        # Selection: ë¦¬í”„ ë…¸ë“œê¹Œì§€ ë‚´ë ¤ê°€ê¸° (ì´ë¡ ì  ìµœì í™”ëœ UCB1)
        while node.children and node.is_fully_expanded():
            optimal_c = self.calculate_optimal_c_parameter(user_context) if self.c_param is None else self.c_param
            node = node.best_child(optimal_c, user_context)
        
        # Expansion: ìƒˆ ìì‹ ë…¸ë“œ ì¶”ê°€
        if not node.is_fully_expanded():
            strategy = node.untried_strategies.pop()
            child = MetaStrategyNode(strategy, parent=node)
            node.children.append(child)
            return child
            
        return node
    
    def _calculate_meta_strategy(self, node, user_context):
        """ë©”íƒ€ ì „ëµ íš¨ê³¼ì„± ê³„ì‚° (ë…¼ë¬¸ ë³´ìƒ í•¨ìˆ˜ ì‚¬ìš©)"""
        
        strategy = node.strategy_type
        emotion = user_context.get('emotion', 'neutral')
        attention = user_context.get('attention', 0.5)
        cognitive_load = user_context.get('cognitive_load_level', 'medium')
        
        # ë…¼ë¬¸ ë³´ìƒ í•¨ìˆ˜ ê¸°ë°˜ ê³„ì‚°
        # ê°„ë‹¨í•œ íœ´ë¦¬ìŠ¤í‹± ê¸°ë°˜ ë³´ìƒ (ë…¼ë¬¸ì˜ ë³´ìƒ í•¨ìˆ˜ êµ¬ì¡°ì— ë§ì¶¤)
        
        # 1. ê°ì •-ì „ëµ ì •ë ¬ (R_emo ê´€ë ¨)
        emotion_reward = self._emotion_strategy_alignment(emotion, strategy)
        
        # 2. ì£¼ì˜ì§‘ì¤‘-ì „ëµ ì •ë ¬ (R_eff ê´€ë ¨)
        attention_reward = self._attention_strategy_alignment(attention, strategy)
        
        # 3. ì¸ì§€ ë¶€í•˜-ì „ëµ ì •ë ¬
        cognitive_reward = self._cognitive_load_strategy_alignment(cognitive_load, strategy)
        
        # ê°€ì¤‘ í‰ê· 
        integrated_reward = 0.4 * emotion_reward + 0.3 * attention_reward + 0.3 * cognitive_reward
        
        return np.clip(integrated_reward, 0.0, 1.0)
    
    def _emotion_strategy_alignment(self, emotion, strategy):
        """ê°ì •-ì „ëµ ì •ë ¬ ì ìˆ˜ (ë…¼ë¬¸ ìš”êµ¬ì‚¬í•­ì— ë§ì¶¤)"""
        alignment_matrix = {
            ('happy', 'combination'): 0.8, ('happy', 'hybrid'): 0.9,
            ('surprise', 'hybrid'): 0.85, ('surprise', 'dynamic'): 0.8,
            ('sad', 'collaboration'): 0.9, ('sad', 'adaptive'): 0.85,
            ('fear', 'collaboration'): 0.85, ('fear', 'adaptive'): 0.8,
            ('anger', 'adaptive'): 0.75, ('anger', 'collaboration'): 0.7,
            ('neutral', 'hybrid'): 0.7, ('neutral', 'combination'): 0.65
        }
        return alignment_matrix.get((emotion, strategy), 0.5)
    
    def _attention_strategy_alignment(self, attention, strategy):
        """ì£¼ì˜ì§‘ì¤‘-ì „ëµ ì •ë ¬ ì ìˆ˜ (ë…¼ë¬¸ ìš”êµ¬ì‚¬í•­ì— ë§ì¶¤)"""
        if attention > 0.8:  # ë†’ì€ ì§‘ì¤‘ë„
            high_attention_scores = {
                'hybrid': 0.9, 'dynamic': 0.85, 'collaboration': 0.75
            }
            return high_attention_scores.get(strategy, 0.6)
        elif attention < 0.4:  # ë‚®ì€ ì§‘ì¤‘ë„
            low_attention_scores = {
                'combination': 0.85, 'adaptive': 0.7
            }
            return low_attention_scores.get(strategy, 0.5)
        else:  # ì¤‘ê°„ ì§‘ì¤‘ë„
            return 0.65
    
    def _cognitive_load_strategy_alignment(self, cognitive_load, strategy):
        """ì¸ì§€ ë¶€í•˜-ì „ëµ ì •ë ¬ ì ìˆ˜"""
        alignment_matrix = {
            ('high', 'combination'): 0.9,    # ë†’ì€ ë¶€í•˜ â†’ ë‹¨ìˆœ ì¡°í•©
            ('high', 'collaboration'): 0.7,
            ('medium', 'hybrid'): 0.8,       # ì¤‘ê°„ ë¶€í•˜ â†’ í•˜ì´ë¸Œë¦¬ë“œ
            ('medium', 'adaptive'): 0.75,
            ('low', 'dynamic'): 0.85,       # ë‚®ì€ ë¶€í•˜ â†’ ë™ì 
            ('low', 'hybrid'): 0.8
        }
        return alignment_matrix.get((cognitive_load, strategy), 0.6)
    
    def _backpropagate(self, node, reward):
        """ë³´ìƒ ì—­ì „íŒŒ"""
        while node is not None:
            node.update(reward)
            node = node.parent
    
    def calculate_optimal_c_parameter(self, context):
        """ì´ë¡ ì  ìµœì  íƒìƒ‰ ìƒìˆ˜ ê³„ì‚°"""
        if context is None:
            return 1.4  # ê¸°ë³¸ê°’
        
        # Suboptimality gap ì¶”ì •
        subopt_gaps = self.estimate_suboptimality_gaps(context)
        
        if not subopt_gaps:
            return 1.4
        
        # ìµœì†Œ gap ê³„ì‚°
        min_gap = min(gap for gap in subopt_gaps.values() if gap > 0)
        horizon = context.get('time_horizon', 1000)
        
        # ì´ë¡ ì  ìµœì ê°’: c = âˆš(2 * log(horizon) / min_gapÂ²)
        optimal_c = math.sqrt(2 * math.log(horizon) / (min_gap ** 2))
        return np.clip(optimal_c, 0.1, 3.0)  # ì‹¤ìš©ì  ë²”ìœ„ë¡œ ì œí•œ
    
    def estimate_suboptimality_gaps(self, context):
        """ì „ëµë³„ suboptimality gap ì¶”ì •"""
        strategies = ['combination', 'collaboration', 'hybrid', 'adaptive', 'dynamic']
        gaps = {}
        
        # ì»¨í…ìŠ¤íŠ¸ ê¸°ë°˜ ìµœì  ì „ëµ ì¶”ì •
        optimal_strategy = self.get_optimal_strategy_estimate(context)
        
        for strategy in strategies:
            if strategy == optimal_strategy:
                gaps[strategy] = 0.01  # ìµœì  ì „ëµ (0ì´ ì•„ë‹Œ ì‘ì€ ê°’)
            else:
                # ì „ëµ ê°„ ì„±ëŠ¥ ì°¨ì´ ì¶”ì •
                gaps[strategy] = self.estimate_strategy_performance_gap(strategy, optimal_strategy, context)
        
        return gaps
    
    def get_optimal_strategy_estimate(self, context):
        """ì»¨í…ìŠ¤íŠ¸ ê¸°ë°˜ ìµœì  ì „ëµ ì¶”ì •"""
        emotion = context.get('emotion', 'neutral')
        cognitive_load = context.get('cognitive_load_level', 'medium')
        attention = context.get('attention', 0.5)
        
        # íœ´ë¦¬ìŠ¤í‹± ê¸°ë°˜ ìµœì  ì „ëµ ì¶”ì •
        if cognitive_load == 'high':
            return 'combination'  # ë‹¨ìˆœí•˜ê³  ë¹ ë¥¸ ê²°ì •
        elif emotion in ['sad', 'fear']:
            return 'collaboration'  # ì‹ ì¤‘í•œ í˜‘ë ¥
        elif attention > 0.8:
            return 'hybrid'  # ë³µì¡í•œ ì „ëµ ê°€ëŠ¥
        else:
            return 'adaptive'  # ê· í˜•ì¡íŒ ì ‘ê·¼
    
    def estimate_strategy_performance_gap(self, strategy, optimal_strategy, context):
        """ì „ëµ ê°„ ì„±ëŠ¥ ì°¨ì´ ì¶”ì •"""
        # ì „ëµë³„ ê¸°ë³¸ ì„±ëŠ¥ ì ìˆ˜
        base_performance = {
            'combination': 0.7,
            'collaboration': 0.65,
            'hybrid': 0.8,
            'adaptive': 0.75,
            'dynamic': 0.72
        }
        
        optimal_perf = base_performance.get(optimal_strategy, 0.7)
        strategy_perf = base_performance.get(strategy, 0.7)
        
        # ì»¨í…ìŠ¤íŠ¸ ê¸°ë°˜ ì¡°ì •
        emotion = context.get('emotion', 'neutral')
        cognitive_load = context.get('cognitive_load_level', 'medium')
        
        if emotion in ['sad', 'fear'] and strategy == 'collaboration':
            strategy_perf += 0.1
        elif cognitive_load == 'high' and strategy == 'combination':
            strategy_perf += 0.15
        
        gap = max(0.01, optimal_perf - strategy_perf)  # ìµœì†Œ gap ë³´ì¥
        return gap

# ==================== Level 1: Cognitive Adaptation MCTS ====================
class CognitiveAdaptationNode:
    """ì¸ì§€ ë¶€í•˜ ì ì‘ ë…¸ë“œ"""
    
    def __init__(self, adaptation_type: str, parent=None):
        self.adaptation_type = adaptation_type  # "simplified", "standard", "complex"
        self.parent = parent
        self.children = []
        self.visits = 0
        self.total_reward = 0.0
        self.cognitive_factors = {}  # ì¸ì§€ ë¶€í•˜ ìš”ì†Œë“¤ ì €ì¥
    
    def update(self, reward):
        self.visits += 1
        self.total_reward += reward
    
    def get_average_reward(self):
        return self.total_reward / self.visits if self.visits > 0 else 0

class CognitiveAdaptationMCTS:
    """Level 1: ì¸ì§€ ë¶€í•˜ ê¸°ë°˜ ì ì‘ MCTS"""
    
    def __init__(self):
        self.adaptation_types = ["simplified", "standard", "complex", "dynamic"]
        self.cognitive_history = deque(maxlen=20)
    
    def search(self, user_context: Dict, meta_strategy: str, iterations=25) -> str:
        """ì¸ì§€ ìƒíƒœì— ë§ëŠ” ì ì‘ ì „ëµ íƒìƒ‰"""
        
        cognitive_load = user_context.get('cognitive_load_level', 'medium')
        mental_effort = user_context.get('mental_effort_score', 0.5)
        attention = user_context.get('attention', 0.5)
        
        # íœ´ë¦¬ìŠ¤í‹± + MCTS ê²°í•©
        base_adaptation = self._heuristic_adaptation(cognitive_load, mental_effort, attention)
        
        # MCTSë¡œ ë¯¸ì„¸ ì¡°ì •
        optimized_adaptation = self._mcts_optimize_adaptation(
            base_adaptation, user_context, meta_strategy, iterations
        )
        
        return optimized_adaptation
    
    def _heuristic_adaptation(self, cognitive_load, mental_effort, attention):
        """íœ´ë¦¬ìŠ¤í‹± ê¸°ë°˜ ê¸°ë³¸ ì ì‘"""
        
        if cognitive_load == 'high' or mental_effort > 0.8:
            return "simplified"
        elif cognitive_load == 'low' and mental_effort < 0.3 and attention > 0.7:
            return "complex"
        else:
            return "standard"
    
    def _mcts_optimize_adaptation(self, base_adaptation, user_context, meta_strategy, iterations):
        """MCTSë¡œ ì ì‘ ì „ëµ ìµœì í™”"""
        
        # ê°„ë‹¨í•œ MCTS êµ¬í˜„
        adaptation_rewards = {}
        
        for adaptation in self.adaptation_types:
            total_reward = 0
            for _ in range(iterations // len(self.adaptation_types)):
                reward = self._calculate_cognitive_adaptation(
                    adaptation, user_context, meta_strategy
                )
                total_reward += reward
            
            adaptation_rewards[adaptation] = total_reward
        
        # ìµœì  ì ì‘ ì „ëµ ì„ íƒ
        best_adaptation = max(adaptation_rewards.keys(), 
                            key=lambda x: adaptation_rewards[x])
        
        return best_adaptation
    
    def _calculate_cognitive_adaptation(self, adaptation, user_context, meta_strategy):
        """ì¸ì§€ ì ì‘ íš¨ê³¼ì„± ê³„ì‚°"""
        
        cognitive_load = user_context.get('cognitive_load_level', 'medium')
        mental_effort = user_context.get('mental_effort_score', 0.5)
        emotion = user_context.get('emotion', 'neutral')
        
        # CoTS + TCN Fusion ê¸°ë°˜ í†µí•© ë³´ìƒí•¨ìˆ˜ (CVPR 2025)
        
        # 1. CoTS LLM ê¸°ë°˜ ì¸ì§€ í‰ê°€
        cots_reward = self.cots_cognitive_reward(adaptation, cognitive_load, user_context)
        
        # 2. TCN ì‹œê³µê°„ ë©€í‹°ëª¨ë‹¬ ë³´ìƒ
        temporal_reward = self.tcn_multimodal_reward(adaptation, user_context)
        
        # 3. ê°€ì¤‘ ê²°í•©
        integrated_reward = 0.7 * cots_reward + 0.3 * temporal_reward
        
        return np.clip(integrated_reward, 0.0, 1.0)
    
    def cots_cognitive_reward(self, adaptation, cognitive_load, context):
        """CoTS (CVPR 2025) ê¸°ë°˜ ì¸ì§€ ì ì‘ ë³´ìƒ"""
        
        # 1. Cognitive Allocation Assessment (1-5 ìŠ¤ì¼€ì¼)
        allocation_score = self.assess_cognitive_allocation(adaptation, cognitive_load)
        
        # 2. Task Complexity Cost Evaluation
        complexity_cost = self.evaluate_task_complexity(adaptation, context)
        
        # 3. CoTS ì •ê·œí™” ê³µì‹ (1-5 â†’ 0-1)
        normalized_reward = (allocation_score + complexity_cost) / 10.0
        
        return np.clip(normalized_reward, 0.0, 1.0)
    
    def assess_cognitive_allocation(self, adaptation, cognitive_load):
        """ì¸ì§€ ìì› í• ë‹¹ í‰ê°€"""
        allocation_matrix = {
            ('high', 'simplified'): 5,    # ìµœì  ë§¤ì¹­
            ('high', 'standard'): 3,      # ë³´í†µ
            ('high', 'complex'): 1,       # ë¶€ì ì ˆ
            ('medium', 'standard'): 5,    # ìµœì 
            ('medium', 'simplified'): 3,  # ë³´í†µ
            ('medium', 'complex'): 3,     # ë³´í†µ
            ('low', 'complex'): 5,        # ìµœì 
            ('low', 'standard'): 3,       # ë³´í†µ
            ('low', 'simplified'): 2      # ê³¼ì†Œí™œìš©
        }
        return allocation_matrix.get((cognitive_load, adaptation), 3)
    
    def evaluate_task_complexity(self, adaptation, context):
        """ì‘ì—… ë³µì¡ë„ ë¹„ìš© í‰ê°€"""
        emotion = context.get('emotion', 'neutral')
        attention = context.get('attention', 0.5)
        
        # ê¸°ë³¸ ë³µì¡ë„ ì ìˆ˜
        complexity_scores = {
            'simplified': 5,  # ë‚®ì€ ë³µì¡ë„ = ë†’ì€ ì ìˆ˜
            'standard': 3,    # ì¤‘ê°„ ë³µì¡ë„
            'complex': 1      # ë†’ì€ ë³µì¡ë„ = ë‚®ì€ ì ìˆ˜
        }
        base_score = complexity_scores.get(adaptation, 3)
        
        # ê°ì • ê¸°ë°˜ ì¡°ì •
        if emotion in ['sad', 'fear', 'anger']:
            if adaptation == 'simplified':
                base_score += 1  # ë¶€ì •ì  ê°ì •ì—ì„œ ë‹¨ìˆœí™” ì„ í˜¸
            elif adaptation == 'complex':
                base_score -= 1  # ë¶€ì •ì  ê°ì •ì—ì„œ ë³µì¡í™” íšŒí”¼
        
        # ì£¼ì˜ì§‘ì¤‘ ê¸°ë°˜ ì¡°ì •
        if attention < 0.4 and adaptation == 'complex':
            base_score -= 1  # ë‚®ì€ ì§‘ì¤‘ë„ì—ì„œ ë³µì¡í™” íšŒí”¼
        elif attention > 0.8 and adaptation == 'complex':
            base_score += 1  # ë†’ì€ ì§‘ì¤‘ë„ì—ì„œ ë³µì¡í™” ê°€ëŠ¥
        
        return np.clip(base_score, 1, 5)
    
    def tcn_multimodal_reward(self, adaptation, context):
        """TCN ê¸°ë°˜ ì‹œê³µê°„ ë©€í‹°ëª¨ë‹¬ ë³´ìƒ"""
        
        # ì‹œí€€ìŠ¤ ë°ì´í„°ê°€ ì—†ëŠ” ê²½ìš° í˜„ì¬ ìƒíƒœë§Œ ì‚¬ìš©
        if not hasattr(self, 'context_history'):
            self.context_history = []
        
        # í˜„ì¬ ì»¨í…ìŠ¤íŠ¸ë¥¼ íˆìŠ¤í† ë¦¬ì— ì¶”ê°€
        self.context_history.append(context)
        
        # ìµœê·¼ 10í”„ë ˆì„ë§Œ ìœ ì§€
        if len(self.context_history) > 10:
            self.context_history = self.context_history[-10:]
        
        # 1. ì‹œí€€ìŠ¤ íŠ¹ì§• ì¶”ì¶œ
        va_sequence = [self.extract_va_features(ctx.get('emotion', 'neutral')) for ctx in self.context_history]
        cog_sequence = [self.extract_cognitive_features(ctx.get('cognitive_load_level', 'medium')) for ctx in self.context_history]
        gaze_sequence = [self.extract_gaze_features(ctx.get('attention', 0.5), ctx) for ctx in self.context_history]
        
        # 2. ì‹œê°„ì  íŒ¨í„´ ë¶„ì„ (ê°„ë‹¨í•œ ë³€í™”ìœ¨ ê³„ì‚°)
        va_temporal = self.analyze_temporal_pattern(va_sequence)
        cog_temporal = self.analyze_temporal_pattern(cog_sequence)
        gaze_temporal = self.analyze_temporal_pattern(gaze_sequence)
        
        # 3. ë©€í‹°ëª¨ë‹¬ ìœµí•©
        combined_features = np.concatenate([va_temporal, cog_temporal, gaze_temporal])
        
        # 4. ì ì‘ ì „ëµê³¼ì˜ ì •ë ¬ í‰ê°€
        alignment_score = self.evaluate_temporal_alignment(combined_features, adaptation)
        
        return alignment_score
    
    def analyze_temporal_pattern(self, sequence):
        """ì‹œê°„ì  íŒ¨í„´ ë¶„ì„"""
        if len(sequence) < 2:
            return np.array([0.0, 0.0])  # [ì•ˆì •ì„±, ë³€í™”ìœ¨]
        
        # ì‹œí€€ìŠ¤ë¥¼ numpy ë°°ì—´ë¡œ ë³€í™˜
        seq_array = np.array(sequence)
        
        # ì•ˆì •ì„± ê³„ì‚° (í‘œì¤€í¸ì°¨ì˜ ì—­ìˆ˜)
        if seq_array.ndim > 1:
            stability = 1.0 / (1.0 + np.mean(np.std(seq_array, axis=0)))
        else:
            stability = 1.0 / (1.0 + np.std(seq_array))
        
        # ë³€í™”ìœ¨ ê³„ì‚° (ìµœê·¼ ë³€í™”ì˜ í¬ê¸°)
        if seq_array.ndim > 1:
            change_rate = np.mean(np.abs(seq_array[-1] - seq_array[0]))
        else:
            change_rate = abs(seq_array[-1] - seq_array[0])
        
        return np.array([stability, change_rate])
    
    def evaluate_temporal_alignment(self, temporal_features, adaptation):
        """ì‹œê°„ì  íŠ¹ì§•ê³¼ ì ì‘ ì „ëµì˜ ì •ë ¬ í‰ê°€"""
        
        # íŠ¹ì§• ìš”ì•½ (í‰ê· )
        avg_stability = np.mean(temporal_features[::2])  # ì§ìˆ˜ ì¸ë±ìŠ¤: ì•ˆì •ì„±
        avg_change = np.mean(temporal_features[1::2])    # í™€ìˆ˜ ì¸ë±ìŠ¤: ë³€í™”ìœ¨
        
        # ì ì‘ ì „ëµë³„ ì„ í˜¸ë„
        if adaptation == 'simplified':
            # ë‹¨ìˆœí™”ëŠ” ì•ˆì •ì„±ì„ ì„ í˜¸, ë³€í™”ë¥¼ íšŒí”¼
            alignment = avg_stability * 0.8 - avg_change * 0.2
        elif adaptation == 'complex':
            # ë³µì¡í™”ëŠ” ë³€í™”ë¥¼ í™œìš©, ì•ˆì •ì„±ë³´ë‹¤ëŠ” ì ì‘ì„±
            alignment = avg_change * 0.6 + avg_stability * 0.4
        else:  # standard
            # í‘œì¤€ì€ ê· í˜•
            alignment = (avg_stability + avg_change) * 0.5
        
        return np.clip(alignment, 0.0, 1.0)
    
    def extract_va_features(self, emotion):
        """Valence-Arousal íŠ¹ì§• ì¶”ì¶œ (MetaStrategyMCTSì™€ ë™ì¼)"""
        va_mapping = {
            'happy': np.array([0.8, 0.6]),      # ë†’ì€ valence, ì¤‘ê°„ arousal
            'surprise': np.array([0.3, 0.9]),   # ì¤‘ê°„ valence, ë†’ì€ arousal
            'sad': np.array([-0.7, -0.3]),      # ë‚®ì€ valence, ë‚®ì€ arousal
            'anger': np.array([-0.6, 0.8]),     # ë‚®ì€ valence, ë†’ì€ arousal
            'fear': np.array([-0.8, 0.7]),      # ë§¤ìš° ë‚®ì€ valence, ë†’ì€ arousal
            'disgust': np.array([-0.5, 0.2]),   # ë‚®ì€ valence, ë‚®ì€ arousal
            'neutral': np.array([0.0, 0.0])     # ì¤‘ë¦½
        }
        return va_mapping.get(emotion, np.array([0.0, 0.0]))
    
    def extract_cognitive_features(self, cognitive_load):
        """ì¸ì§€ ë¶€í•˜ íŠ¹ì§• ì¶”ì¶œ (MetaStrategyMCTSì™€ ë™ì¼)"""
        cognitive_mapping = {
            'low': np.array([0.2, 0.8, 0.9]),     # [ë¶€í•˜, ì—¬ìœ ë„, ì²˜ë¦¬ëŠ¥ë ¥]
            'medium': np.array([0.5, 0.5, 0.6]),
            'high': np.array([0.9, 0.2, 0.3])
        }
        return cognitive_mapping.get(cognitive_load, np.array([0.5, 0.5, 0.6]))
    
    def extract_gaze_features(self, attention, context):
        """ì‹œì„  ì¶”ì  íŠ¹ì§• ì¶”ì¶œ (MetaStrategyMCTSì™€ ë™ì¼)"""
        fixation_stability = context.get('fixation_stability', 0.5)
        pupil_size = context.get('pupil_size', 0.5)
        
        return np.array([attention, fixation_stability, pupil_size])

# ==================== final_sac.py ë°©ì‹ì˜ ì „ì—­ ë³€ìˆ˜ë“¤ ====================
# ì´ë¯¸ì§€ ì „ì²˜ë¦¬ (final_sac.pyì™€ ë™ì¼)
transform = transforms.Compose([
    transforms.Resize((64, 64)),
    transforms.Grayscale(num_output_channels=3),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
])

# ==================== ì‹¤ì œ ResEmoteNet ëª¨ë¸ ====================
class ResEmoteNet:
    """ì‹¤ì œ í›ˆë ¨ëœ ResEmoteNet ëª¨ë¸"""
    def __init__(self, model_path='fer2013_model.pth'):
        self.device = torch.device("mps" if torch.backends.mps.is_available() else "cpu")
        self.emotions = ['happy', 'surprise', 'sad', 'anger', 'disgust', 'fear', 'neutral']
        
        print(f"Brain Device: {self.device}")
        
        # ì‹¤ì œ ResEmoteNet ëª¨ë¸ ë¡œë“œ
        from approach.ResEmoteNet import ResEmoteNet
        self.model = ResEmoteNet().to(self.device)
        
        # í›ˆë ¨ëœ ëª¨ë¸ ë¡œë“œ
        checkpoint = torch.load(model_path, map_location=self.device)
        self.model.load_state_dict(checkpoint['model_state_dict'])
        self.model.eval()
        
        print("âœ… ì‹¤ì œ í›ˆë ¨ëœ ResEmoteNet ëª¨ë¸ ë¡œë“œë¨")
        
        # ì´ë¯¸ì§€ ì „ì²˜ë¦¬ (final_sac.pyì™€ ë™ì¼)
        self.transform = transforms.Compose([
            transforms.Resize((64, 64)),
            transforms.Grayscale(num_output_channels=3),
            transforms.ToTensor(),
            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
        ])
        
        self.last_emotion = 'neutral'
        self.emotion_stability_count = 0
        

# ==================== ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ë“¤ (ê¸°ì¡´ ì½”ë“œ) ====================
class EMA:
    def __init__(self, alpha=0.3):
        self.alpha = alpha
        self.value = None
    
    def update(self, new_value):
        if self.value is None:
            self.value = new_value
        else:
            self.value = self.alpha * new_value + (1 - self.alpha) * self.value
        return self.value

class VectorEMA:
    def __init__(self, alpha: float, length: int):
        self.alpha = alpha
        self.length = length
        self.value = None
    
    def update(self, new_values):
        # Ensure correct length and numpy array
        arr = np.array(new_values, dtype=float)
        if arr.shape[0] != self.length:
            # Pad or truncate to target length
            if arr.shape[0] < self.length:
                pad = np.zeros(self.length - arr.shape[0], dtype=float)
                arr = np.concatenate([arr, pad])
            else:
                arr = arr[:self.length]
        if self.value is None:
            self.value = arr
        else:
            self.value = self.alpha * arr + (1.0 - self.alpha) * self.value
        return self.value

def clamp_float(value: float, min_v: float, max_v: float, default: float) -> float:
    try:
        if value is None or not np.isfinite(value):
            return default
        return float(min(max(value, min_v), max_v))
    except Exception:
        return default

def sanitize_probs(probs: List[float], length: int, default_idx: int) -> List[float]:
    arr = np.array(probs if probs is not None else [], dtype=float)
    if arr.size != length:
        if arr.size < length:
            pad = np.zeros(length - arr.size, dtype=float)
            arr = np.concatenate([arr, pad])
        else:
            arr = arr[:length]
    arr[~np.isfinite(arr)] = 0.0
    s = float(arr.sum())
    if s <= 1e-8:
        arr[:] = 0.0
        arr[default_idx] = 1.0
        return arr.tolist()
    return (arr / s).tolist()

def iris_center_radius(landmarks, iris_indices, frame_width, frame_height):
    if not iris_indices:
        return None, None, None
    
    points = []
    for idx in iris_indices:
        if idx < len(landmarks):
            x = int(landmarks[idx].x * frame_width)
            y = int(landmarks[idx].y * frame_height)
            points.append((x, y))
    
    if len(points) < 3:
        return None, None, None
    
    points = np.array(points)
    center_x = np.mean(points[:, 0])
    center_y = np.mean(points[:, 1])
    distances = np.sqrt((points[:, 0] - center_x)**2 + (points[:, 1] - center_y)**2)
    radius = np.max(distances)
    
    return center_x, center_y, radius

def calculate_fixation_stability(gaze_buffer):
    if len(gaze_buffer) < 2:
        return None, None
    
    points = np.array(list(gaze_buffer))
    if points.shape[0] < 2:
        return None, None
    
    cov_matrix = np.cov(points.T)
    if cov_matrix.shape == ():
        cov_matrix = np.array([[cov_matrix]])
    elif cov_matrix.shape == (2,):
        cov_matrix = np.diag(cov_matrix)
    
    eigenvalues = np.linalg.eigvals(cov_matrix)
    eigenvalues = np.real(eigenvalues)
    eigenvalues = np.sort(eigenvalues)[::-1]
    
    if len(eigenvalues) < 2:
        eigenvalues = np.pad(eigenvalues, (0, 2-len(eigenvalues)), 'constant')
    
    lambda1, lambda2 = eigenvalues[0], eigenvalues[1]
    area = np.pi * np.sqrt(max(lambda1, 0)) * np.sqrt(max(lambda2, 0))
    fix_stab = 1 / (1 + area)
    
    return area, fix_stab

def calculate_mad(values):
    if len(values) == 0:
        return 0
    median = np.median(values)
    mad = np.median(np.abs(values - median))
    return mad

def euclidean(p1, p2):
    return np.linalg.norm(np.array(p1) - np.array(p2))

def calculate_ear(eye_points):
    A = euclidean(eye_points[1], eye_points[5])
    B = euclidean(eye_points[2], eye_points[4])
    C = euclidean(eye_points[0], eye_points[3])
    ear = (A + B) / (2.0 * C)
    return ear

# ==================== Level 2: Agent Combination MCTS ====================
class CombinationNode:
    """ì—ì´ì „íŠ¸ ì¡°í•© ì„ íƒ ë…¸ë“œ"""
    
    def __init__(self, combination: Tuple[Optional[str], Optional[str], Optional[str]], parent=None):
        self.combination = combination  # (planner_action, critic_action, executor_action)
        self.parent = parent
        self.children = []
        self.visits = 0
        self.total_reward = 0.0
        self.depth = 0 if parent is None else parent.depth + 1
        
        # ì¡°í•© êµ¬ì„± ìš”ì†Œë“¤
        self.planner_actions = ["encouraging", "challenging", "adaptive", "calming", "motivating"]
        self.critic_actions = ["supportive", "thorough", "balanced", "understanding", "optimistic"]
        self.executor_actions = ["gentle", "intensive", "standard", "careful", "energetic"]
    
    def is_terminal(self):
        """ì™„ì „í•œ ì¡°í•©ì´ ì™„ì„±ë˜ì—ˆëŠ”ì§€ í™•ì¸"""
        return all(x is not None for x in self.combination)
    
    def get_possible_expansions(self):
        """ë‹¤ìŒì— í™•ì¥ ê°€ëŠ¥í•œ ë…¸ë“œë“¤"""
        planner, critic, executor = self.combination
        
        expansions = []
        if planner is None:
            for action in self.planner_actions:
                expansions.append((action, critic, executor))
        elif critic is None:
            for action in self.critic_actions:
                expansions.append((planner, action, executor))
        elif executor is None:
            for action in self.executor_actions:
                expansions.append((planner, critic, action))
        
        return expansions
    
    def update(self, reward):
        self.visits += 1
        self.total_reward += reward
    
    def get_average_reward(self):
        return self.total_reward / self.visits if self.visits > 0 else 0

class CombinationMCTS:
    """Level 2: ì—ì´ì „íŠ¸ ì¡°í•© ìµœì í™” MCTS"""
    
    def __init__(self, c_param=1.4):
        self.c_param = c_param
        self.combination_history = deque(maxlen=30)
        
    def search(self, user_context: Dict, meta_strategy: str, adaptation_type: str, iterations=40) -> Tuple[str, str, str]:
        """ìµœì  ì—ì´ì „íŠ¸ ì¡°í•© íƒìƒ‰"""
        
        # ë£¨íŠ¸ ë…¸ë“œ ìƒì„± (ë¹ˆ ì¡°í•©ì—ì„œ ì‹œì‘)
        root = CombinationNode((None, None, None))
        
        for _ in range(iterations):
            # Selection & Expansion
            leaf = self._select_and_expand(root, user_context, adaptation_type)
            
            # Simulation
            reward = self._calculate_combination(leaf, user_context, meta_strategy, adaptation_type)
            
            # Backpropagation
            self._backpropagate(leaf, reward)
        
        # ìµœì  ì¡°í•© ì„ íƒ
        best_combination = self._get_best_combination(root)
        return best_combination
    
    def _select_and_expand(self, root, user_context, adaptation_type):
        """UCB1 ê¸°ë°˜ ì„ íƒ ë° í™•ì¥"""
        
        node = root
        
        # Selection: í„°ë¯¸ë„ ë…¸ë“œë‚˜ í™•ì¥ ê°€ëŠ¥í•œ ë…¸ë“œê¹Œì§€
        while node.children and not node.is_terminal():
            if not node.children:
                break
            node = self._best_child_ucb1(node)
        
        # Expansion: ìƒˆë¡œìš´ ìì‹ ë…¸ë“œ ì¶”ê°€
        if not node.is_terminal():
            expansions = node.get_possible_expansions()
            if expansions:
                # ì ì‘ íƒ€ì…ì— ë”°ë¼ í™•ì¥ ì „ëµ ì¡°ì •
                if adaptation_type == "simplified":
                    # ë‹¨ìˆœí™” ëª¨ë“œ: ì²« ë²ˆì§¸ ì˜µì…˜ ì„ íƒ
                    new_combination = expansions[0]
                else:
                    # í‘œì¤€/ë³µì¡ ëª¨ë“œ: ëœë¤ ì„ íƒ
                    new_combination = random.choice(expansions)
                
                child = CombinationNode(new_combination, parent=node)
                node.children.append(child)
                return child
        
        return node
    
    def _best_child_ucb1(self, node):
        """UCB1ìœ¼ë¡œ ìµœì  ìì‹ ì„ íƒ"""
        
        best_score = float('-inf')
        best_child = None
        
        for child in node.children:
            if child.visits == 0:
                return child  # ì•„ì§ ë°©ë¬¸í•˜ì§€ ì•Šì€ ë…¸ë“œ ìš°ì„ 
            
            exploitation = child.get_average_reward()
            exploration = self.c_param * math.sqrt(2 * math.log(node.visits) / child.visits)
            ucb1_score = exploitation + exploration
            
            if ucb1_score > best_score:
                best_score = ucb1_score
                best_child = child
        
        return best_child
    
    def _calculate_combination(self, node, user_context, meta_strategy, adaptation_type):
        """ì¡°í•© íš¨ê³¼ì„± ê³„ì‚°"""
        
        combination = node.combination
        planner, critic, executor = combination
        
        # ì™„ì „í•œ ì¡°í•©ì´ ì•„ë‹Œ ê²½ìš° ëœë¤ìœ¼ë¡œ ì™„ì„±
        if not node.is_terminal():
            if planner is None:
                planner = random.choice(node.planner_actions)
            if critic is None:
                critic = random.choice(node.critic_actions)
            if executor is None:
                executor = random.choice(node.executor_actions)
        
        emotion = user_context.get('emotion', 'neutral')
        attention = user_context.get('attention', 0.5)
        cognitive_load = user_context.get('cognitive_load_level', 'medium')
        
        # Mixed-R1 BMAS ê¸°ë°˜ í†µí•© ë³´ìƒí•¨ìˆ˜ (NeurIPS 2024)
        
        # 1. BMAS ì—ì´ì „íŠ¸ ìœ ì‚¬ë„ ê³„ì‚°
        bmas_reward = self.mixed_r1_combination_reward((planner, critic, executor), user_context)
        
        # 2. ìƒë¦¬ì‹ í˜¸ í†µí•© ë³´ìƒ
        physio_reward = self.physiological_gaze_reward((planner, critic, executor), user_context)
        
        # 3. ê°€ì¤‘ ê²°í•©
        integrated_reward = 0.5 * bmas_reward + 0.5 * physio_reward
        
        return np.clip(integrated_reward, 0.0, 1.0)
    
    def mixed_r1_combination_reward(self, combination, context):
        """Mixed-R1 (NeurIPS 2024) BMAS ê¸°ë°˜ ì¡°í•© ë³´ìƒ"""
        
        pred_combination = combination  # (planner, critic, executor)
        
        # 1. ìµœì  ì¡°í•© ì¶”ì • (ì»¨í…ìŠ¤íŠ¸ ê¸°ë°˜)
        optimal_combination = self.get_optimal_combination(context)
        
        # 2. BMAS ê³„ì‚°
        bmas_score = self.calculate_bmas(pred_combination, optimal_combination)
        
        return bmas_score
    
    def calculate_bmas(self, pred_agents, optimal_agents):
        """BMAS (Bidirectional Max-Average Similarity) ê³µì‹"""
        
        # Agent embedding ê³„ì‚°
        pred_embeddings = [self.get_agent_embedding(agent) for agent in pred_agents if agent is not None]
        optimal_embeddings = [self.get_agent_embedding(agent) for agent in optimal_agents if agent is not None]
        
        if not pred_embeddings or not optimal_embeddings:
            return 0.5  # ê¸°ë³¸ê°’
        
        # Forward similarity: pred â†’ optimal
        forward_similarities = []
        for pred_emb in pred_embeddings:
            sims = [self.cosine_similarity(pred_emb, opt_emb) for opt_emb in optimal_embeddings]
            forward_similarities.extend(sims)
        
        # Backward similarity: optimal â†’ pred  
        backward_similarities = []
        for opt_emb in optimal_embeddings:
            sims = [self.cosine_similarity(opt_emb, pred_emb) for pred_emb in pred_embeddings]
            backward_similarities.extend(sims)
        
        # BMAS ê³µì‹
        if forward_similarities and backward_similarities:
            bmas = (max(forward_similarities) + np.mean(backward_similarities)) / 2
        else:
            bmas = 0.5
        
        return np.clip(bmas, 0.0, 1.0)
    
    def get_optimal_combination(self, context):
        """ì»¨í…ìŠ¤íŠ¸ ê¸°ë°˜ ìµœì  ì¡°í•© ê²°ì •"""
        emotion = context.get('emotion', 'neutral')
        cognitive_load = context.get('cognitive_load_level', 'medium')
        attention = context.get('attention', 0.5)
        
        if emotion in ['sad', 'fear']:
            return ('encouraging', 'supportive', 'gentle')
        elif cognitive_load == 'high':
            return ('calming', 'understanding', 'careful')
        elif attention > 0.8:
            return ('challenging', 'thorough', 'intensive')
        elif emotion in ['happy', 'surprise']:
            return ('motivating', 'optimistic', 'energetic')
        else:
            return ('adaptive', 'balanced', 'standard')
    
    def get_agent_embedding(self, agent):
        """ì—ì´ì „íŠ¸ ì„ë² ë”© ë²¡í„° ìƒì„±"""
        if agent is None:
            return np.zeros(5)
        
        # ì—ì´ì „íŠ¸ë³„ íŠ¹ì„± ë²¡í„° [í™œë™ì„±, ì§€ì§€ì„±, ë„ì „ì„±, ì‹ ì¤‘ì„±, íš¨ìœ¨ì„±]
        agent_embeddings = {
            # Planner agents
            'encouraging': np.array([0.7, 0.9, 0.6, 0.4, 0.6]),
            'calming': np.array([0.3, 0.8, 0.2, 0.9, 0.5]),
            'challenging': np.array([0.9, 0.3, 0.9, 0.5, 0.7]),
            'motivating': np.array([0.8, 0.7, 0.8, 0.4, 0.8]),
            'adaptive': np.array([0.5, 0.5, 0.5, 0.7, 0.7]),
            
            # Critic agents
            'supportive': np.array([0.4, 0.9, 0.3, 0.6, 0.5]),
            'understanding': np.array([0.3, 0.8, 0.2, 0.8, 0.4]),
            'thorough': np.array([0.6, 0.4, 0.7, 0.9, 0.8]),
            'optimistic': np.array([0.7, 0.6, 0.6, 0.3, 0.6]),
            'balanced': np.array([0.5, 0.6, 0.5, 0.6, 0.7]),
            
            # Executor agents
            'gentle': np.array([0.3, 0.8, 0.2, 0.7, 0.5]),
            'careful': np.array([0.4, 0.6, 0.3, 0.9, 0.6]),
            'intensive': np.array([0.9, 0.3, 0.8, 0.5, 0.9]),
            'energetic': np.array([0.8, 0.5, 0.7, 0.3, 0.8]),
            'standard': np.array([0.5, 0.5, 0.5, 0.5, 0.7])
        }
        
        return agent_embeddings.get(agent, np.array([0.5, 0.5, 0.5, 0.5, 0.5]))
    
    def cosine_similarity(self, vec1, vec2):
        """ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°"""
        dot_product = np.dot(vec1, vec2)
        norm1 = np.linalg.norm(vec1)
        norm2 = np.linalg.norm(vec2)
        
        if norm1 == 0 or norm2 == 0:
            return 0.0
        
        return dot_product / (norm1 * norm2)
    
    def physiological_gaze_reward(self, combination, context):
        """ìƒë¦¬ì‹ í˜¸-ì‹œì„  í†µí•© ë³´ìƒ (EMBC 2024)"""
        
        # 1. ë™ê³µ í¬ê¸° ê¸°ë°˜ ì¸ì§€ë¶€í•˜ (EEG ëŒ€ì•ˆ)
        cognitive_2d = self.pupil_to_cognitive_load(context.get('pupil_size', 0.5))
        
        # 2. Eye Tracking íŠ¹ì§•
        gaze_features = {
            'fixation_duration': context.get('fixation_duration', 0.5),
            'saccade_velocity': context.get('saccade_velocity', 0.5), 
            'pupil_diameter': context.get('pupil_size', 0.5),
            'blink_rate': context.get('blink_rate', 0.5)
        }
        
        # 3. íŠ¹ì§• ì¶”ì¶œ ë° ìœµí•©
        eeg_features = self.extract_eeg_features(cognitive_2d)
        eye_features = self.extract_eye_features(gaze_features)
        
        # 4. ì¡°í•©ê³¼ì˜ ì •ë ¬ í‰ê°€
        combination_alignment = self.evaluate_combination_alignment(
            eeg_features, eye_features, combination, context
        )
        
        return combination_alignment
    
    def pupil_to_cognitive_load(self, pupil_size):
        """ë™ê³µ í¬ê¸°ë¥¼ ì¸ì§€ë¶€í•˜ 2D í‘œí˜„ìœ¼ë¡œ ë³€í™˜"""
        # ë™ê³µ í¬ê¸°ë¥¼ 2D ê³µê°„ íŒ¨í„´ìœ¼ë¡œ ë§¤í•‘
        load_intensity = pupil_size
        spatial_pattern = np.array([
            [load_intensity * 0.8, load_intensity * 0.6],
            [load_intensity * 0.9, load_intensity * 0.7]
        ])
        return spatial_pattern
    
    def extract_eeg_features(self, cognitive_2d):
        """ì¸ì§€ë¶€í•˜ 2D íŒ¨í„´ì—ì„œ íŠ¹ì§• ì¶”ì¶œ"""
        # ê°„ë‹¨í•œ í†µê³„ì  íŠ¹ì§•
        mean_activity = np.mean(cognitive_2d)
        std_activity = np.std(cognitive_2d)
        max_activity = np.max(cognitive_2d)
        
        return np.array([mean_activity, std_activity, max_activity])
    
    def extract_eye_features(self, gaze_features):
        """ì‹œì„  ì¶”ì  íŠ¹ì§• ë²¡í„° ìƒì„±"""
        return np.array([
            gaze_features['fixation_duration'],
            gaze_features['saccade_velocity'],
            gaze_features['pupil_diameter'],
            gaze_features['blink_rate']
        ])
    
    def evaluate_combination_alignment(self, eeg_features, eye_features, combination, context):
        """ì¡°í•©ê³¼ ìƒë¦¬ì‹ í˜¸ì˜ ì •ë ¬ í‰ê°€"""
        
        planner, critic, executor = combination
        
        # ìœµí•©ëœ íŠ¹ì§•
        fused_features = np.concatenate([eeg_features, eye_features])
        
        # ì¡°í•©ë³„ ê°€ì¤‘ì¹˜
        combination_weights = {
            ('encouraging', 'supportive', 'gentle'): np.array([0.3, 0.2, 0.4, 0.6, 0.8, 0.7, 0.5]),
            ('challenging', 'thorough', 'intensive'): np.array([0.8, 0.7, 0.9, 0.4, 0.3, 0.5, 0.6]),
            ('calming', 'understanding', 'careful'): np.array([0.2, 0.1, 0.3, 0.8, 0.9, 0.8, 0.6]),
            ('adaptive', 'balanced', 'standard'): np.array([0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5])
        }
        
        # ê¸°ë³¸ ê°€ì¤‘ì¹˜ (ì¡°í•©ì´ ì •í™•íˆ ë§¤ì¹­ë˜ì§€ ì•ŠëŠ” ê²½ìš°)
        weights = combination_weights.get(combination, np.ones(len(fused_features)) / len(fused_features))
        
        # ê¸¸ì´ ë§ì¶¤
        if len(weights) != len(fused_features):
            if len(weights) > len(fused_features):
                weights = weights[:len(fused_features)]
            else:
                weights = np.pad(weights, (0, len(fused_features) - len(weights)), 'constant', constant_values=0.5)
        
        # ê°€ì¤‘ ì ìˆ˜ ê³„ì‚°
        alignment_score = np.dot(fused_features, weights) / np.sum(weights)
        
        return np.clip(alignment_score, 0.0, 1.0)
        
        # ğŸ˜Š ê°ì • ê¸°ë°˜ ì¡°í•© í‰ê°€
        emotion_bonuses = {
            'happy': {
                'challenging': 0.3, 'motivating': 0.2,
                'optimistic': 0.2, 'thorough': 0.1,
                'energetic': 0.2, 'intensive': 0.1
            },
            'sad': {
                'encouraging': 0.3, 'calming': 0.2,
                'supportive': 0.3, 'understanding': 0.2,
                'gentle': 0.2, 'careful': 0.1
            },
            'anger': {
                'calming': 0.4, 'adaptive': 0.2,
                'understanding': 0.3, 'supportive': 0.1,
                'careful': 0.2, 'gentle': 0.1
            },
            'fear': {
                'encouraging': 0.2, 'calming': 0.3,
                'supportive': 0.3, 'understanding': 0.2,
                'gentle': 0.3, 'careful': 0.2
            }
        }
        
        if emotion in emotion_bonuses:
            bonuses = emotion_bonuses[emotion]
            reward += bonuses.get(planner, 0)
            reward += bonuses.get(critic, 0)
            reward += bonuses.get(executor, 0)
        
        # ğŸ‘ï¸ ì§‘ì¤‘ë„ ê¸°ë°˜ í‰ê°€
        if attention > 0.8:
            # ë†’ì€ ì§‘ì¤‘ë„ â†’ ë„ì „ì  ì¡°í•© ê°€ëŠ¥
            if planner == 'challenging' and critic == 'thorough':
                reward += 0.15
        elif attention < 0.4:
            # ë‚®ì€ ì§‘ì¤‘ë„ â†’ ë¶€ë“œëŸ¬ìš´ ì ‘ê·¼ í•„ìš”
            if planner == 'encouraging' and executor == 'gentle':
                reward += 0.1
        
        # ğŸ“Š ë©”íƒ€ ì „ëµê³¼ì˜ ì‹œë„ˆì§€
        if meta_strategy == 'combination':
            # ì¡°í•© ìµœì í™”ì™€ ì–´ìš¸ë¦¬ëŠ” íš¨ìœ¨ì  ì¡°í•©
            if planner == 'adaptive' and critic == 'balanced':
                reward += 0.1
        elif meta_strategy == 'collaboration':
            # í˜‘ë ¥ê³¼ ì–´ìš¸ë¦¬ëŠ” ì†Œí†µ ì¤‘ì‹¬ ì¡°í•©  
            if critic == 'supportive' and executor == 'standard':
                reward += 0.1
        
        # ğŸ”„ ì ì‘ íƒ€ì… ë°˜ì˜
        if adaptation_type == 'simplified':
            # ë‹¨ìˆœí™” â†’ í‘œì¤€ì ì¸ ì¡°í•© ì„ í˜¸
            if planner == 'adaptive' and critic == 'balanced' and executor == 'standard':
                reward += 0.2
        elif adaptation_type == 'complex':
            # ë³µì¡í™” â†’ ì •êµí•œ ì¡°í•© ì„ í˜¸
            if len(set([planner, critic, executor])) == 3:  # ëª¨ë‘ ë‹¤ë¥¸ ìŠ¤íƒ€ì¼
                reward += 0.15
        
        # ì¡°í•© ì‹œë„ˆì§€ ë³´ë„ˆìŠ¤
        synergy_combinations = [
            ('challenging', 'thorough', 'intensive'),      # ê³ ì„±ëŠ¥ ì¡°í•©
            ('encouraging', 'supportive', 'gentle'),       # ë°°ë ¤ ì¡°í•©
            ('calming', 'understanding', 'careful'),       # ì•ˆì • ì¡°í•©
            ('adaptive', 'balanced', 'standard'),          # ê· í˜• ì¡°í•©
            ('motivating', 'optimistic', 'energetic')     # í™œë ¥ ì¡°í•©
        ]
        
        if (planner, critic, executor) in synergy_combinations:
            reward += 0.2
        
        # ëœë¤ ë…¸ì´ì¦ˆ
        reward += np.random.normal(0, 0.05)
        return np.clip(reward, 0.0, 1.0)
    
    def _backpropagate(self, node, reward):
        """ë³´ìƒ ì—­ì „íŒŒ"""
        while node is not None:
            node.update(reward)
            node = node.parent
    
    def _get_best_combination(self, root):
        """ìµœì  ì¡°í•© ì„ íƒ"""
        
        # ëª¨ë“  í„°ë¯¸ë„ ë…¸ë“œ ì°¾ê¸°
        terminal_nodes = []
        self._collect_terminal_nodes(root, terminal_nodes)
        
        if not terminal_nodes:
            # í„°ë¯¸ë„ ë…¸ë“œê°€ ì—†ìœ¼ë©´ ê¸°ë³¸ ì¡°í•© ë°˜í™˜
            return ("adaptive", "balanced", "standard")
        
        # ê°€ì¥ ë†’ì€ í‰ê·  ë³´ìƒì„ ê°€ì§„ ì¡°í•© ì„ íƒ
        best_node = max(terminal_nodes, key=lambda x: x.get_average_reward())
        return best_node.combination
    
    def _collect_terminal_nodes(self, node, terminal_nodes):
        """í„°ë¯¸ë„ ë…¸ë“œ ìˆ˜ì§‘"""
        if node.is_terminal():
            terminal_nodes.append(node)
        else:
            for child in node.children:
                self._collect_terminal_nodes(child, terminal_nodes)

# ==================== Level 3: Execution Strategy MCTS ====================
class ExecutionStrategyMCTS:
    """
    Level 3: R* (NeurIPS 2024) ê¸°ë°˜ ì‹¤í–‰ ì „ëµ ìµœì í™”
    
    R* ë…¼ë¬¸ì˜ í•µì‹¬ ê¸°ë²•:
    - Reward Structure Evolution
    - Multi-module Dynamic Weighting  
    - Context-adaptive Fitness Evaluation
    """
    
    # R* ì„¤ì • ìƒìˆ˜ë“¤
    class Config:
        # ì§„í™” ì•Œê³ ë¦¬ì¦˜ íŒŒë¼ë¯¸í„°
        EVOLUTION_GENERATIONS = 3
        POPULATION_SIZE = 10
        ELITE_SIZE = 2
        MUTATION_RATE_BASE = 0.1
        TOURNAMENT_SIZE = 2
        CROSSOVER_ALPHA = 0.5
        CONVERGENCE_THRESHOLD = 0.005
        
        # ìºì‹± ì‹œìŠ¤í…œ íŒŒë¼ë¯¸í„°
        CACHE_HIT_THRESHOLD = 0.8
        MAX_CACHE_SIZE = 50
        
        # ì»¨í…ìŠ¤íŠ¸ ìœ ì‚¬ë„ íŒŒë¼ë¯¸í„°
        ATTENTION_SIMILARITY_THRESHOLD = 0.2
        
        # ë©”ëª¨ë¦¬ ê´€ë¦¬ íŒŒë¼ë¯¸í„°
        POPULATION_CLEANUP_INTERVAL = 10
        
        # í´ë°± ë³´ìƒ íŒŒë¼ë¯¸í„°
        FALLBACK_REWARD_DEFAULT = 0.65
        CONTEXT_BONUS = 0.1
        COGNITIVE_BONUS = 0.05
    
    def __init__(self):
        self.execution_strategies = [
            "gentle_adaptive",      # ë¶€ë“œëŸ¬ìš´ ì ì‘í˜•
            "intensive_focused",    # ì§‘ì¤‘í˜• ê°•í™”
            "balanced_standard",    # ê· í˜•ì¡íŒ í‘œì¤€
            "careful_methodical",   # ì‹ ì¤‘í•œ ì²´ê³„í˜•  
            "energetic_dynamic",    # í™œë ¥ìˆëŠ” ë™ì 
            "supportive_gradual",   # ì§€ì§€ì  ì ì§„í˜•
            "optimized_efficient"   # ìµœì í™” íš¨ìœ¨í˜•
        ]
        
        # R* ì „ìš© ì†ì„± (ì„¤ì •í™”ë¨)
        # ì§„í™” ì•Œê³ ë¦¬ì¦˜ íŒŒë¼ë¯¸í„°ë“¤ì„ Config í´ë˜ìŠ¤ì—ì„œ ê°€ì ¸ì˜´
        self.reward_structure_population = []  # ë³´ìƒ êµ¬ì¡° ê°œì²´êµ°
        self.evolution_generations = self.Config.EVOLUTION_GENERATIONS
        self.population_size = self.Config.POPULATION_SIZE
        self.elite_size = self.Config.ELITE_SIZE
        self.mutation_rate_base = self.Config.MUTATION_RATE_BASE
        self.tournament_size = self.Config.TOURNAMENT_SIZE
        self.crossover_alpha = self.Config.CROSSOVER_ALPHA
        self.convergence_threshold = self.Config.CONVERGENCE_THRESHOLD
        
        # R* ìºì‹± ì‹œìŠ¤í…œ (ì„¤ì •í™”ë¨)
        self.structure_cache = {}              # ì»¨í…ìŠ¤íŠ¸ í•´ì‹œ: ìµœì  êµ¬ì¡°
        self.cache_hit_threshold = self.Config.CACHE_HIT_THRESHOLD
        self.max_cache_size = self.Config.MAX_CACHE_SIZE
        self.cache_hits = 0                    # ìºì‹œ íˆíŠ¸ ì¹´ìš´íŠ¸
        self.cache_misses = 0                  # ìºì‹œ ë¯¸ìŠ¤ ì¹´ìš´íŠ¸
        
    def search(self, user_context: Dict, meta_strategy: str, adaptation_type: str, 
               combination: Tuple[str, str, str], iterations=20) -> str:
        """ìµœì  ì‹¤í–‰ ì „ëµ ì„ íƒ"""
        
        strategy_rewards = {}
        
        # ê° ì‹¤í–‰ ì „ëµ í‰ê°€
        for strategy in self.execution_strategies:
            total_reward = 0
            for _ in range(iterations // len(self.execution_strategies) + 1):
                reward = self._calculate_execution_strategy(
                    strategy, user_context, meta_strategy, adaptation_type, combination
                )
                total_reward += reward
                
            strategy_rewards[strategy] = total_reward / (iterations // len(self.execution_strategies) + 1)
        
        # ìµœì  ì „ëµ ì„ íƒ
        best_strategy = max(strategy_rewards.keys(), 
                          key=lambda x: strategy_rewards[x])
        
        return best_strategy
    
    def _calculate_execution_strategy(self, strategy, user_context, meta_strategy, 
                                    adaptation_type, combination):
        """ì‹¤í–‰ ì „ëµ íš¨ê³¼ì„± ê³„ì‚°"""
        
        emotion = user_context.get('emotion', 'neutral')
        cognitive_load = user_context.get('cognitive_load_level', 'medium')
        attention = user_context.get('attention', 0.5)
        
        planner_action, critic_action, executor_action = combination
        
        # R* (NeurIPS 2024) ê¸°ë°˜ ìë™ ë³´ìƒ ì„¤ê³„
        
        # R* ì§„í™”ëœ ë³´ìƒ í•¨ìˆ˜ ì‚¬ìš©
        r_star_reward = self.r_star_evolved_reward(
            strategy, user_context, meta_strategy, adaptation_type, combination
        )
        
        return r_star_reward
    
    def r_star_evolved_reward(self, strategy: str, user_context: Dict[str, Any], 
                             meta_strategy: str, adaptation_type: str, 
                             combination: Tuple[str, str, str]) -> float:
        """R* (NeurIPS 2024) ê¸°ë°˜ ì§„í™”ëœ ë³´ìƒ ì„¤ê³„ (ìºì‹± + ì—ëŸ¬ ì²˜ë¦¬)"""
        
        try:
            # 1. ì…ë ¥ ê²€ì¦
            if not strategy or not user_context:
                raise ValueError("Invalid input: strategy or user_context is empty")
            
            # 2. ìºì‹œ í™•ì¸
            context_hash = self.hash_context(user_context, strategy, meta_strategy, adaptation_type)
            cached_structure = self.get_cached_structure(context_hash)
            
            # 3. ë³´ìƒ ëª¨ë“ˆ ìƒì„±
            reward_modules = self.generate_reward_modules(strategy, user_context, combination)
            
            # 4. ëª¨ë“ˆ ê²€ì¦
            if not reward_modules or len(reward_modules) == 0:
                raise ValueError("No reward modules generated")
            
            # 5. NaN/Inf ê²€ì‚¬
            for module_name, value in reward_modules.items():
                if not np.isfinite(value):
                    print(f"Warning: {module_name} has invalid value {value}, using default 0.5")
                    reward_modules[module_name] = 0.5
            
            if cached_structure is not None:
                # ìºì‹œ íˆíŠ¸: ì¦‰ì‹œ ê³„ì‚°
                self.cache_hits += 1
                final_reward = self.calculate_evolved_reward(reward_modules, cached_structure)
            else:
                # ìºì‹œ ë¯¸ìŠ¤: ì§„í™” ì•Œê³ ë¦¬ì¦˜ ì‹¤í–‰
                self.cache_misses += 1
                optimal_structure = self.evolve_reward_structure(reward_modules, user_context, strategy)
                
                # ê²°ê³¼ ê²€ì¦
                if not optimal_structure:
                    raise ValueError("Evolution failed to produce valid structure")
                
                # ê²°ê³¼ ìºì‹±
                self.cache_structure(context_hash, optimal_structure)
                
                final_reward = self.calculate_evolved_reward(reward_modules, optimal_structure)
            
            # 6. ìµœì¢… ê²€ì¦
            if not np.isfinite(final_reward):
                print(f"Warning: Final reward is invalid {final_reward}, using fallback")
                final_reward = self.fallback_reward(strategy, user_context)
            
            return np.clip(final_reward, 0.0, 1.0)
            
        except Exception as e:
            print(f"Error in R* evolution: {e}")
            # í´ë°± ë³´ìƒ ê³„ì‚°
            return self.fallback_reward(strategy, user_context)
    
    def generate_reward_modules(self, strategy: str, context: Dict[str, Any], 
                               combination: Tuple[str, str, str]) -> Dict[str, float]:
        """
        R* ë³´ìƒ ëª¨ë“ˆ ìƒì„±
        
        6ê°œì˜ ë³´ìƒ ëª¨ë“ˆì„ ìƒì„±í•˜ì—¬ ì§„í™” ì•Œê³ ë¦¬ì¦˜ì—ì„œ ì‚¬ìš©
        
        Args:
            strategy (str): ì‹¤í–‰ ì „ëµ ì´ë¦„
            context (Dict): ì‚¬ìš©ì ì»¨í…ìŠ¤íŠ¸ (ê°ì •, ì¸ì§€ë¶€í•˜, ì£¼ì˜ì§‘ì¤‘ë„)
            combination (Tuple): ì—ì´ì „íŠ¸ ì¡°í•© (planner, critic, executor)
            
        Returns:
            Dict[str, float]: ëª¨ë“ˆëª… -> ë³´ìƒê°’ ë§¤í•‘ (0.0-1.0 ë²”ìœ„)
            
        Modules:
            - efficiency: ì „ëµì˜ íš¨ìœ¨ì„± í‰ê°€
            - satisfaction: ì‚¬ìš©ì ë§Œì¡±ë„ í‰ê°€
            - resource: ìì› ì‚¬ìš© ìµœì í™” í‰ê°€
            - cognitive_alignment: ì¸ì§€ ìƒíƒœì™€ì˜ ì •ë ¬ë„
            - emotion_adaptation: ê°ì • ì ì‘ë„ (R* ìƒˆ ëª¨ë“ˆ)
            - temporal_efficiency: ì‹œê°„ íš¨ìœ¨ì„± (R* ìƒˆ ëª¨ë“ˆ)
        """
        modules = {}
        
        # ê¸°ì¡´ 4ê°œ ëª¨ë“ˆ (ì´ë¦„ ë³€ê²½)
        modules['efficiency'] = self.calculate_efficiency_module(strategy, context, combination)
        modules['satisfaction'] = self.calculate_satisfaction_module(strategy, context, combination)
        modules['resource'] = self.calculate_resource_module(strategy, context, combination)
        modules['cognitive_alignment'] = self.calculate_alignment_module(strategy, context, combination)
        
        # R* ìƒˆë¡œìš´ ëª¨ë“ˆë“¤
        modules['emotion_adaptation'] = self.calculate_emotion_module(strategy, context)
        modules['temporal_efficiency'] = self.calculate_temporal_module(strategy, context)
        
        return modules
    
    def evolve_reward_structure(self, reward_modules: Dict[str, float], 
                               context: Dict[str, Any], strategy: str) -> Dict[str, float]:
        """
        R* ì§„í™” ì•Œê³ ë¦¬ì¦˜ìœ¼ë¡œ ë³´ìƒ êµ¬ì¡° ìµœì í™”
        
        ì§„í™” ì•Œê³ ë¦¬ì¦˜ì„ ì‚¬ìš©í•˜ì—¬ 6ê°œ ë³´ìƒ ëª¨ë“ˆì˜ ìµœì  ê°€ì¤‘ì¹˜ ì¡°í•©ì„ ì°¾ìŒ
        
        Args:
            reward_modules (Dict[str, float]): ë³´ìƒ ëª¨ë“ˆë“¤
            context (Dict): ì‚¬ìš©ì ì»¨í…ìŠ¤íŠ¸
            strategy (str): ì‹¤í–‰ ì „ëµ
            
        Returns:
            Dict[str, float]: ìµœì  ë³´ìƒ êµ¬ì¡° (ëª¨ë“ˆëª… -> ê°€ì¤‘ì¹˜)
            
        Evolution Process:
            1. ì´ˆê¸° ê°œì²´êµ° ìƒì„± (Dirichlet ë¶„í¬)
            2. ì§„í™” ë£¨í”„ (3ì„¸ëŒ€)
               - ì í•©ë„ í‰ê°€ (ì»¨í…ìŠ¤íŠ¸ + ë‹¤ì–‘ì„±)
               - ì¡°ê¸° ìˆ˜ë ´ ì²´í¬
               - ì„ íƒ, êµë°°, ë³€ì´
            3. ìµœì  êµ¬ì¡° ì„ íƒ
            4. ë©”ëª¨ë¦¬ ì •ë¦¬
        """
        
        # ì´ˆê¸° ê°œì²´êµ° ìƒì„± (ë§¤ë²ˆ ìƒˆë¡œ ìƒì„±ìœ¼ë¡œ ë©”ëª¨ë¦¬ ëˆ„ìˆ˜ ë°©ì§€)
        current_population = self.initialize_population(reward_modules)
        
        # ì§„í™” ê³¼ì •
        for generation in range(self.evolution_generations):
            # ì í•©ë„ í‰ê°€
            fitness_scores = []
            for structure in current_population:
                fitness = self.evaluate_structure_fitness(structure, reward_modules, context, strategy)
                fitness_scores.append(fitness)
            
            # ì¡°ê¸° ìˆ˜ë ´ ì²´í¬
            if generation > 1 and self.check_convergence(fitness_scores):
                break
            
            # ì„ íƒ, êµë°°, ë³€ì´
            current_population = self.evolve_population(
                current_population, fitness_scores, generation
            )
        
        # ìµœì  êµ¬ì¡° ì„ íƒ
        final_fitness = [self.evaluate_structure_fitness(s, reward_modules, context, strategy) 
                        for s in current_population]
        best_idx = np.argmax(final_fitness)
        best_structure = current_population[best_idx]
        
        # ë©”ëª¨ë¦¬ ì •ë¦¬: ì „ì—­ ê°œì²´êµ°ì„ ì£¼ê¸°ì ìœ¼ë¡œ ì—…ë°ì´íŠ¸
        self.cleanup_population()
        
        return best_structure
    
    def initialize_population(self, reward_modules: Dict[str, float]) -> List[Dict[str, float]]:
        """
        ì´ˆê¸° ë³´ìƒ êµ¬ì¡° ê°œì²´êµ° ìƒì„±
        
        Dirichlet ë¶„í¬ë¥¼ ì‚¬ìš©í•˜ì—¬ ë¬´ì‘ìœ„ë¡œ ì •ê·œí™”ëœ ê°€ì¤‘ì¹˜ ì¡°í•©ë“¤ì„ ìƒì„±
        
        Args:
            reward_modules (Dict): ë³´ìƒ ëª¨ë“ˆë“¤ (ëª¨ë“ˆ ì´ë¦„ ì¶”ì¶œìš©)
            
        Returns:
            List[Dict]: ì´ˆê¸° ê°œì²´êµ° (ê° ê°œì²´ëŠ” ëª¨ë“ˆë³„ ê°€ì¤‘ì¹˜ ë§¤í•‘)
        """
        population = []
        module_names = list(reward_modules.keys())
        
        for _ in range(self.population_size):
            # ëœë¤ ê°€ì¤‘ì¹˜ ìƒì„± (Dirichlet ë¶„í¬ ì‚¬ìš©)
            weights = np.random.dirichlet(np.ones(len(module_names)))
            structure = {name: weight for name, weight in zip(module_names, weights)}
            population.append(structure)
        
        return population
    
    def evaluate_structure_fitness(self, structure: Dict[str, float], 
                                   reward_modules: Dict[str, float], 
                                   context: Dict[str, Any], strategy: str) -> float:
        """
        ë³´ìƒ êµ¬ì¡°ì˜ ì í•©ë„ í‰ê°€
        
        ë³´ìƒ êµ¬ì¡°ì˜ ì„±ëŠ¥ì„ 3ê°€ì§€ ê¸°ì¤€ìœ¼ë¡œ í‰ê°€
        
        Args:
            structure (Dict): ë³´ìƒ êµ¬ì¡° (ëª¨ë“ˆë³„ ê°€ì¤‘ì¹˜)
            reward_modules (Dict): ë³´ìƒ ëª¨ë“ˆ ê°’ë“¤
            context (Dict): ì‚¬ìš©ì ì»¨í…ìŠ¤íŠ¸
            strategy (str): ì‹¤í–‰ ì „ëµ
            
        Returns:
            float: ì „ì²´ ì í•©ë„ (0.0-1.0 ë²”ìœ„)
            
        Fitness Components:
            1. ê°€ì¤‘í•© ë³´ìƒ (70%)
            2. ì»¨í…ìŠ¤íŠ¸ ì í•©ì„± ë³´ë„ˆìŠ¤ (25%)
            3. êµ¬ì¡° ë‹¤ì–‘ì„± ë³´ë„ˆìŠ¤ (5%)
        """
        
        # ê°€ì¤‘í•© ê³„ì‚°
        weighted_reward = sum(structure[module] * reward_modules[module] 
                             for module in structure.keys())
        
        # ì»¨í…ìŠ¤íŠ¸ ì í•©ì„± ë³´ë„ˆìŠ¤
        context_bonus = self.calculate_context_fitness(structure, context, strategy)
        
        # êµ¬ì¡° ë‹¤ì–‘ì„± ë³´ë„ˆìŠ¤ (ë„ˆë¬´ ê·¹ë‹¨ì ì´ì§€ ì•Šë„ë¡)
        diversity_bonus = self.calculate_diversity_bonus(structure)
        
        total_fitness = weighted_reward + 0.1 * context_bonus + 0.05 * diversity_bonus
        
        return np.clip(total_fitness, 0.0, 1.0)
    
    def calculate_context_fitness(self, structure: Dict[str, float], 
                                  context: Dict[str, Any], strategy: str) -> float:
        """
        ì»¨í…ìŠ¤íŠ¸ì— ë”°ë¥¸ êµ¬ì¡° ì í•©ì„±
        
        ì‚¬ìš©ìì˜ ê°ì • ë° ì¸ì§€ ìƒíƒœì— ë”°ë¼ ë³´ìƒ êµ¬ì¡°ì˜ ì í•©ì„±ì„ í‰ê°€
        
        Args:
            structure (Dict): ë³´ìƒ êµ¬ì¡°
            context (Dict): ì‚¬ìš©ì ì»¨í…ìŠ¤íŠ¸
            strategy (str): ì‹¤í–‰ ì „ëµ
            
        Returns:
            float: ì»¨í…ìŠ¤íŠ¸ ì í•©ì„± ì ìˆ˜
            
        Context Preferences:
            - ë¶€ì •ì  ê°ì •: satisfaction, emotion_adaptation ëª¨ë“ˆ ì„ í˜¸
            - ê¸ì •ì  ê°ì •: efficiency, temporal_efficiency ëª¨ë“ˆ ì„ í˜¸
            - ë†’ì€ ì¸ì§€ë¶€í•˜: cognitive_alignment, resource ëª¨ë“ˆ ì„ í˜¸
        """
        
        emotion = context.get('emotion', 'neutral')
        cognitive_load = context.get('cognitive_load_level', 'medium')
        
        fitness = 0.0
        
        # ê°ì •ë³„ ëª¨ë“ˆ ê°€ì¤‘ì¹˜ ì„ í˜¸ë„
        if emotion in ['sad', 'fear']:
            if structure.get('satisfaction', 0) > 0.3:
                fitness += 0.2
            if structure.get('emotion_adaptation', 0) > 0.3:
                fitness += 0.3
        elif emotion in ['happy', 'surprise']:
            if structure.get('efficiency', 0) > 0.3:
                fitness += 0.2
            if structure.get('temporal_efficiency', 0) > 0.2:
                fitness += 0.1
        
        # ì¸ì§€ ë¶€í•˜ë³„ ëª¨ë“ˆ ì„ í˜¸ë„
        if cognitive_load == 'high':
            if structure.get('cognitive_alignment', 0) > 0.4:
                fitness += 0.3
            if structure.get('resource', 0) > 0.3:
                fitness += 0.2
        elif cognitive_load == 'low':
            if structure.get('efficiency', 0) > 0.4:
                fitness += 0.2
        
        return fitness
    
    def calculate_diversity_bonus(self, structure: Dict[str, float]) -> float:
        """
        êµ¬ì¡° ë‹¤ì–‘ì„± ë³´ë„ˆìŠ¤
        
        ê·¹ë‹¨ì ìœ¼ë¡œ í•œ ëª¨ë“ˆì—ë§Œ ì§‘ì¤‘ë˜ëŠ” ê²ƒì„ ë°©ì§€í•˜ê¸° ìœ„í•´ ë‹¤ì–‘ì„± ì§€í‘œ ê³„ì‚°
        ì—”íŠ¸ë¡œí”¼ë¥¼ ì‚¬ìš©í•˜ì—¬ ê°€ì¤‘ì¹˜ ë¶„í¬ì˜ ê· ë“±ì„± ì¸¡ì •
        
        Args:
            structure (Dict): ë³´ìƒ êµ¬ì¡°
            
        Returns:
            float: ë‹¤ì–‘ì„± ì ìˆ˜ (0.0-1.0, 1.0ì´ ìµœëŒ€ ë‹¤ì–‘ì„±)
        """
        weights = list(structure.values())
        
        # ì—”íŠ¸ë¡œí”¼ ê³„ì‚° (ë†’ì„ìˆ˜ë¡ ë‹¤ì–‘ì„± ìˆìŒ)
        entropy = -sum(w * np.log(w + 1e-8) for w in weights)
        max_entropy = -np.log(1.0 / len(weights))  # ê· ë“± ë¶„í¬ì¼ ë•Œ ìµœëŒ€ ì—”íŠ¸ë¡œí”¼
        
        diversity_score = entropy / max_entropy
        return diversity_score
    
    def check_convergence(self, fitness_scores: List[float]) -> bool:
        """
        ìˆ˜ë ´ ì—¬ë¶€ í™•ì¸
        
        ìƒìœ„ 25% ê°œì²´ë“¤ì˜ í‰ê·  ì í•©ë„ ê°œì„ ë„ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ìˆ˜ë ´ íŒë‹¨
        ê°œì„ ë„ê°€ ì„ê³„ê°’ ì´í•˜ë©´ ì¡°ê¸° ì¢…ë£Œ
        
        Args:
            fitness_scores (List[float]): í˜„ì¬ ì„¸ëŒ€ì˜ ì í•©ë„ ì ìˆ˜ë“¤
            
        Returns:
            bool: ìˆ˜ë ´ ì—¬ë¶€ (True: ìˆ˜ë ´, False: ê³„ì† ì§„í™”)
        """
        if len(fitness_scores) < 2:
            return False
        
        # ìƒìœ„ 25% í‰ê·  ê°œì„ ë„ í™•ì¸
        top_25_percent = int(len(fitness_scores) * 0.25) or 1
        current_top = np.mean(sorted(fitness_scores, reverse=True)[:top_25_percent])
        
        # ì´ì „ ì„¸ëŒ€ì™€ ë¹„êµ (ê°„ë‹¨í•˜ê²Œ í˜„ì¬ ìµœëŒ€ê°’ìœ¼ë¡œ ë¹„êµ)
        if hasattr(self, '_previous_best_fitness'):
            improvement = current_top - self._previous_best_fitness
            if improvement < self.convergence_threshold:
                return True
        
        self._previous_best_fitness = current_top
        return False
    
    def evolve_population(self, population: List[Dict[str, float]], 
                         fitness_scores: List[float], generation: int) -> List[Dict[str, float]]:
        """
        ê°œì²´êµ° ì§„í™”
        
        ì„ íƒ, êµë°°, ë³€ì´ ì—°ì‚°ì„ í†µí•´ ë‹¤ìŒ ì„¸ëŒ€ ê°œì²´êµ°ì„ ìƒì„±
        
        Args:
            population (List[Dict]): í˜„ì¬ ê°œì²´êµ°
            fitness_scores (List[float]): ì í•©ë„ ì ìˆ˜ë“¤
            generation (int): í˜„ì¬ ì„¸ëŒ€ ë²ˆí˜¸
            
        Returns:
            List[Dict]: ë‹¤ìŒ ì„¸ëŒ€ ê°œì²´êµ°
            
        Evolution Steps:
            1. ì—˜ë¦¬íŠ¸ ë³´ì¡´ (ìƒìœ„ 20%)
            2. ë‚˜ë¨¸ì§€ëŠ” ì„ íƒ-êµë°°-ë³€ì´ë¡œ ìƒì„±
        """
        
        new_population = []
        
        # ì—˜ë¦¬íŠ¸ ë³´ì¡´ (ìƒìœ„ 20%)
        elite_indices = np.argsort(fitness_scores)[-self.elite_size:]
        for idx in elite_indices:
            new_population.append(population[idx].copy())
        
        # ë‚˜ë¨¸ì§€ëŠ” ì„ íƒ, êµë°°, ë³€ì´ë¡œ ìƒì„±
        while len(new_population) < self.population_size:
            # í† ë„ˆë¨¼íŠ¸ ì„ íƒ
            parent1 = self.tournament_selection(population, fitness_scores)
            parent2 = self.tournament_selection(population, fitness_scores)
            
            # êµë°°
            child = self.crossover(parent1, parent2)
            
            # ë³€ì´
            child = self.mutate(child, generation)
            
            new_population.append(child)
        
        return new_population
    
    def tournament_selection(self, population: List[Dict[str, float]], 
                            fitness_scores: List[float]) -> Dict[str, float]:
        """
        í† ë„ˆë¨¼íŠ¸ ì„ íƒ
        
        ë¬´ì‘ìœ„ë¡œ ì„ íƒëœ ê°œì²´ë“¤ ì¤‘ì—ì„œ ê°€ì¥ ì í•©ë„ê°€ ë†’ì€ ê°œì²´ë¥¼ ì„ íƒ
        
        Args:
            population (List[Dict]): ê°œì²´êµ°
            fitness_scores (List[float]): ì í•©ë„ ì ìˆ˜ë“¤
            
        Returns:
            Dict: ì„ íƒëœ ê°œì²´ (ë³µì‚¬ë³¸)
        """
        tournament_indices = np.random.choice(len(population), self.tournament_size, replace=False)
        tournament_fitness = [fitness_scores[i] for i in tournament_indices]
        winner_idx = tournament_indices[np.argmax(tournament_fitness)]
        
        return population[winner_idx].copy()
    
    def crossover(self, parent1: Dict[str, float], parent2: Dict[str, float]) -> Dict[str, float]:
        """
        êµë°° (ê°€ì¤‘ í‰ê· )
        
        ë‘ ë¶€ëª¨ ê°œì²´ì˜ ê°€ì¤‘ì¹˜ë¥¼ ì„ í˜• ê²°í•©í•˜ì—¬ ìì‹ ìƒì„±
        
        Args:
            parent1 (Dict): ë¶€ëª¨ 1
            parent2 (Dict): ë¶€ëª¨ 2
            
        Returns:
            Dict: ìì‹ ê°œì²´ (ì •ê·œí™”ëœ ê°€ì¤‘ì¹˜)
        """
        child = {}
        for key in parent1.keys():
            child[key] = self.crossover_alpha * parent1[key] + (1 - self.crossover_alpha) * parent2[key]
        
        # ê°€ì¤‘ì¹˜ ì •ê·œí™”
        total_weight = sum(child.values())
        for key in child:
            child[key] /= total_weight
        
        return child
    
    def mutate(self, individual: Dict[str, float], generation: int) -> Dict[str, float]:
        """
        ë³€ì´ (ì ì‘ì  ë³€ì´ìœ¨)
        
        ê°€ìš°ì‹œì•ˆ ë…¸ì´ì¦ˆë¥¼ ì¶”ê°€í•˜ì—¬ ê°œì²´ë¥¼ ë³€ì´ì‹œí‚´
        ë³€ì´ìœ¨ì€ ì„¸ëŒ€ì— ë”°ë¼ ì ì‘ì ìœ¼ë¡œ ì¡°ì • (ì´ˆê¸° ë†’ìŒ -> í›„ë°˜ ë‚®ìŒ)
        
        Args:
            individual (Dict): ë³€ì´ì‹œí‚¬ ê°œì²´
            generation (int): í˜„ì¬ ì„¸ëŒ€ ë²ˆí˜¸
            
        Returns:
            Dict: ë³€ì´ëœ ê°œì²´ (ì •ê·œí™”ëœ ê°€ì¤‘ì¹˜)
        """
        mutated = individual.copy()
        
        # ì ì‘ì  ë³€ì´ìœ¨ (ì´ˆê¸°ì—ëŠ” ë†’ê²Œ, í›„ë°˜ì—ëŠ” ë‚®ê²Œ)
        mutation_rate = self.adaptive_mutation_rate(generation)
        
        for key in mutated:
            if np.random.random() < mutation_rate:
                # ê°€ìš°ì‹œì•ˆ ë…¸ì´ì¦ˆ ì¶”ê°€
                noise = np.random.normal(0, 0.05)
                mutated[key] = max(0.01, mutated[key] + noise)
        
        # ê°€ì¤‘ì¹˜ ì¬ì •ê·œí™”
        total_weight = sum(mutated.values())
        for key in mutated:
            mutated[key] /= total_weight
        
        return mutated
    
    def adaptive_mutation_rate(self, generation: int) -> float:
        """
        ì ì‘ì  ë³€ì´ìœ¨ ê³„ì‚°
        
        ì„¸ëŒ€ì— ë”°ë¼ ë³€ì´ìœ¨ì„ ì¡°ì •í•˜ì—¬ ì´ˆê¸°ì—ëŠ” íƒìƒ‰ì„, í›„ë°˜ì—ëŠ” í™œìš©ì„ ê°•í™”
        
        Args:
            generation (int): í˜„ì¬ ì„¸ëŒ€ ë²ˆí˜¸
            
        Returns:
            float: ì ì‘ëœ ë³€ì´ìœ¨ (0.05-0.15 ë²”ìœ„)
        """
        # ì´ˆê¸°ì—ëŠ” ë†’ê²Œ, í›„ë°˜ì—ëŠ” ë‚®ê²Œ
        return 0.15 * (1 - generation / self.evolution_generations) + 0.05
    
    def calculate_evolved_reward(self, reward_modules: Dict[str, float], 
                                optimal_structure: Dict[str, float]) -> float:
        """
        ì§„í™”ëœ êµ¬ì¡°ë¡œ ìµœì¢… ë³´ìƒ ê³„ì‚°
        
        ìµœì í™”ëœ ê°€ì¤‘ì¹˜ë¥¼ ì‚¬ìš©í•˜ì—¬ 6ê°œ ëª¨ë“ˆì˜ ê°€ì¤‘ í•©ì„ ê³„ì‚°
        
        Args:
            reward_modules (Dict[str, float]): ë³´ìƒ ëª¨ë“ˆ ê°’ë“¤
            optimal_structure (Dict[str, float]): ìµœì  ê°€ì¤‘ì¹˜ êµ¬ì¡°
            
        Returns:
            float: ìµœì¢… ë³´ìƒ ê°’ (0.0-1.0 ë²”ìœ„)
        """
        
        final_reward = sum(optimal_structure[module] * reward_modules[module] 
                          for module in optimal_structure.keys())
        
        return final_reward
    
    def calculate_emotion_module(self, strategy: str, context: Dict[str, Any]) -> float:
        """
        R* ìƒˆë¡œìš´ ëª¨ë“ˆ: ê°ì • ì ì‘
        
        ì‚¬ìš©ìì˜ ê°ì • ìƒíƒœì— ë”°ë¥¸ ì „ëµì˜ ì í•©ì„±ì„ í‰ê°€
        ê° ì „ëµì´ íŠ¹ì • ê°ì •ì— ì–¼ë§ˆë‚˜ ì˜ ë§ëŠ”ì§€ ì¸¡ì •
        
        Args:
            strategy (str): ì‹¤í–‰ ì „ëµ
            context (Dict): ì‚¬ìš©ì ì»¨í…ìŠ¤íŠ¸
            
        Returns:
            float: ê°ì • ì ì‘ ì ìˆ˜ (0.0-1.0)
        """
        emotion = context.get('emotion', 'neutral')
        
        emotion_strategy_scores = {
            'gentle_adaptive': {'sad': 0.9, 'fear': 0.8, 'anger': 0.7, 'disgust': 0.6},
            'intensive_focused': {'happy': 0.8, 'surprise': 0.7, 'neutral': 0.5},
            'energetic_dynamic': {'happy': 0.9, 'surprise': 0.8, 'neutral': 0.6},
            'supportive_gradual': {'sad': 0.8, 'fear': 0.9, 'disgust': 0.7, 'anger': 0.6},
            'careful_methodical': {'anger': 0.8, 'fear': 0.7, 'disgust': 0.6, 'neutral': 0.7},
            'balanced_standard': {'neutral': 0.8, 'happy': 0.6, 'sad': 0.6},
            'optimized_efficient': {'neutral': 0.7, 'happy': 0.6, 'surprise': 0.5}
        }
        
        return emotion_strategy_scores.get(strategy, {}).get(emotion, 0.5)
    
    def calculate_temporal_module(self, strategy: str, context: Dict[str, Any]) -> float:
        """
        R* ìƒˆë¡œìš´ ëª¨ë“ˆ: ì‹œê°„ íš¨ìœ¨ì„±
        
        ì‚¬ìš©ìì˜ ì£¼ì˜ì§‘ì¤‘ë„ì™€ ì¸ì§€ë¶€í•˜ë¥¼ ê³ ë ¤í•œ ì‹œê°„ íš¨ìœ¨ì„± í‰ê°€
        ì „ëµë³„ ì‹œê°„ íš¨ìœ¨ì„± ê°€ì¤‘ì¹˜ë¥¼ ì ìš©
        
        Args:
            strategy (str): ì‹¤í–‰ ì „ëµ
            context (Dict): ì‚¬ìš©ì ì»¨í…ìŠ¤íŠ¸
            
        Returns:
            float: ì‹œê°„ íš¨ìœ¨ì„± ì ìˆ˜ (0.0-1.0+)
            
        Formula:
            temporal_score = attention Ã— cognitive_load_factor Ã— strategy_multiplier
        """
        cognitive_load = context.get('cognitive_load_level', 'medium')
        attention = context.get('attention', 0.5)
        
        # ì‹œê°„ íš¨ìœ¨ì„± = ì£¼ì˜ì§‘ì¤‘ë„ Ã— ì¸ì§€ë¶€í•˜ ì—­í•¨ìˆ˜
        load_factor = {'low': 1.0, 'medium': 0.7, 'high': 0.4}.get(cognitive_load, 0.7)
        temporal_score = attention * load_factor
        
        # ì „ëµë³„ ì‹œê°„ íš¨ìœ¨ì„± ì¡°ì •
        strategy_multipliers = {
            'optimized_efficient': 1.2,
            'intensive_focused': 1.1,
            'energetic_dynamic': 1.0,
            'balanced_standard': 0.9,
            'gentle_adaptive': 0.8,
            'supportive_gradual': 0.7,
            'careful_methodical': 0.6
        }
        
        return temporal_score * strategy_multipliers.get(strategy, 1.0)
    
    def hash_context(self, context: Dict[str, Any], strategy: str, 
                    meta_strategy: str, adaptation_type: str) -> str:
        """
        ì»¨í…ìŠ¤íŠ¸ í•´ì‹œ ìƒì„±
        
        ìºì‹±ì„ ìœ„í•´ ì‚¬ìš©ì ì»¨í…ìŠ¤íŠ¸ì™€ ì „ëµ ì •ë³´ë¥¼ ë¬¸ìì—´ í•´ì‹œë¡œ ë³€í™˜
        
        Args:
            context (Dict): ì‚¬ìš©ì ì»¨í…ìŠ¤íŠ¸
            strategy (str): ì‹¤í–‰ ì „ëµ
            meta_strategy (str): ë©”íƒ€ ì „ëµ
            adaptation_type (str): ì ì‘ íƒ€ì…
            
        Returns:
            str: ì»¨í…ìŠ¤íŠ¸ í•´ì‹œ ë¬¸ìì—´
        """
        emotion = context.get('emotion', 'neutral')
        cognitive_load = context.get('cognitive_load_level', 'medium')
        attention = round(context.get('attention', 0.5), 1)  # 0.1 ë‹¨ìœ„ë¡œ ë°˜ì˜¬ë¦¼
        
        # í•´ì‹œ ë¬¸ìì—´ ìƒì„±
        hash_str = f"{strategy}_{meta_strategy}_{adaptation_type}_{emotion}_{cognitive_load}_{attention}"
        return hash_str
    
    def get_cached_structure(self, context_hash: str) -> Optional[Dict[str, float]]:
        """
        ìºì‹œëœ êµ¬ì¡° ê²€ìƒ‰
        
        ì£¼ì–´ì§„ ì»¨í…ìŠ¤íŠ¸ í•´ì‹œì— ëŒ€í•´ ìºì‹œëœ ë³´ìƒ êµ¬ì¡°ë¥¼ ì°¾ìŒ
        ì •í™•í•œ ë§¤ì¹˜ê°€ ì—†ìœ¼ë©´ ìœ ì‚¬í•œ ì»¨í…ìŠ¤íŠ¸ë¥¼ ì°¾ìŒ
        
        Args:
            context_hash (str): ì»¨í…ìŠ¤íŠ¸ í•´ì‹œ
            
        Returns:
            Dict or None: ìºì‹œëœ ë³´ìƒ êµ¬ì¡° ë˜ëŠ” None
        """
        # ì •í™•í•œ ë§¤ì¹˜ ë¨¼ì € í™•ì¸
        if context_hash in self.structure_cache:
            return self.structure_cache[context_hash]
        
        # ìœ ì‚¬í•œ ì»¨í…ìŠ¤íŠ¸ ê²€ìƒ‰
        for cached_hash, structure in self.structure_cache.items():
            similarity = self.calculate_context_similarity(context_hash, cached_hash)
            if similarity > self.cache_hit_threshold:
                return structure
        
        return None
    
    def calculate_context_similarity(self, hash1: str, hash2: str) -> float:
        """
        ì»¨í…ìŠ¤íŠ¸ ìœ ì‚¬ë„ ê³„ì‚°
        
        ë‘ ì»¨í…ìŠ¤íŠ¸ í•´ì‹œ ê°„ì˜ ìœ ì‚¬ë„ë¥¼ 0.0-1.0 ë²”ìœ„ë¡œ ê³„ì‚°
        ë¬¸ìì—´ ìš”ì†Œë“¤ì„ ë¹„êµí•˜ì—¬ ì¼ì¹˜ë„ë¥¼ ì¸¡ì •
        
        Args:
            hash1 (str): ì»¨í…ìŠ¤íŠ¸ í•´ì‹œ 1
            hash2 (str): ì»¨í…ìŠ¤íŠ¸ í•´ì‹œ 2
            
        Returns:
            float: ìœ ì‚¬ë„ (0.0-1.0, 1.0ì´ ì™„ì „ ì¼ì¹˜)
        """
        parts1 = hash1.split('_')
        parts2 = hash2.split('_')
        
        if len(parts1) != len(parts2):
            return 0.0
        
        matches = 0
        for p1, p2 in zip(parts1, parts2):
            if p1 == p2:
                matches += 1
            elif p1.replace('.', '').isdigit() and p2.replace('.', '').isdigit():
                # ìˆ«ì ë¹„êµ (ì£¼ì˜ì§‘ì¤‘ë„)
                diff = abs(float(p1) - float(p2))
                if diff <= self.Config.ATTENTION_SIMILARITY_THRESHOLD:
                    matches += 0.8
        
        similarity = matches / len(parts1)
        return similarity
    
    def cache_structure(self, context_hash: str, structure: Dict[str, float]) -> None:
        """
        êµ¬ì¡° ìºì‹±
        
        ì§„í™”ëœ ë³´ìƒ êµ¬ì¡°ë¥¼ ìºì‹œì— ì €ì¥
        ìºì‹œ í¬ê¸° ì œí•œì„ ì´ˆê³¼í•˜ë©´ LRU ë°©ì‹ìœ¼ë¡œ ì˜¤ë˜ëœ í•­ëª© ì œê±°
        
        Args:
            context_hash (str): ì»¨í…ìŠ¤íŠ¸ í•´ì‹œ
            structure (Dict): ë³´ìƒ êµ¬ì¡° (ë³µì‚¬ë˜ì–´ ì €ì¥)
        """
        # ìºì‹œ í¬ê¸° ì œí•œ
        if len(self.structure_cache) >= self.max_cache_size:
            # LRU: ê°€ì¥ ì˜¤ë˜ëœ í•­ëª© ì œê±°
            oldest_key = next(iter(self.structure_cache))
            del self.structure_cache[oldest_key]
        
        self.structure_cache[context_hash] = structure.copy()
    
    def get_cache_stats(self) -> Dict[str, Union[int, float]]:
        """
        ìºì‹œ í†µê³„ ë°˜í™˜
        
        R* ìºì‹± ì‹œìŠ¤í…œì˜ ì„±ëŠ¥ ì§€í‘œë¥¼ ë°˜í™˜
        
        Returns:
            Dict: ìºì‹œ í†µê³„ ì •ë³´
                - cache_hits: ìºì‹œ íˆíŠ¸ íšŸìˆ˜
                - cache_misses: ìºì‹œ ë¯¸ìŠ¤ íšŸìˆ˜
                - hit_rate: ìºì‹œ íˆíŠ¸ìœ¨ (0.0-1.0)
                - cache_size: í˜„ì¬ ìºì‹œ í¬ê¸°
        """
        total_requests = self.cache_hits + self.cache_misses
        hit_rate = self.cache_hits / total_requests if total_requests > 0 else 0.0
        
        return {
            'cache_hits': self.cache_hits,
            'cache_misses': self.cache_misses,
            'hit_rate': hit_rate,
            'cache_size': len(self.structure_cache)
        }
    
    def cleanup_population(self) -> None:
        """
        ë©”ëª¨ë¦¬ ê´€ë¦¬: ê°œì²´êµ° ì£¼ê¸°ì  ì •ë¦¬
        
        ë©”ëª¨ë¦¬ ëˆ„ìˆ˜ë¥¼ ë°©ì§€í•˜ê¸° ìœ„í•´ ì£¼ê¸°ì ìœ¼ë¡œ ê°œì²´êµ° í¬ê¸°ë¥¼ ì œí•œí•˜ê³  ê°€ë¹„ì§€ ì»¨ë ‰ì…˜ ìˆ˜í–‰
        
        Cleanup Strategy:
            - ë§¤ 10ë²ˆì§¸ ì§„í™”ë§ˆë‹¤ ì‹¤í–‰
            - ê°œì²´êµ° í¬ê¸°ë¥¼ ì„¤ì •ëœ í¬ê¸°ë¡œ ì œí•œ
            - Python ê°€ë¹„ì§€ ì»¨ë ‰ì…˜ í˜¸ì¶œ
        """
        # ì „ì—­ ê°œì²´êµ°ì„ ì£¼ê¸°ì ìœ¼ë¡œ ì—…ë°ì´íŠ¸ (ë§¤ 10ë²ˆì§¸ ì§„í™”ë§ˆë‹¤)
        if not hasattr(self, '_evolution_count'):
            self._evolution_count = 0
        
        self._evolution_count += 1
        
        # ì„¤ì •ëœ ê°„ê²©ë§ˆë‹¤ ì „ì—­ ê°œì²´êµ° ì—…ë°ì´íŠ¸
        if self._evolution_count % self.Config.POPULATION_CLEANUP_INTERVAL == 0:
            # ìµœê·¼ ì§„í™” ê²°ê³¼ë¡œ ì „ì—­ ê°œì²´êµ° ì—…ë°ì´íŠ¸
            if len(self.reward_structure_population) > self.population_size:
                # í¬ê¸° ì œí•œ: ìƒìœ„ population_sizeê°œë§Œ ìœ ì§€
                self.reward_structure_population = self.reward_structure_population[:self.population_size]
            
            # ë©”ëª¨ë¦¬ ì •ë¦¬ í˜¸ì¶œ
            import gc
            gc.collect()
    
    def get_memory_stats(self) -> Dict[str, Union[int, float]]:
        """
        ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ í†µê³„
        
        R* ì‹œìŠ¤í…œì˜ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì„ ì¶”ì •í•˜ì—¬ ë°˜í™˜
        
        Returns:
            Dict: ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì •ë³´
                - population_count: ê°œì²´êµ° í¬ê¸°
                - cache_count: ìºì‹œ í¬ê¸°
                - estimated_population_memory_bytes: ê°œì²´êµ° ë©”ëª¨ë¦¬ ì˜ˆìƒ í¬ê¸°
                - estimated_cache_memory_bytes: ìºì‹œ ë©”ëª¨ë¦¬ ì˜ˆìƒ í¬ê¸°
                - evolution_count: ì§„í™” ì‹¤í–‰ íšŸìˆ˜
        """
        import sys
        
        population_size = len(self.reward_structure_population)
        cache_size = len(self.structure_cache)
        
        # ê°œì²´êµ° ë©”ëª¨ë¦¬ ì˜ˆìƒ í¬ê¸° (ê° êµ¬ì¡°ë‹¹ ì•½ 6ê°œ ëª¨ë“ˆ)
        estimated_population_memory = population_size * 6 * sys.getsizeof(0.5)  # float í¬ê¸°
        estimated_cache_memory = cache_size * 6 * sys.getsizeof(0.5)
        
        return {
            'population_count': population_size,
            'cache_count': cache_size,
            'estimated_population_memory_bytes': estimated_population_memory,
            'estimated_cache_memory_bytes': estimated_cache_memory,
            'evolution_count': getattr(self, '_evolution_count', 0)
        }
    
    def fallback_reward(self, strategy: str, context: Dict[str, Any]) -> float:
        """
        ì—ëŸ¬ ë°œìƒ ì‹œ í´ë°± ë³´ìƒ ê³„ì‚°
        
        R* ì§„í™” ì•Œê³ ë¦¬ì¦˜ì—ì„œ ì—ëŸ¬ê°€ ë°œìƒí–ˆì„ ë•Œ ì‚¬ìš©í•  ê°„ë‹¨í•œ íœ´ë¦¬ìŠ¤í‹± ë³´ìƒ
        
        Args:
            strategy (str): ì‹¤í–‰ ì „ëµ
            context (Dict): ì‚¬ìš©ì ì»¨í…ìŠ¤íŠ¸
            
        Returns:
            float: í´ë°± ë³´ìƒ ê°’ (0.0-1.0 ë²”ìœ„)
            
        Fallback Strategy:
            1. ì „ëµë³„ ê¸°ë³¸ ì ìˆ˜ ì‚¬ìš©
            2. ê°„ë‹¨í•œ ì»¨í…ìŠ¤íŠ¸ ì¡°ì • ì ìš©
            3. ìµœì¢… ì‹¤íŒ¨ ì‹œ ê³ ì •ê°’ ë°˜í™˜
        """
        try:
            # ê°„ë‹¨í•œ íœ´ë¦¬ìŠ¤í‹± ê¸°ë°˜ ë³´ìƒ
            emotion = context.get('emotion', 'neutral')
            cognitive_load = context.get('cognitive_load_level', 'medium')
            
            # ì „ëµë³„ ê¸°ë³¸ ì ìˆ˜
            base_scores = {
                'gentle_adaptive': 0.6,
                'intensive_focused': 0.7,
                'balanced_standard': 0.65,
                'careful_methodical': 0.6,
                'energetic_dynamic': 0.7,
                'supportive_gradual': 0.6,
                'optimized_efficient': 0.75
            }
            
            base_reward = base_scores.get(strategy, self.Config.FALLBACK_REWARD_DEFAULT)
            
            # ê°„ë‹¨í•œ ì»¨í…ìŠ¤íŠ¸ ì¡°ì •
            if emotion in ['sad', 'fear'] and strategy in ['supportive_gradual', 'gentle_adaptive']:
                base_reward += self.Config.CONTEXT_BONUS
            elif emotion in ['happy', 'surprise'] and strategy in ['energetic_dynamic', 'intensive_focused']:
                base_reward += self.Config.CONTEXT_BONUS
            
            if cognitive_load == 'high' and strategy in ['gentle_adaptive', 'supportive_gradual']:
                base_reward += self.Config.COGNITIVE_BONUS
            
            return np.clip(base_reward, 0.0, 1.0)
            
        except Exception:
            # ìµœì¢… í´ë°±: ê³ ì •ê°’
            return self.Config.FALLBACK_REWARD_DEFAULT
    
    def calculate_efficiency_module(self, strategy: str, context: Dict[str, Any], 
                                   combination: Tuple[str, str, str]) -> float:
        """
        íš¨ìœ¨ì„± ëª¨ë“ˆ
        
        ì „ëµì˜ ê¸°ë³¸ íš¨ìœ¨ì„±ì„ í‰ê°€í•˜ê³  ì‚¬ìš©ìì˜ ì¸ì§€ ìƒíƒœì— ë”°ë¼ ì¡°ì •
        
        Args:
            strategy (str): ì‹¤í–‰ ì „ëµ
            context (Dict): ì‚¬ìš©ì ì»¨í…ìŠ¤íŠ¸
            combination (Tuple): ì—ì´ì „íŠ¸ ì¡°í•©
            
        Returns:
            float: íš¨ìœ¨ì„± ì ìˆ˜ (0.0-1.0)
        """
        cognitive_load = context.get('cognitive_load_level', 'medium')
        attention = context.get('attention', 0.5)
        
        # ì „ëµë³„ ê¸°ë³¸ íš¨ìœ¨ì„±
        efficiency_scores = {
            'optimized_efficient': 0.9,
            'intensive_focused': 0.8,
            'balanced_standard': 0.7,
            'gentle_adaptive': 0.6,
            'supportive_gradual': 0.5,
            'careful_methodical': 0.6,
            'energetic_dynamic': 0.7
        }
        
        base_efficiency = efficiency_scores.get(strategy, 0.5)
        
        # ì¸ì§€ë¶€í•˜ ê¸°ë°˜ ì¡°ì •
        if cognitive_load == 'low' and strategy in ['optimized_efficient', 'intensive_focused']:
            base_efficiency += 0.2
        elif cognitive_load == 'high' and strategy in ['gentle_adaptive', 'supportive_gradual']:
            base_efficiency += 0.15
        
        # ì£¼ì˜ì§‘ì¤‘ ê¸°ë°˜ ì¡°ì •
        if attention > 0.8 and strategy in ['intensive_focused', 'energetic_dynamic']:
            base_efficiency += 0.1
        elif attention < 0.4 and strategy in ['gentle_adaptive', 'careful_methodical']:
            base_efficiency += 0.1
        
        return np.clip(base_efficiency, 0.0, 1.0)
    
    def calculate_satisfaction_module(self, strategy: str, context: Dict[str, Any], 
                                     combination: Tuple[str, str, str]) -> float:
        """
        ë§Œì¡±ë„ ëª¨ë“ˆ
        
        ì‚¬ìš©ìì˜ ê°ì • ìƒíƒœì— ë”°ë¥¸ ì „ëµ ì„ í˜¸ë„ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë§Œì¡±ë„ í‰ê°€
        
        Args:
            strategy (str): ì‹¤í–‰ ì „ëµ
            context (Dict): ì‚¬ìš©ì ì»¨í…ìŠ¤íŠ¸
            combination (Tuple): ì—ì´ì „íŠ¸ ì¡°í•©
            
        Returns:
            float: ë§Œì¡±ë„ ì ìˆ˜ (0.0-1.0)
        """
        emotion = context.get('emotion', 'neutral')
        
        # ê°ì •ë³„ ì„ í˜¸ ì „ëµ
        emotion_preferences = {
            'happy': ['energetic_dynamic', 'intensive_focused', 'optimized_efficient'],
            'surprise': ['energetic_dynamic', 'balanced_standard'],
            'sad': ['supportive_gradual', 'gentle_adaptive', 'careful_methodical'],
            'fear': ['supportive_gradual', 'gentle_adaptive'],
            'anger': ['careful_methodical', 'balanced_standard'],
            'disgust': ['careful_methodical', 'gentle_adaptive'],
            'neutral': ['balanced_standard', 'optimized_efficient']
        }
        
        preferred_strategies = emotion_preferences.get(emotion, ['balanced_standard'])
        
        if strategy in preferred_strategies:
            satisfaction = 0.8 + (preferred_strategies.index(strategy) * -0.1)  # ì²« ë²ˆì§¸ê°€ ê°€ì¥ ë†’ìŒ
        else:
            satisfaction = 0.4
        
        return np.clip(satisfaction, 0.0, 1.0)
    
    def calculate_resource_module(self, strategy: str, context: Dict[str, Any], 
                                 combination: Tuple[str, str, str]) -> float:
        """
        ìì› ìµœì í™” ëª¨ë“ˆ
        
        ì „ëµì˜ ìì› ì‚¬ìš©ëŸ‰ì„ í‰ê°€í•˜ê³  ì‚¬ìš©ìì˜ ì¸ì§€ë¶€í•˜ì— ë”°ë¼ ì¡°ì •
        
        Args:
            strategy (str): ì‹¤í–‰ ì „ëµ
            context (Dict): ì‚¬ìš©ì ì»¨í…ìŠ¤íŠ¸
            combination (Tuple): ì—ì´ì „íŠ¸ ì¡°í•©
            
        Returns:
            float: ìì› ìµœì í™” ì ìˆ˜ (0.0-1.0)
        """
        cognitive_load = context.get('cognitive_load_level', 'medium')
        
        # ì „ëµë³„ ìì› ì‚¬ìš©ëŸ‰ (ë‚®ì„ìˆ˜ë¡ ì¢‹ìŒ)
        resource_usage = {
            'gentle_adaptive': 0.3,
            'supportive_gradual': 0.4,
            'balanced_standard': 0.5,
            'careful_methodical': 0.6,
            'optimized_efficient': 0.7,
            'energetic_dynamic': 0.8,
            'intensive_focused': 0.9
        }
        
        usage = resource_usage.get(strategy, 0.5)
        
        # ì¸ì§€ë¶€í•˜ê°€ ë†’ì„ ë•ŒëŠ” ë‚®ì€ ìì› ì‚¬ìš© ì„ í˜¸
        if cognitive_load == 'high':
            resource_score = 1.0 - usage  # ì‚¬ìš©ëŸ‰ì´ ë‚®ì„ìˆ˜ë¡ ë†’ì€ ì ìˆ˜
        else:
            resource_score = 0.5 + (usage * 0.5)  # ì ì ˆí•œ ìì› ì‚¬ìš© ì„ í˜¸
        
        return np.clip(resource_score, 0.0, 1.0)
    
    def calculate_alignment_module(self, strategy: str, context: Dict[str, Any], 
                                  combination: Tuple[str, str, str]) -> float:
        """
        ì¸ì§€ ì •ë ¬ ëª¨ë“ˆ
        
        ì‚¬ìš©ìì˜ ì¸ì§€ ìƒíƒœ(ì¸ì§€ë¶€í•˜, ì£¼ì˜ì§‘ì¤‘ë„, ê°ì •)ì™€ ì „ëµ ê°„ì˜ ì •ë ¬ë„ í‰ê°€
        
        Args:
            strategy (str): ì‹¤í–‰ ì „ëµ
            context (Dict): ì‚¬ìš©ì ì»¨í…ìŠ¤íŠ¸
            combination (Tuple): ì—ì´ì „íŠ¸ ì¡°í•©
            
        Returns:
            float: ì¸ì§€ ì •ë ¬ ì ìˆ˜ (0.0-1.0)
        """
        cognitive_load = context.get('cognitive_load_level', 'medium')
        attention = context.get('attention', 0.5)
        emotion = context.get('emotion', 'neutral')
        
        # ë‹¤ì°¨ì› ì •ë ¬ ì ìˆ˜
        load_alignment = self.get_load_strategy_alignment(cognitive_load, strategy)
        attention_alignment = self.get_attention_strategy_alignment(attention, strategy)
        emotion_alignment = self.get_emotion_strategy_alignment(emotion, strategy)
        
        # ê°€ì¤‘ í‰ê· 
        cognitive_alignment = (load_alignment * 0.4 + attention_alignment * 0.3 + emotion_alignment * 0.3)
        
        return np.clip(cognitive_alignment, 0.0, 1.0)
    
    def get_load_strategy_alignment(self, cognitive_load, strategy):
        """ì¸ì§€ë¶€í•˜-ì „ëµ ì •ë ¬"""
        alignment_matrix = {
            ('high', 'gentle_adaptive'): 0.9,
            ('high', 'supportive_gradual'): 0.8,
            ('high', 'careful_methodical'): 0.7,
            ('medium', 'balanced_standard'): 0.9,
            ('medium', 'optimized_efficient'): 0.7,
            ('low', 'intensive_focused'): 0.9,
            ('low', 'energetic_dynamic'): 0.8,
            ('low', 'optimized_efficient'): 0.8
        }
        return alignment_matrix.get((cognitive_load, strategy), 0.5)
    
    def get_attention_strategy_alignment(self, attention, strategy):
        """ì£¼ì˜ì§‘ì¤‘-ì „ëµ ì •ë ¬"""
        if attention > 0.8:
            high_attention_strategies = {
                'intensive_focused': 0.9,
                'energetic_dynamic': 0.8,
                'optimized_efficient': 0.7
            }
            return high_attention_strategies.get(strategy, 0.4)
        elif attention < 0.4:
            low_attention_strategies = {
                'gentle_adaptive': 0.9,
                'supportive_gradual': 0.8,
                'careful_methodical': 0.7
            }
            return low_attention_strategies.get(strategy, 0.4)
        else:
            return 0.6  # ì¤‘ê°„ ì§‘ì¤‘ë„ì—ì„œëŠ” ëª¨ë“  ì „ëµì´ ì ë‹¹í•¨
    
    def get_emotion_strategy_alignment(self, emotion, strategy):
        """ê°ì •-ì „ëµ ì •ë ¬"""
        emotion_strategy_scores = {
            ('happy', 'energetic_dynamic'): 0.9,
            ('happy', 'intensive_focused'): 0.8,
            ('surprise', 'energetic_dynamic'): 0.8,
            ('sad', 'supportive_gradual'): 0.9,
            ('sad', 'gentle_adaptive'): 0.8,
            ('fear', 'supportive_gradual'): 0.9,
            ('fear', 'gentle_adaptive'): 0.8,
            ('anger', 'careful_methodical'): 0.8,
            ('anger', 'balanced_standard'): 0.7,
            ('neutral', 'balanced_standard'): 0.8,
            ('neutral', 'optimized_efficient'): 0.7
        }
        return emotion_strategy_scores.get((emotion, strategy), 0.5)
    
    # mcts_reward_optimization ë©”ì„œë“œ ì œê±°ë¨ - R* ì§„í™” ì•Œê³ ë¦¬ì¦˜ìœ¼ë¡œ ëŒ€ì²´
    
    # evaluate_reward_performance ë©”ì„œë“œ ì œê±°ë¨ - R* fitness í‰ê°€ë¡œ ëŒ€ì²´

# ==================== Hierarchical MCTS Integration System ====================
class HierarchicalMCTSSystem:
    """ğŸ§  4ë‹¨ê³„ ê³„ì¸µì  MCTS í†µí•© ì‹œìŠ¤í…œ (ë…¼ë¬¸ êµ¬í˜„)"""
    
    def __init__(self):
        print("Initializing Revolutionary Hierarchical MCTS System...")
        
        # 4ë‹¨ê³„ MCTS ì‹œìŠ¤í…œë“¤
        self.meta_mcts = MetaStrategyMCTS()
        self.cognitive_mcts = CognitiveAdaptationMCTS()
        self.combination_mcts = CombinationMCTS()
        self.execution_mcts = ExecutionStrategyMCTS()
        
        # ì„±ëŠ¥ ì¶”ì 
        self.decision_history = deque(maxlen=100)
        self.level_stats = {
            0: MCTSLevelStats(0, 0, 0.0, 0, 0, 0.0),
            1: MCTSLevelStats(1, 0, 0.0, 0, 0, 0.0),
            2: MCTSLevelStats(2, 0, 0.0, 0, 0, 0.0),
            3: MCTSLevelStats(3, 0, 0.0, 0, 0, 0.0)
        }
        
        # ì‹œê°í™”ìš© ë°ì´í„°
        self.current_tree_visualization = {}
        self.adaptation_history = deque(maxlen=50)
        self.performance_metrics = deque(maxlen=200)
        
        # ë©”ì‹œì§€ íˆìŠ¤í† ë¦¬
        self.message_history = deque(maxlen=40)
        self.add_system_message("Hierarchical MCTS System initialized")
        self.add_system_message("4-Level intelligent decision making ready")
        
        # ë…¼ë¬¸ êµ¬í˜„: GPT-4 ì—ì´ì „íŠ¸ ë° GEMMAS (ì„ íƒì )
        self.integrated_agents = None
        self.use_llm_agents = False
        try:
            import os
            if os.getenv("OPENAI_API_KEY"):
                print("ğŸ¤– Initializing GPT-4 Multi-Agent System...")
                from integration_wrapper import IntegratedMultiAgentSystem
                self.integrated_agents = IntegratedMultiAgentSystem(api_key=os.getenv("OPENAI_API_KEY"))
                self.use_llm_agents = True  # â† GPT-4 ì—ì´ì „íŠ¸ í™œì„±í™”!
                self.add_system_message("GPT-4 agents ready")
                print("âœ… GPT-4 agents integrated and ACTIVATED")
                print("ğŸ”„ Real GPT-4 collaboration enabled:")
                print("   â†’ Planner: Generates candidate actions")
                print("   â†’ Critic: Evaluates with Q-values")
                print("   â†’ Executor: Selects final action")
                print("   â†’ GEMMAS: Measures collaboration quality (IDS, UPR)")
        except Exception as e:
            print(f"âš ï¸  GPT-4 integration skipped: {e}")
            self.integrated_agents = None
        
        # ë…¼ë¬¸ ë³´ìƒ í•¨ìˆ˜
        try:
            from paper_reward_function import PaperRewardFunction
            self.paper_reward_function = PaperRewardFunction()
            self.add_system_message("Paper reward function loaded")
            print("âœ… Paper reward function loaded")
        except Exception as e:
            print(f"âš ï¸  Paper reward function skipped: {e}")
            self.paper_reward_function = None
        
        print("ğŸš€ Hierarchical MCTS System Ready!")
    
    def add_system_message(self, message):
        """ì‹œìŠ¤í…œ ë©”ì‹œì§€ ì¶”ê°€"""
        timestamp = time.strftime("%H:%M:%S")
        self.message_history.append({
            "time": timestamp,
            "type": "system",
            "message": message
        })
    
    def add_level_message(self, level, message):
        """ë ˆë²¨ë³„ ë©”ì‹œì§€ ì¶”ê°€"""
        timestamp = time.strftime("%H:%M:%S")
        self.message_history.append({
            "time": timestamp,
            "type": "level",
            "level": level,
            "message": message
        })
    
    def hierarchical_decision_making(self, user_context: Dict, face_detected: bool) -> HierarchicalDecision:
        """ğŸ”¶âœ¨ Multi-Adaptive: ë‹¤ì¤‘ ì—ì´ì „íŠ¸ + ì‚¬ìš©ì ì ì‘ (ì œì•ˆ ì‹œìŠ¤í…œ + ë…¼ë¬¸ êµ¬í˜„)"""
        
        if not face_detected:
            return self._generate_no_face_decision()
        
        start_time = time.time()
        
        # ğŸ¯ Level 0: Meta-Strategy Selection
        self.add_level_message(0, "Level 0: Meta-strategy selection starting...")
        meta_strategy = self.meta_mcts.search(user_context, iterations=30)
        self.add_level_message(0, f"Meta-strategy selected: {meta_strategy}")
        
        # ğŸ§  Level 1: Cognitive Adaptation
        self.add_level_message(1, "Level 1: Cognitive adaptation analysis...")
        cognitive_adaptation = self.cognitive_mcts.search(user_context, meta_strategy, iterations=25)
        self.add_level_message(1, f"Adaptation type: {cognitive_adaptation}")
        
        # âš¡ Level 2: Agent Combination
        self.add_level_message(2, "Level 2: Agent combination optimization...")
        # ì¸ì§€ ë¶€í•˜ì— ë”°ë¥¸ ë°˜ë³µ íšŸìˆ˜ ì¡°ì •
        cognitive_load = user_context.get('cognitive_load_level', 'medium')
        combination_iterations = self._get_adaptive_iterations(cognitive_load, cognitive_adaptation, 'combination')
        
        optimal_combination = self.combination_mcts.search(
            user_context, meta_strategy, cognitive_adaptation, combination_iterations
        )
        self.add_level_message(2, f"Optimal combination: {optimal_combination}")
        
        # ğŸ¯ Level 3: Execution Strategy
        self.add_level_message(3, "Level 3: Execution strategy optimization...")
        execution_iterations = self._get_adaptive_iterations(cognitive_load, cognitive_adaptation, 'execution')
        
        execution_strategy = self.execution_mcts.search(
            user_context, meta_strategy, cognitive_adaptation, optimal_combination, execution_iterations
        )
        self.add_level_message(3, f"Execution strategy: {execution_strategy}")
        
        # ì˜ì‚¬ê²°ì • ì™„ë£Œ
        decision_time = time.time() - start_time
        
        # í’ˆì§ˆ ì ìˆ˜ ê³„ì‚° (ë…¼ë¬¸ ë³´ìƒ í•¨ìˆ˜ ì‚¬ìš©)
        quality_score = self._calculate_decision_quality_with_paper_reward(
            meta_strategy, cognitive_adaptation, optimal_combination, 
            execution_strategy, user_context
        )
        
        # ì‹ ë¢°ë„ ê³„ì‚°
        confidence = self._calculate_confidence(user_context, decision_time, quality_score)
        
        # íŠ¸ë¦¬ ì‹œê°í™” ë°ì´í„° ìƒì„±
        tree_visualization = self._generate_tree_visualization(
            meta_strategy, cognitive_adaptation, optimal_combination, execution_strategy
        )
        
        # ë ˆë²¨ë³„ ê²°ì • ì •ë³´
        level_decisions = [
            {"level": 0, "decision": meta_strategy, "type": "meta_strategy"},
            {"level": 1, "decision": cognitive_adaptation, "type": "adaptation"},
            {"level": 2, "decision": optimal_combination, "type": "combination"},
            {"level": 3, "decision": execution_strategy, "type": "execution"}
        ]
        
        # ìµœì¢… ê²°ì • ìƒì„±
        hierarchical_decision = HierarchicalDecision(
            meta_strategy=meta_strategy,
            cognitive_adaptation=cognitive_adaptation,
            combination_choice=optimal_combination,
            execution_strategy=execution_strategy,
            tree_depth=4,
            quality_score=quality_score,
            decision_time=decision_time,
            confidence=confidence,
            tree_visualization=tree_visualization,
            level_decisions=level_decisions
        )
        
        # ë…¼ë¬¸ êµ¬í˜„: GPT-4 ì—ì´ì „íŠ¸ í˜‘ë ¥ (ì„ íƒì , ë¹„ë™ê¸°)
        if self.use_llm_agents and self.integrated_agents:
            try:
                self.add_system_message("Consulting GPT-4 agents...")
                agent_result = self.integrated_agents.process_decision(
                    f"Evaluate MCTS decision: {meta_strategy}",
                    user_context
                )
                # GEMMAS í’ˆì§ˆ ì •ë³´ ì¶”ê°€
                hierarchical_decision.ids = agent_result.get('ids', 0.0)
                hierarchical_decision.upr = agent_result.get('upr', 0.0)
                hierarchical_decision.llm_feedback = agent_result.get('collaboration_quality', '')
                self.add_system_message(f"GPT-4: IDS={agent_result.get('ids', 0):.2f}, UPR={agent_result.get('upr', 0):.2f}")
            except Exception as e:
                print(f"âš ï¸  GPT-4 evaluation skipped: {e}")
        
        # í†µê³„ ì—…ë°ì´íŠ¸
        self._update_statistics(hierarchical_decision, user_context)
        
        # íˆìŠ¤í† ë¦¬ ì €ì¥
        self.decision_history.append(hierarchical_decision)
        
        self.add_system_message(f"Hierarchical decision complete! Quality: {quality_score:.3f}")
        
        return hierarchical_decision
    
    def _calculate_decision_quality_with_paper_reward(self, meta_strategy, cognitive_adaptation, 
                                                      optimal_combination, execution_strategy, user_context):
        """ë…¼ë¬¸ ë³´ìƒ í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•œ í’ˆì§ˆ ê³„ì‚°"""
        if self.paper_reward_function:
            try:
                decision = {
                    'meta_strategy': meta_strategy,
                    'cognitive_adaptation': cognitive_adaptation,
                    'combination_choice': optimal_combination,
                    'execution_strategy': execution_strategy,
                    'decision_time': 0.0,
                    'tree_depth': 4,
                    'confidence': 0.8
                }
                return self.paper_reward_function.calculate_total_reward(user_context, decision)
            except:
                pass
        
        # Fallback to original
        return self._calculate_decision_quality(meta_strategy, cognitive_adaptation, 
                                               optimal_combination, execution_strategy, user_context)
    
    def _get_adaptive_iterations(self, cognitive_load: str, adaptation_type: str, decision_type: str) -> int:
        """ì¸ì§€ ë¶€í•˜ì— ë”°ë¥¸ ì ì‘ì  ë°˜ë³µ íšŸìˆ˜ ê²°ì •"""
        
        base_iterations = {
            'combination': 40,
            'execution': 20
        }
        
        base = base_iterations.get(decision_type, 30)
        
        # ì¸ì§€ ë¶€í•˜ì— ë”°ë¥¸ ì¡°ì •
        if cognitive_load == 'high':
            multiplier = 0.7  # ë†’ì€ ì¸ì§€ë¶€í•˜ â†’ ë¹ ë¥¸ ê²°ì •
        elif cognitive_load == 'low':
            multiplier = 1.3  # ë‚®ì€ ì¸ì§€ë¶€í•˜ â†’ ì •êµí•œ íƒìƒ‰
        else:
            multiplier = 1.0  # í‘œì¤€
        
        # ì ì‘ íƒ€ì…ì— ë”°ë¥¸ ì¡°ì •
        if adaptation_type == 'simplified':
            multiplier *= 0.8
        elif adaptation_type == 'complex':
            multiplier *= 1.2
        
        return int(base * multiplier)
    
    def _calculate_decision_quality(self, meta_strategy, adaptation, combination, execution, user_context):
        """ì˜ì‚¬ê²°ì • í’ˆì§ˆ ì ìˆ˜ ê³„ì‚°"""
        
        base_quality = 0.5
        
        # ë©”íƒ€ ì „ëµ í’ˆì§ˆ
        emotion = user_context.get('emotion', 'neutral')
        cognitive_load = user_context.get('cognitive_load_level', 'medium')
        
        if cognitive_load == 'high' and meta_strategy == 'combination':
            base_quality += 0.2
        elif cognitive_load == 'low' and meta_strategy == 'hybrid':
            base_quality += 0.15
        
        # ì ì‘ í’ˆì§ˆ
        if cognitive_load == 'high' and adaptation == 'simplified':
            base_quality += 0.15
        elif cognitive_load == 'low' and adaptation == 'complex':
            base_quality += 0.1
        
        # ì¡°í•©-ì‹¤í–‰ ì¼ì¹˜ì„±
        planner, critic, executor = combination
        if executor == 'gentle' and execution.startswith('gentle'):
            base_quality += 0.1
        elif executor == 'intensive' and execution.startswith('intensive'):
            base_quality += 0.1
        
        # ì „ì²´ì  ì¼ê´€ì„± ë³´ë„ˆìŠ¤
        if (meta_strategy == 'combination' and adaptation == 'simplified' and 
            execution == 'optimized_efficient'):
            base_quality += 0.1
        
        return np.clip(base_quality + np.random.normal(0, 0.02), 0.0, 1.0)
    
    def _calculate_confidence(self, user_context, decision_time, quality_score):
        """ì˜ì‚¬ê²°ì • ì‹ ë¢°ë„ ê³„ì‚°"""
        
        base_confidence = 0.7
        
        # ê²°ì • ì‹œê°„ ê¸°ë°˜ ì‹ ë¢°ë„
        if decision_time < 0.1:
            base_confidence += 0.1  # ë¹ ë¥¸ ê²°ì •
        elif decision_time > 0.5:
            base_confidence -= 0.1  # ë„ˆë¬´ ëŠë¦° ê²°ì •
        
        # í’ˆì§ˆ ê¸°ë°˜ ì‹ ë¢°ë„
        base_confidence += (quality_score - 0.5) * 0.4
        
        # ì¸ì§€ ë¶€í•˜ ê¸°ë°˜ ì‹ ë¢°ë„
        cognitive_load = user_context.get('cognitive_load_level', 'medium')
        if cognitive_load == 'high':
            base_confidence -= 0.1  # ë†’ì€ ë¶€í•˜ â†’ ë‚®ì€ ì‹ ë¢°ë„
        
        return np.clip(base_confidence, 0.0, 1.0)
    
    def _generate_tree_visualization(self, meta_strategy, adaptation, combination, execution):
        """íŠ¸ë¦¬ ì‹œê°í™” ë°ì´í„° ìƒì„±"""
        
        planner, critic, executor = combination
        
        return {
            "levels": [
                {
                    "level": 0,
                    "name": "Meta Strategy",
                    "decision": meta_strategy,
                    "alternatives": ["combination", "collaboration", "hybrid"],
                    "confidence": 0.85
                },
                {
                    "level": 1, 
                    "name": "Cognitive Adaptation",
                    "decision": adaptation,
                    "alternatives": ["simplified", "standard", "complex"],
                    "confidence": 0.78
                },
                {
                    "level": 2,
                    "name": "Agent Combination",
                    "decision": f"{planner}+{critic}+{executor}",
                    "alternatives": ["125 combinations evaluated"],
                    "confidence": 0.82
                },
                {
                    "level": 3,
                    "name": "Execution Strategy", 
                    "decision": execution,
                    "alternatives": ["gentle_adaptive", "intensive_focused", "balanced_standard"],
                    "confidence": 0.76
                }
            ],
            "connections": [
                {"from": 0, "to": 1, "strength": 0.9},
                {"from": 1, "to": 2, "strength": 0.85},
                {"from": 2, "to": 3, "strength": 0.8}
            ]
        }
    
    def _update_statistics(self, decision, user_context):
        """í†µê³„ ì—…ë°ì´íŠ¸"""
        
        # ë ˆë²¨ë³„ í†µê³„ ì—…ë°ì´íŠ¸
        for level in range(4):
            stats = self.level_stats[level]
            stats.decision_count += 1
            stats.avg_decision_time = (
                (stats.avg_decision_time * (stats.decision_count - 1) + 
                 decision.decision_time) / stats.decision_count
            )
            if decision.quality_score > stats.best_value:
                stats.best_value = decision.quality_score
        
        # ì ì‘ íˆìŠ¤í† ë¦¬ ì—…ë°ì´íŠ¸
        self.adaptation_history.append({
            'timestamp': time.time(),
            'cognitive_load': user_context.get('cognitive_load_level', 'medium'),
            'adaptation': decision.cognitive_adaptation,
            'quality': decision.quality_score
        })
        
        # ì„±ëŠ¥ ë©”íŠ¸ë¦­ ì—…ë°ì´íŠ¸
        self.performance_metrics.append({
            'timestamp': time.time(),
            'decision_time': decision.decision_time,
            'quality_score': decision.quality_score,
            'confidence': decision.confidence,
            'tree_depth': decision.tree_depth
        })
    
    def _generate_no_face_decision(self):
        """ì–¼êµ´ ê°ì§€ë˜ì§€ ì•Šì„ ë•Œ ê¸°ë³¸ ê²°ì •"""
        
        return HierarchicalDecision(
            meta_strategy="standby",
            cognitive_adaptation="standard",
            combination_choice=("adaptive", "balanced", "standard"),
            execution_strategy="balanced_standard",
            tree_depth=0,
            quality_score=0.0,
            decision_time=0.001,
            confidence=0.0,
            tree_visualization={"levels": [], "connections": []},
            level_decisions=[]
        )
    
    def get_current_performance_summary(self):
        """í˜„ì¬ ì„±ëŠ¥ ìš”ì•½"""
        
        if not self.performance_metrics:
            return {"avg_quality": 0.0, "avg_time": 0.0, "avg_confidence": 0.0}
        
        recent_metrics = list(self.performance_metrics)[-20:]  # ìµœê·¼ 20ê°œ
        
        return {
            "avg_quality": np.mean([m['quality_score'] for m in recent_metrics]),
            "avg_time": np.mean([m['decision_time'] for m in recent_metrics]),
            "avg_confidence": np.mean([m['confidence'] for m in recent_metrics]),
            "total_decisions": len(self.decision_history)
        }

# ==================== Hierarchical Visualization Windows ====================

class HierarchicalTreeWindow:
    """ğŸŒ² ê³„ì¸µì  ì˜ì‚¬ê²°ì • íŠ¸ë¦¬ ì‹œê°í™” ì°½"""
    
    def __init__(self, width=1000, height=800):
        self.width = width
        self.height = height
        self.window_name = "ğŸ§  Hierarchical Decision Tree"
        
        # ìƒ‰ìƒ íŒ”ë ˆíŠ¸
        self.colors = {
            'background': (15, 15, 25),
            'header': (255, 215, 0),  # Gold
            'level_0': (255, 100, 100),  # Meta Strategy - Red
            'level_1': (100, 255, 100),  # Cognitive - Green
            'level_2': (100, 100, 255),  # Combination - Blue
            'level_3': (255, 100, 255),  # Execution - Magenta
            'connection': (200, 200, 200),
            'text': (255, 255, 255),
            'active': (255, 255, 100)
        }
        
        print("ğŸŒ² Hierarchical Tree Visualization Window initialized")
    
    def create_tree_window(self, hierarchical_decision, system_stats):
        """ê³„ì¸µì  íŠ¸ë¦¬ ì‹œê°í™” ìƒì„±"""
        
        canvas = np.full((self.height, self.width, 3), self.colors['background'], dtype=np.uint8)
        
        # í—¤ë”
        self.draw_header(canvas, hierarchical_decision)
        
        # ê³„ì¸µ êµ¬ì¡° ì‹œê°í™”
        self.draw_hierarchical_levels(canvas, hierarchical_decision)
        
        # ì—°ê²°ì„  ê·¸ë¦¬ê¸°
        self.draw_connections(canvas, hierarchical_decision)
        
        # ì„±ëŠ¥ ì§€í‘œ
        self.draw_performance_indicators(canvas, hierarchical_decision, system_stats)
        
        # ì˜ì‚¬ê²°ì • ê²½ë¡œ ê°•ì¡°
        self.highlight_decision_path(canvas, hierarchical_decision)
        
        return canvas
    
    def draw_header(self, canvas, decision):
        """í—¤ë” ê·¸ë¦¬ê¸°"""
        
        cv2.putText(canvas, "HIERARCHICAL MCTS DECISION TREE", (20, 40), 
                   cv2.FONT_HERSHEY_SIMPLEX, 1.0, self.colors['header'], 3)
        
        current_time = time.strftime("%H:%M:%S")
        cv2.putText(canvas, f"Time: {current_time}", (self.width - 200, 40), 
                   cv2.FONT_HERSHEY_SIMPLEX, 0.6, self.colors['text'], 2)
        
        # í’ˆì§ˆ ì ìˆ˜ í‘œì‹œ
        quality_text = f"Quality Score: {decision.quality_score:.3f}"
        cv2.putText(canvas, quality_text, (20, 70), 
                   cv2.FONT_HERSHEY_SIMPLEX, 0.7, self.colors['active'], 2)
        
        # ì‹ ë¢°ë„ í‘œì‹œ
        confidence_text = f"Confidence: {decision.confidence:.3f}"
        cv2.putText(canvas, confidence_text, (300, 70), 
                   cv2.FONT_HERSHEY_SIMPLEX, 0.7, self.colors['active'], 2)
        
        cv2.line(canvas, (20, 85), (self.width - 20, 85), self.colors['header'], 2)
    
    def draw_hierarchical_levels(self, canvas, decision):
        """4ë‹¨ê³„ ê³„ì¸µ êµ¬ì¡° ê·¸ë¦¬ê¸°"""
        
        level_y_positions = [150, 300, 450, 600]
        level_colors = [self.colors['level_0'], self.colors['level_1'], 
                       self.colors['level_2'], self.colors['level_3']]
        
        level_data = [
            ("Level 0: Meta Strategy", decision.meta_strategy),
            ("Level 1: Cognitive Adaptation", decision.cognitive_adaptation),
            ("Level 2: Agent Combination", f"{decision.combination_choice[0]}+{decision.combination_choice[1]}+{decision.combination_choice[2]}"),
            ("Level 3: Execution Strategy", decision.execution_strategy)
        ]
        
        for i, (level_name, decision_text) in enumerate(level_data):
            y_pos = level_y_positions[i]
            color = level_colors[i]
            
            # ë ˆë²¨ ë°•ìŠ¤ ê·¸ë¦¬ê¸°
            box_x, box_y = 100, y_pos - 30
            box_w, box_h = 800, 80
            
            cv2.rectangle(canvas, (box_x, box_y), (box_x + box_w, box_y + box_h), color, 3)
            cv2.rectangle(canvas, (box_x + 3, box_y + 3), (box_x + box_w - 3, box_y + box_h - 3), 
                         self.colors['background'], -1)
            
            # ë ˆë²¨ ì´ë¦„
            cv2.putText(canvas, level_name, (box_x + 20, box_y + 30), 
                       cv2.FONT_HERSHEY_SIMPLEX, 0.7, color, 2)
            
            # ê²°ì • ë‚´ìš©
            cv2.putText(canvas, f"Decision: {decision_text}", (box_x + 20, box_y + 60), 
                       cv2.FONT_HERSHEY_SIMPLEX, 0.6, self.colors['text'], 2)
    
    def draw_connections(self, canvas, decision):
        """ë ˆë²¨ ê°„ ì—°ê²°ì„  ê·¸ë¦¬ê¸°"""
        
        level_centers = [(500, 150), (500, 300), (500, 450), (500, 600)]
        
        for i in range(len(level_centers) - 1):
            start_point = (level_centers[i][0], level_centers[i][1] + 40)
            end_point = (level_centers[i + 1][0], level_centers[i + 1][1] - 40)
            
            # ì—°ê²°ì„  ê·¸ë¦¬ê¸° (í™”ì‚´í‘œ ì œê±°)
            cv2.line(canvas, start_point, end_point, 
                    self.colors['connection'], 3)
            
            # ì—°ê²° ê°•ë„ í‘œì‹œ
            mid_x = (start_point[0] + end_point[0]) // 2
            mid_y = (start_point[1] + end_point[1]) // 2
            strength = 0.9 - i * 0.05  # ë ˆë²¨ì´ ë‚´ë ¤ê°ˆìˆ˜ë¡ ê°•ë„ ê°ì†Œ
            
            cv2.putText(canvas, f"{strength:.2f}", (mid_x + 20, mid_y), 
                       cv2.FONT_HERSHEY_SIMPLEX, 0.5, self.colors['active'], 1)
    
    def draw_performance_indicators(self, canvas, decision, system_stats):
        """ì„±ëŠ¥ ì§€í‘œ ê·¸ë¦¬ê¸°"""
        
        # ìš°ì¸¡ ì„±ëŠ¥ íŒ¨ë„
        panel_x = 720
        panel_y = 100
        
        cv2.putText(canvas, "PERFORMANCE METRICS", (panel_x, panel_y), 
                   cv2.FONT_HERSHEY_SIMPLEX, 0.6, self.colors['header'], 2)
        
        metrics = [
            f"Decision Time: {decision.decision_time:.3f}s",
            f"Tree Depth: {decision.tree_depth}",
            f"Quality Score: {decision.quality_score:.3f}",
            f"Confidence: {decision.confidence:.3f}"
        ]
        
        for i, metric in enumerate(metrics):
            cv2.putText(canvas, metric, (panel_x, panel_y + 30 + i*25), 
                       cv2.FONT_HERSHEY_SIMPLEX, 0.5, self.colors['text'], 1)
        
        # ì‹œìŠ¤í…œ í†µê³„
        if system_stats:
            cv2.putText(canvas, "SYSTEM STATS", (panel_x, panel_y + 150), 
                       cv2.FONT_HERSHEY_SIMPLEX, 0.6, self.colors['header'], 2)
            
            sys_metrics = [
                f"Avg Quality: {system_stats.get('avg_quality', 0.0):.3f}",
                f"Avg Time: {system_stats.get('avg_time', 0.0):.3f}s",
                f"Total Decisions: {system_stats.get('total_decisions', 0)}"
            ]
            
            for i, metric in enumerate(sys_metrics):
                cv2.putText(canvas, metric, (panel_x, panel_y + 180 + i*25), 
                           cv2.FONT_HERSHEY_SIMPLEX, 0.5, self.colors['text'], 1)
    
    def highlight_decision_path(self, canvas, decision):
        """ì˜ì‚¬ê²°ì • ê²½ë¡œ ê°•ì¡°"""
        
        # ì¢Œì¸¡ì— ì˜ì‚¬ê²°ì • í”Œë¡œìš° ê·¸ë¦¬ê¸°
        flow_x = 20
        flow_y = 150
        
        cv2.putText(canvas, "DECISION FLOW", (flow_x, flow_y - 20), 
                   cv2.FONT_HERSHEY_SIMPLEX, 0.6, self.colors['active'], 2)
        
        flow_steps = [
            f"1. Meta: {decision.meta_strategy}",
            f"2. Adapt: {decision.cognitive_adaptation}",
            f"3. Combine: {decision.combination_choice[0][:8]}+...",
            f"4. Execute: {decision.execution_strategy[:15]}"
        ]
        
        for i, step in enumerate(flow_steps):
            y_pos = flow_y + i * 110
            
            # ìŠ¤í… ì› ê·¸ë¦¬ê¸°
            cv2.circle(canvas, (flow_x + 15, y_pos), 12, self.colors['active'], 3)
            cv2.putText(canvas, str(i+1), (flow_x + 10, y_pos + 5), 
                       cv2.FONT_HERSHEY_SIMPLEX, 0.5, self.colors['active'], 2)
            
            # ìŠ¤í… ì„¤ëª…
            cv2.putText(canvas, step[3:], (flow_x + 35, y_pos + 5), 
                       cv2.FONT_HERSHEY_SIMPLEX, 0.4, self.colors['text'], 1)
            
            # ë‹¤ìŒ ìŠ¤í…ìœ¼ë¡œ ì—°ê²°ì„  (í™”ì‚´í‘œ ì œê±°)
            if i < len(flow_steps) - 1:
                cv2.line(canvas, (flow_x + 15, y_pos + 15), 
                        (flow_x + 15, y_pos + 95), 
                        self.colors['active'], 2)

class CognitiveAdaptationWindow:
    """ğŸ§  ì¸ì§€ ë¶€í•˜ ì ì‘ ì‹œê°í™” ì°½"""
    
    def __init__(self, width=900, height=700):
        self.width = width
        self.height = height
        self.window_name = "ğŸ§  Cognitive Load Adaptation"
        
        # ìƒ‰ìƒ
        self.colors = {
            'background': (20, 30, 40),
            'header': (100, 255, 255),  # Cyan
            'low_load': (100, 255, 100),    # Green
            'medium_load': (255, 255, 100), # Yellow  
            'high_load': (255, 100, 100),   # Red
            'adaptation': (255, 150, 255),   # Pink
            'text': (255, 255, 255),
            'grid': (100, 100, 100)
        }
        
        print("ğŸ§  Cognitive Adaptation Window initialized")
    
    def create_adaptation_window(self, user_context, hierarchical_decision, adaptation_history):
        """ì¸ì§€ ì ì‘ ì‹œê°í™” ìƒì„±"""
        
        canvas = np.full((self.height, self.width, 3), self.colors['background'], dtype=np.uint8)
        
        # í—¤ë”
        self.draw_adaptation_header(canvas, user_context)
        
        # í˜„ì¬ ì¸ì§€ ìƒíƒœ
        self.draw_current_cognitive_state(canvas, user_context, hierarchical_decision)
        
        # ì ì‘ íˆìŠ¤í† ë¦¬ ê·¸ë˜í”„
        self.draw_adaptation_history_graph(canvas, adaptation_history)
        
        # ì ì‘ ì „ëµ ì„¤ëª…
        self.draw_adaptation_strategy_explanation(canvas, hierarchical_decision)
        
        return canvas
    
    def draw_adaptation_header(self, canvas, user_context):
        """í—¤ë” ê·¸ë¦¬ê¸°"""
        
        cv2.putText(canvas, "COGNITIVE LOAD ADAPTATION SYSTEM", (20, 40), 
                   cv2.FONT_HERSHEY_SIMPLEX, 0.8, self.colors['header'], 2)
        
        current_time = time.strftime("%H:%M:%S")
        cv2.putText(canvas, f"Time: {current_time}", (self.width - 150, 40), 
                   cv2.FONT_HERSHEY_SIMPLEX, 0.5, self.colors['text'], 1)
        
        cv2.line(canvas, (20, 55), (self.width - 20, 55), self.colors['header'], 2)
    
    def draw_current_cognitive_state(self, canvas, user_context, decision):
        """í˜„ì¬ ì¸ì§€ ìƒíƒœ ì‹œê°í™”"""
        
        y_start = 80
        
        cv2.putText(canvas, "CURRENT COGNITIVE STATE", (20, y_start), 
                   cv2.FONT_HERSHEY_SIMPLEX, 0.7, self.colors['header'], 2)
        
        # ì¸ì§€ ë¶€í•˜ ë ˆë²¨ í‘œì‹œ
        cognitive_load = user_context.get('cognitive_load_level', 'medium')
        load_colors = {
            'low': self.colors['low_load'],
            'medium': self.colors['medium_load'], 
            'high': self.colors['high_load']
        }
        
        load_color = load_colors.get(cognitive_load, self.colors['medium_load'])
        
        # ì¸ì§€ ë¶€í•˜ ë°” ê·¸ë¦¬ê¸°
        bar_x, bar_y = 30, y_start + 30
        bar_width = 200
        bar_height = 30
        
        cv2.rectangle(canvas, (bar_x, bar_y), (bar_x + bar_width, bar_y + bar_height), 
                     self.colors['text'], 2)
        
        # ë¶€í•˜ ìˆ˜ì¤€ì— ë”°ë¥¸ ì±„ìš°ê¸°
        load_levels = {'low': 0.3, 'medium': 0.6, 'high': 0.9}
        fill_width = int(bar_width * load_levels.get(cognitive_load, 0.5))
        
        cv2.rectangle(canvas, (bar_x + 2, bar_y + 2), 
                     (bar_x + fill_width, bar_y + bar_height - 2), 
                     load_color, -1)
        
        cv2.putText(canvas, f"Cognitive Load: {cognitive_load.upper()}", 
                   (bar_x + bar_width + 20, bar_y + 20), 
                   cv2.FONT_HERSHEY_SIMPLEX, 0.6, load_color, 2)
        
        # ì •ì‹ ì  ë…¸ë ¥ ì ìˆ˜
        mental_effort = user_context.get('mental_effort_score', 0.5)
        effort_text = f"Mental Effort: {mental_effort:.3f}"
        cv2.putText(canvas, effort_text, (30, y_start + 80), 
                   cv2.FONT_HERSHEY_SIMPLEX, 0.5, self.colors['text'], 1)
        
        # ë™ê³µ í™•ì¥ë¥ 
        pupil_dilation = user_context.get('pupil_dilation_rate', 0.0)
        pupil_text = f"Pupil Dilation Rate: {pupil_dilation:.3f}"
        cv2.putText(canvas, pupil_text, (30, y_start + 105), 
                   cv2.FONT_HERSHEY_SIMPLEX, 0.5, self.colors['text'], 1)
        
        # ì ì‘ ì „ëµ
        adaptation = decision.cognitive_adaptation
        adaptation_text = f"Adaptation Strategy: {adaptation.upper()}"
        cv2.putText(canvas, adaptation_text, (30, y_start + 130), 
                   cv2.FONT_HERSHEY_SIMPLEX, 0.6, self.colors['adaptation'], 2)
    
    def draw_adaptation_history_graph(self, canvas, adaptation_history):
        """ì ì‘ íˆìŠ¤í† ë¦¬ ê·¸ë˜í”„"""
        
        if not adaptation_history:
            return
        
        graph_x, graph_y = 50, 280
        graph_width, graph_height = 800, 200
        
        cv2.putText(canvas, "ADAPTATION HISTORY", (graph_x, graph_y - 20), 
                   cv2.FONT_HERSHEY_SIMPLEX, 0.6, self.colors['header'], 2)
        
        # ê·¸ë˜í”„ í…Œë‘ë¦¬
        cv2.rectangle(canvas, (graph_x, graph_y), 
                     (graph_x + graph_width, graph_y + graph_height), 
                     self.colors['text'], 2)
        
        # ê·¸ë¦¬ë“œ ê·¸ë¦¬ê¸°
        for i in range(1, 10):
            grid_x = graph_x + i * (graph_width // 10)
            cv2.line(canvas, (grid_x, graph_y), (grid_x, graph_y + graph_height), 
                    self.colors['grid'], 1)
        
        for i in range(1, 5):
            grid_y = graph_y + i * (graph_height // 5)
            cv2.line(canvas, (graph_x, grid_y), (graph_x + graph_width, grid_y), 
                    self.colors['grid'], 1)
        
        # ë°ì´í„° í¬ì¸íŠ¸ ê·¸ë¦¬ê¸°
        history_list = list(adaptation_history)[-50:]  # ìµœê·¼ 50ê°œ
        if len(history_list) < 2:
            return
        
        for i in range(len(history_list) - 1):
            current = history_list[i]
            next_point = history_list[i + 1]
            
            # ì¸ì§€ ë¶€í•˜ë¥¼ yì¢Œí‘œë¡œ ë³€í™˜
            load_levels = {'low': 0.2, 'medium': 0.5, 'high': 0.8}
            
            current_y = graph_y + graph_height - (load_levels.get(current['cognitive_load'], 0.5) * graph_height)
            next_y = graph_y + graph_height - (load_levels.get(next_point['cognitive_load'], 0.5) * graph_height)
            
            current_x = graph_x + (i * graph_width // len(history_list))
            next_x = graph_x + ((i + 1) * graph_width // len(history_list))
            
            # ì„  ê·¸ë¦¬ê¸°
            load_color = {
                'low': self.colors['low_load'],
                'medium': self.colors['medium_load'],
                'high': self.colors['high_load']
            }.get(current['cognitive_load'], self.colors['medium_load'])
            
            cv2.line(canvas, (int(current_x), int(current_y)), 
                    (int(next_x), int(next_y)), load_color, 2)
            
            # í¬ì¸íŠ¸ í‘œì‹œ
            cv2.circle(canvas, (int(current_x), int(current_y)), 3, load_color, -1)
    
    def draw_adaptation_strategy_explanation(self, canvas, decision):
        """ì ì‘ ì „ëµ ì„¤ëª…"""
        
        y_start = 520
        
        cv2.putText(canvas, "ADAPTATION STRATEGY DETAILS", (20, y_start), 
                   cv2.FONT_HERSHEY_SIMPLEX, 0.6, self.colors['header'], 2)
        
        adaptation = decision.cognitive_adaptation
        
        explanations = {
            'simplified': [
                "â€¢ Reduced computational complexity",
                "â€¢ Faster decision making",
                "â€¢ Lower cognitive burden",
                "â€¢ Streamlined user interface"
            ],
            'standard': [
                "â€¢ Balanced approach",
                "â€¢ Moderate complexity",
                "â€¢ Standard processing time", 
                "â€¢ Regular interface elements"
            ],
            'complex': [
                "â€¢ Advanced optimization",
                "â€¢ Thorough analysis",
                "â€¢ Higher accuracy",
                "â€¢ Rich interface features"
            ]
        }
        
        strategy_explanations = explanations.get(adaptation, ["â€¢ Standard approach"])
        
        for i, explanation in enumerate(strategy_explanations):
            cv2.putText(canvas, explanation, (30, y_start + 30 + i*25), 
                       cv2.FONT_HERSHEY_SIMPLEX, 0.5, self.colors['adaptation'], 1)

class PerformanceAnalyticsWindow:
    """ğŸ“Š ì‹¤ì‹œê°„ ì„±ëŠ¥ ë¶„ì„ ëŒ€ì‹œë³´ë“œ"""
    
    def __init__(self, width=900, height=700):
        self.width = width
        self.height = height
        self.window_name = "ğŸ“Š Performance Analytics"
        
        # ìƒ‰ìƒ
        self.colors = {
            'background': (25, 25, 35),
            'header': (255, 165, 0),  # Orange
            'quality': (100, 255, 150),   # Light Green
            'time': (150, 150, 255),      # Light Blue  
            'confidence': (255, 150, 100), # Light Orange
            'trend_up': (100, 255, 100),   # Green
            'trend_down': (255, 100, 100), # Red
            'text': (255, 255, 255),
            'grid': (80, 80, 80),
            'panel': (40, 40, 50)
        }
        
        print("ğŸ“Š Performance Analytics Window initialized")
    
    def create_analytics_window(self, hierarchical_system, current_decision):
        """ì„±ëŠ¥ ë¶„ì„ ëŒ€ì‹œë³´ë“œ ìƒì„±"""
        
        canvas = np.full((self.height, self.width, 3), self.colors['background'], dtype=np.uint8)
        
        # í—¤ë”
        self.draw_analytics_header(canvas)
        
        # ì‹¤ì‹œê°„ ì„±ëŠ¥ ë©”íŠ¸ë¦­
        self.draw_realtime_metrics(canvas, hierarchical_system, current_decision)
        
        # ì„±ëŠ¥ íŠ¸ë Œë“œ ê·¸ë˜í”„
        self.draw_performance_trends(canvas, hierarchical_system)
        
        # ë ˆë²¨ë³„ íš¨ìœ¨ì„± ë¶„ì„
        self.draw_level_efficiency_analysis(canvas, hierarchical_system)
        
        # ì¶”ì²œ ì‚¬í•­
        self.draw_recommendations(canvas, hierarchical_system, current_decision)
        
        return canvas
    
    def draw_analytics_header(self, canvas):
        """í—¤ë” ê·¸ë¦¬ê¸°"""
        
        cv2.putText(canvas, "PERFORMANCE ANALYTICS DASHBOARD", (20, 40), 
                   cv2.FONT_HERSHEY_SIMPLEX, 0.8, self.colors['header'], 2)
        
        current_time = time.strftime("%H:%M:%S")
        cv2.putText(canvas, f"Time: {current_time}", (self.width - 150, 40), 
                   cv2.FONT_HERSHEY_SIMPLEX, 0.5, self.colors['text'], 1)
        
        cv2.line(canvas, (20, 55), (self.width - 20, 55), self.colors['header'], 2)
    
    def draw_realtime_metrics(self, canvas, system, current_decision):
        """ì‹¤ì‹œê°„ ì„±ëŠ¥ ë©”íŠ¸ë¦­"""
        
        y_start = 80
        
        cv2.putText(canvas, "REAL-TIME PERFORMANCE", (20, y_start), 
                   cv2.FONT_HERSHEY_SIMPLEX, 0.7, self.colors['header'], 2)
        
        # ì„±ëŠ¥ ìš”ì•½ ê°€ì ¸ì˜¤ê¸°
        performance_summary = system.get_current_performance_summary()
        
        # ë©”íŠ¸ë¦­ íŒ¨ë„ë“¤
        panels = [
            {
                'title': 'Decision Quality',
                'value': current_decision.quality_score,
                'avg': performance_summary.get('avg_quality', 0.0),
                'color': self.colors['quality'],
                'position': (30, y_start + 40)
            },
            {
                'title': 'Response Time',
                'value': current_decision.decision_time,
                'avg': performance_summary.get('avg_time', 0.0),
                'color': self.colors['time'],
                'position': (320, y_start + 40)
            },
            {
                'title': 'Confidence Level',
                'value': current_decision.confidence,
                'avg': performance_summary.get('avg_confidence', 0.0),
                'color': self.colors['confidence'],
                'position': (610, y_start + 40)
            }
        ]
        
        for panel in panels:
            self.draw_metric_panel(canvas, panel)
    
    def draw_metric_panel(self, canvas, panel):
        """ê°œë³„ ë©”íŠ¸ë¦­ íŒ¨ë„ ê·¸ë¦¬ê¸°"""
        
        x, y = panel['position']
        width, height = 250, 120
        
        # íŒ¨ë„ ë°°ê²½
        cv2.rectangle(canvas, (x, y), (x + width, y + height), 
                     self.colors['panel'], -1)
        cv2.rectangle(canvas, (x, y), (x + width, y + height), 
                     panel['color'], 2)
        
        # ì œëª©
        cv2.putText(canvas, panel['title'], (x + 10, y + 25), 
                   cv2.FONT_HERSHEY_SIMPLEX, 0.6, panel['color'], 2)
        
        # í˜„ì¬ ê°’
        if panel['title'] == 'Response Time':
            value_text = f"{panel['value']:.3f}s"
        else:
            value_text = f"{panel['value']:.3f}"
        
        cv2.putText(canvas, f"Current: {value_text}", (x + 10, y + 50), 
                   cv2.FONT_HERSHEY_SIMPLEX, 0.5, self.colors['text'], 1)
        
        # í‰ê· ê°’
        if panel['title'] == 'Response Time':
            avg_text = f"{panel['avg']:.3f}s"
        else:
            avg_text = f"{panel['avg']:.3f}"
        
        cv2.putText(canvas, f"Average: {avg_text}", (x + 10, y + 75), 
                   cv2.FONT_HERSHEY_SIMPLEX, 0.5, self.colors['text'], 1)
        
        # íŠ¸ë Œë“œ í‘œì‹œ (í™”ì‚´í‘œ ì œê±°)
        trend_color = self.colors['trend_up'] if panel['value'] >= panel['avg'] else self.colors['trend_down']
        trend_symbol = "+" if panel['value'] >= panel['avg'] else "-"
        
        cv2.putText(canvas, trend_symbol, (x + width - 40, y + 60), 
                   cv2.FONT_HERSHEY_SIMPLEX, 1.5, trend_color, 3)
    
    def draw_performance_trends(self, canvas, system):
        """ì„±ëŠ¥ íŠ¸ë Œë“œ ê·¸ë˜í”„"""
        
        graph_y = 250
        
        cv2.putText(canvas, "PERFORMANCE TRENDS", (20, graph_y), 
                   cv2.FONT_HERSHEY_SIMPLEX, 0.6, self.colors['header'], 2)
        
        # ê·¸ë˜í”„ ì˜ì—­
        graph_x, graph_y = 50, graph_y + 20
        graph_width, graph_height = 800, 180
        
        cv2.rectangle(canvas, (graph_x, graph_y), 
                     (graph_x + graph_width, graph_y + graph_height), 
                     self.colors['text'], 2)
        
        # ê·¸ë¦¬ë“œ
        for i in range(1, 8):
            grid_x = graph_x + i * (graph_width // 8)
            cv2.line(canvas, (grid_x, graph_y), (grid_x, graph_y + graph_height), 
                    self.colors['grid'], 1)
        
        for i in range(1, 4):
            grid_y = graph_y + i * (graph_height // 4)
            cv2.line(canvas, (graph_x, grid_y), (graph_x + graph_width, grid_y), 
                    self.colors['grid'], 1)
        
        # ì„±ëŠ¥ ë°ì´í„° ê·¸ë¦¬ê¸°
        if len(system.performance_metrics) > 1:
            metrics_list = list(system.performance_metrics)[-40:]  # ìµœê·¼ 40ê°œ
            
            for i in range(len(metrics_list) - 1):
                current = metrics_list[i]
                next_point = metrics_list[i + 1]
                
                # í’ˆì§ˆ ì ìˆ˜ ë¼ì¸
                current_x = graph_x + (i * graph_width // len(metrics_list))
                next_x = graph_x + ((i + 1) * graph_width // len(metrics_list))
                
                current_y_quality = graph_y + graph_height - (current['quality_score'] * graph_height)
                next_y_quality = graph_y + graph_height - (next_point['quality_score'] * graph_height)
                
                cv2.line(canvas, (int(current_x), int(current_y_quality)), 
                        (int(next_x), int(next_y_quality)), self.colors['quality'], 2)
                
                # ì‹ ë¢°ë„ ë¼ì¸
                current_y_conf = graph_y + graph_height - (current['confidence'] * graph_height)
                next_y_conf = graph_y + graph_height - (next_point['confidence'] * graph_height)
                
                cv2.line(canvas, (int(current_x), int(current_y_conf)), 
                        (int(next_x), int(next_y_conf)), self.colors['confidence'], 2)
        
        # ë²”ë¡€
        cv2.putText(canvas, "Quality", (graph_x + graph_width - 150, graph_y - 5), 
                   cv2.FONT_HERSHEY_SIMPLEX, 0.4, self.colors['quality'], 1)
        cv2.putText(canvas, "Confidence", (graph_x + graph_width - 80, graph_y - 5), 
                   cv2.FONT_HERSHEY_SIMPLEX, 0.4, self.colors['confidence'], 1)
    
    def draw_level_efficiency_analysis(self, canvas, system):
        """ë ˆë²¨ë³„ íš¨ìœ¨ì„± ë¶„ì„"""
        
        y_start = 480
        
        cv2.putText(canvas, "MCTS LEVEL EFFICIENCY", (20, y_start), 
                   cv2.FONT_HERSHEY_SIMPLEX, 0.6, self.colors['header'], 2)
        
        level_names = ["Meta Strategy", "Cognitive Adapt", "Combination", "Execution"]
        level_colors = [self.colors['quality'], self.colors['time'], 
                       self.colors['confidence'], self.colors['header']]
        
        for i, (level_name, color) in enumerate(zip(level_names, level_colors)):
            x_pos = 30 + i * 200
            y_pos = y_start + 30
            
            # ë ˆë²¨ ë°•ìŠ¤
            cv2.rectangle(canvas, (x_pos, y_pos), (x_pos + 180, y_pos + 80), 
                         color, 2)
            
            # ë ˆë²¨ ì´ë¦„
            cv2.putText(canvas, f"Level {i}", (x_pos + 10, y_pos + 20), 
                       cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 1)
            cv2.putText(canvas, level_name, (x_pos + 10, y_pos + 40), 
                       cv2.FONT_HERSHEY_SIMPLEX, 0.4, self.colors['text'], 1)
            
            # í†µê³„
            stats = system.level_stats.get(i, None)
            if stats:
                cv2.putText(canvas, f"Decisions: {stats.decision_count}", 
                           (x_pos + 10, y_pos + 60), 
                           cv2.FONT_HERSHEY_SIMPLEX, 0.3, self.colors['text'], 1)
                cv2.putText(canvas, f"Best: {stats.best_value:.3f}", 
                           (x_pos + 10, y_pos + 75), 
                           cv2.FONT_HERSHEY_SIMPLEX, 0.3, self.colors['text'], 1)
    
    def draw_recommendations(self, canvas, system, current_decision):
        """ì¶”ì²œ ì‚¬í•­"""
        
        y_start = 600
        
        cv2.putText(canvas, "OPTIMIZATION RECOMMENDATIONS", (20, y_start), 
                   cv2.FONT_HERSHEY_SIMPLEX, 0.6, self.colors['header'], 2)
        
        # ì„±ëŠ¥ ë¶„ì„ ê¸°ë°˜ ì¶”ì²œ
        recommendations = self.generate_recommendations(system, current_decision)
        
        for i, recommendation in enumerate(recommendations):
            cv2.putText(canvas, f"â€¢ {recommendation}", (30, y_start + 30 + i*20), 
                       cv2.FONT_HERSHEY_SIMPLEX, 0.4, self.colors['text'], 1)
    
    def generate_recommendations(self, system, decision):
        """ì„±ëŠ¥ ê¸°ë°˜ ì¶”ì²œ ìƒì„±"""
        
        recommendations = []
        
        performance_summary = system.get_current_performance_summary()
        
        # í’ˆì§ˆ ê¸°ë°˜ ì¶”ì²œ
        if decision.quality_score < 0.7:
            recommendations.append("Consider increasing MCTS iterations for better quality")
        
        # ì‹œê°„ ê¸°ë°˜ ì¶”ì²œ  
        if decision.decision_time > 0.3:
            recommendations.append("Optimize cognitive adaptation for faster decisions")
        
        # ì‹ ë¢°ë„ ê¸°ë°˜ ì¶”ì²œ
        if decision.confidence < 0.6:
            recommendations.append("Improve user state detection accuracy")
        
        # ì¼ë°˜ì  ì¶”ì²œ
        total_decisions = performance_summary.get('total_decisions', 0)
        if total_decisions > 50:
            recommendations.append("System learning: Performance improving over time")
        else:
            recommendations.append("Collecting data: More decisions needed for optimization")
        
        # ê¸°ë³¸ ì¶”ì²œì´ ì—†ì„ ê²½ìš°
        if not recommendations:
            recommendations.append("System running normally")
        
        return recommendations[:4]  # ìµœëŒ€ 4ê°œ

class PerceptionVisualizationWindow:
    """ğŸ‘ï¸ ì‹œì„ ì¶”ì  ë° ê°ì •ë¶„ë¥˜ ì‹œê°í™” ìœˆë„ìš°"""
    
    def __init__(self, width=800, height=600):
        self.width = width
        self.height = height
        self.window_name = "ğŸ‘ï¸ Perception Analysis"
        
        # ìƒ‰ìƒ
        self.colors = {
            'background': (20, 20, 30),
            'header': (0, 255, 255),  # Cyan
            'emotion': (255, 100, 100),  # Red
            'gaze': (100, 255, 100),  # Green
            'pupil': (255, 255, 100),  # Yellow
            'text': (255, 255, 255),
            'grid': (60, 60, 60),
            'panel': (40, 40, 50)
        }
        
        # ê°ì • ë¦¬ìŠ¤íŠ¸
        self.emotions = ['happy', 'surprise', 'sad', 'anger', 'disgust', 'fear', 'neutral']
        
        # ê°ì •ë³„ ìƒ‰ìƒ
        self.emotion_colors = {
            'happy': (0, 255, 0),
            'sad': (0, 0, 255),
            'anger': (0, 0, 255),
            'fear': (128, 0, 128),
            'surprise': (255, 165, 0),
            'disgust': (0, 128, 128),
            'neutral': (128, 128, 128)
        }
        
        print("ğŸ‘ï¸ Perception Visualization Window initialized")
    
    def create_perception_window(self, frame_data, cognitive_data):
        """ì‹œì„ ì¶”ì  ë° ê°ì •ë¶„ë¥˜ ì‹œê°í™” ì°½ ìƒì„±"""
        
        canvas = np.full((self.height, self.width, 3), self.colors['background'], dtype=np.uint8)
        
        # í—¤ë”
        self.draw_perception_header(canvas)
        
        # ê°ì • ë¶„ì„ ì„¹ì…˜
        self.draw_emotion_analysis(canvas, frame_data)
        
        # ì‹œì„  ì¶”ì  ì„¹ì…˜
        self.draw_gaze_tracking(canvas, frame_data)
        
        # ì¸ì§€ ë¶€í•˜ ë¶„ì„
        self.draw_cognitive_load_analysis(canvas, cognitive_data)
        
        # ì‹¤ì‹œê°„ ìƒíƒœ í‘œì‹œ
        self.draw_realtime_status(canvas, frame_data, cognitive_data)
        
        return canvas
    
    def draw_perception_header(self, canvas):
        """í—¤ë” ê·¸ë¦¬ê¸°"""
        
        cv2.putText(canvas, "PERCEPTION ANALYSIS DASHBOARD", (20, 40), 
                   cv2.FONT_HERSHEY_SIMPLEX, 0.8, self.colors['header'], 2)
        
        current_time = time.strftime("%H:%M:%S")
        cv2.putText(canvas, f"Time: {current_time}", (self.width - 150, 40), 
                   cv2.FONT_HERSHEY_SIMPLEX, 0.5, self.colors['text'], 1)
        
        cv2.line(canvas, (20, 55), (self.width - 20, 55), self.colors['header'], 2)
    
    def draw_emotion_analysis(self, canvas, frame_data):
        """ê°ì • ë¶„ì„ ì„¹ì…˜"""
        
        y_start = 80
        
        cv2.putText(canvas, "EMOTION ANALYSIS", (20, y_start), 
                   cv2.FONT_HERSHEY_SIMPLEX, 0.7, self.colors['header'], 2)
        
        # ê°ì • ì •ë³´
        emotion = frame_data.get('emotion', 'neutral')
        scores = frame_data.get('scores', {})
        
        # ê°ì • ë°•ìŠ¤
        emotion_color = self.emotion_colors.get(emotion, (128, 128, 128))
        cv2.rectangle(canvas, (30, y_start + 30), (350, y_start + 120), 
                     self.colors['panel'], -1)
        cv2.rectangle(canvas, (30, y_start + 30), (350, y_start + 120), 
                     emotion_color, 2)
        
        # í˜„ì¬ ê°ì •
        cv2.putText(canvas, f"Current Emotion: {emotion.upper()}", 
                   (40, y_start + 55), cv2.FONT_HERSHEY_SIMPLEX, 0.6, emotion_color, 2)
        
        # ê°ì • ì ìˆ˜ë“¤
        y_offset = 80
        if isinstance(scores, dict):
            for i, (emotion_name, score) in enumerate(scores.items()):
                if i < 3:  # ìƒìœ„ 3ê°œë§Œ í‘œì‹œ
                    color = self.emotion_colors.get(emotion_name, (128, 128, 128))
                    cv2.putText(canvas, f"{emotion_name}: {score:.3f}", 
                               (40, y_start + y_offset), 
                               cv2.FONT_HERSHEY_SIMPLEX, 0.4, color, 1)
                    y_offset += 20
        elif isinstance(scores, list):
            for i, score in enumerate(scores):
                if i < 3:  # ìƒìœ„ 3ê°œë§Œ í‘œì‹œ
                    emotion_name = self.emotions[i] if i < len(self.emotions) else f"emotion_{i}"
                    color = self.emotion_colors.get(emotion_name, (128, 128, 128))
                    cv2.putText(canvas, f"{emotion_name}: {score:.3f}", 
                               (40, y_start + y_offset), 
                               cv2.FONT_HERSHEY_SIMPLEX, 0.4, color, 1)
                    y_offset += 20
    
    def draw_gaze_tracking(self, canvas, frame_data):
        """ì‹œì„  ì¶”ì  ì„¹ì…˜"""
        
        y_start = 220
        
        cv2.putText(canvas, "GAZE TRACKING", (20, y_start), 
                   cv2.FONT_HERSHEY_SIMPLEX, 0.7, self.colors['header'], 2)
        
        # ì‹œì„  ì•ˆì •ì„±
        fix_stab = frame_data.get('fix_stab', 0.0)
        fix_flag = frame_data.get('fix_flag', False)
        
        # ì‹œì„  ë°•ìŠ¤
        cv2.rectangle(canvas, (30, y_start + 30), (350, y_start + 100), 
                     self.colors['panel'], -1)
        cv2.rectangle(canvas, (30, y_start + 30), (350, y_start + 100), 
                     self.colors['gaze'], 2)
        
        # ì‹œì„  ì•ˆì •ì„± í‘œì‹œ
        stability_text = "STABLE" if fix_flag else "UNSTABLE"
        stability_color = (0, 255, 0) if fix_flag else (0, 0, 255)
        
        cv2.putText(canvas, f"Gaze Status: {stability_text}", 
                   (40, y_start + 55), cv2.FONT_HERSHEY_SIMPLEX, 0.6, stability_color, 2)
        cv2.putText(canvas, f"Stability: {fix_stab:.3f}", 
                   (40, y_start + 80), cv2.FONT_HERSHEY_SIMPLEX, 0.4, self.colors['text'], 1)
    
    def draw_cognitive_load_analysis(self, canvas, cognitive_data):
        """ì¸ì§€ ë¶€í•˜ ë¶„ì„"""
        
        y_start = 340
        
        cv2.putText(canvas, "COGNITIVE LOAD ANALYSIS", (20, y_start), 
                   cv2.FONT_HERSHEY_SIMPLEX, 0.7, self.colors['header'], 2)
        
        # ì¸ì§€ ë¶€í•˜ ë ˆë²¨
        load_level = cognitive_data.get('cognitive_load_level', 'medium')
        mental_effort = cognitive_data.get('mental_effort_score', 0.5)
        pupil_rate = cognitive_data.get('pupil_dilation_rate', 0.0)
        
        # ì¸ì§€ ë¶€í•˜ ë°•ìŠ¤
        load_color = (0, 255, 0) if load_level == 'low' else (0, 255, 255) if load_level == 'medium' else (0, 0, 255)
        cv2.rectangle(canvas, (30, y_start + 30), (350, y_start + 120), 
                     self.colors['panel'], -1)
        cv2.rectangle(canvas, (30, y_start + 30), (350, y_start + 120), 
                     load_color, 2)
        
        # ì¸ì§€ ë¶€í•˜ ì •ë³´
        cv2.putText(canvas, f"Load Level: {load_level.upper()}", 
                   (40, y_start + 55), cv2.FONT_HERSHEY_SIMPLEX, 0.6, load_color, 2)
        cv2.putText(canvas, f"Mental Effort: {mental_effort:.3f}", 
                   (40, y_start + 80), cv2.FONT_HERSHEY_SIMPLEX, 0.4, self.colors['text'], 1)
        cv2.putText(canvas, f"Pupil Rate: {pupil_rate:.3f}", 
                   (40, y_start + 100), cv2.FONT_HERSHEY_SIMPLEX, 0.4, self.colors['text'], 1)
    
    def draw_realtime_status(self, canvas, frame_data, cognitive_data):
        """ì‹¤ì‹œê°„ ìƒíƒœ í‘œì‹œ"""
        
        y_start = 480
        
        cv2.putText(canvas, "REAL-TIME STATUS", (20, y_start), 
                   cv2.FONT_HERSHEY_SIMPLEX, 0.7, self.colors['header'], 2)
        
        # ìƒíƒœ ì •ë³´
        face_detected = frame_data.get('face_detected', False)
        left_pupil = frame_data.get('left_pupil', (0, 0))
        right_pupil = frame_data.get('right_pupil', (0, 0))
        
        # ìƒíƒœ ë°•ìŠ¤
        status_color = (0, 255, 0) if face_detected else (0, 0, 255)
        cv2.rectangle(canvas, (30, y_start + 30), (self.width - 30, y_start + 80), 
                     self.colors['panel'], -1)
        cv2.rectangle(canvas, (30, y_start + 30), (self.width - 30, y_start + 80), 
                     status_color, 2)
        
        # ìƒíƒœ ì •ë³´ í‘œì‹œ
        status_text = "FACE DETECTED" if face_detected else "NO FACE"
        cv2.putText(canvas, f"Status: {status_text}", 
                   (40, y_start + 55), cv2.FONT_HERSHEY_SIMPLEX, 0.6, status_color, 2)
        
        if face_detected:
            cv2.putText(canvas, f"Left Pupil: ({left_pupil[0]:.1f}, {left_pupil[1]:.1f})", 
                       (40, y_start + 80), cv2.FONT_HERSHEY_SIMPLEX, 0.4, self.colors['text'], 1)
            cv2.putText(canvas, f"Right Pupil: ({right_pupil[0]:.1f}, {right_pupil[1]:.1f})", 
                       (40, y_start + 100), cv2.FONT_HERSHEY_SIMPLEX, 0.4, self.colors['text'], 1)

# ==================== Main Integrated System ====================
class HierarchicalMCTSIntegratedSystem:
    """ğŸš€ ë©”ì¸ í†µí•© ì‹œìŠ¤í…œ - 4ë‹¨ê³„ ê³„ì¸µì  MCTS + ì‹¤ì‹œê°„ ì¸í„°í˜ì´ìŠ¤"""
    
    def __init__(self, user_name, system_mode="proposed"):
        print("ğŸš€ Hierarchical MCTS Integrated System Starting...")
        print("=" * 70)
        print("ğŸ§  REVOLUTIONARY FEATURES:")
        print("   ğŸ¯ 4-Level Hierarchical MCTS Decision Making")
        print("   ğŸ§  Real-time Cognitive Load Adaptation")  
        print("   ğŸ‘ï¸ Advanced Pupil-based Intelligence")
        print("   ğŸ¨ Triple Advanced Visualization Windows")
        print("   ğŸ“Š Comprehensive Performance Analytics")
        print("   ğŸ”¬ Research-Grade Data Collection")
        print("-" * 70)
        
        self.user_name = user_name
        self.system_mode = system_mode  # ì‹¤í—˜ ëª¨ë“œ
        
        # ë² ì´ìŠ¤ë¼ì¸ ì‹œìŠ¤í…œ ì´ˆê¸°í™” (ì‚¬ìš© ì•ˆ í•¨ - ê° ì¡°ê±´ë³„ë¡œ ì§ì ‘ êµ¬í˜„)
        self.baseline_systems = None
        print(f"âœ… Condition mode: {system_mode}")
        
        # ê¸°ë³¸ ì„¤ì •
        self.device = torch.device("mps" if torch.backends.mps.is_available() else "cpu")
        self.emotions = ['happy', 'surprise', 'sad', 'anger', 'disgust', 'fear', 'neutral']
        
        # ì„±ëŠ¥ ìµœì í™” ì„¤ì •
        cv2.setNumThreads(1)  # OpenCV ìŠ¤ë ˆë“œ ì¶©ëŒ ë°©ì§€
        torch.set_num_threads(2)  # PyTorch ìŠ¤ë ˆë“œ ìˆ˜ ì œí•œ
        if hasattr(torch, 'set_num_interop_threads'):
            torch.set_num_interop_threads(1)
        
        # AI ëª¨ë¸ ì´ˆê¸°í™”
        self.model = ResEmoteNet()
        # Emotion smoothing (EMA on probability vector)
        self.emotion_smoother = VectorEMA(alpha=0.3, length=len(self.emotions))
        self.smoothed_emotion_probs = [0.0] * len(self.emotions)
        
        # MediaPipe ì´ˆê¸°í™” (ê°€ëŠ¥í•œ ê²½ìš°)
        if MEDIAPIPE_AVAILABLE:
            self.mp_face_mesh = mp.solutions.face_mesh
            self.face_mesh = self.mp_face_mesh.FaceMesh(
                static_image_mode=False, max_num_faces=1,
                refine_landmarks=True, min_detection_confidence=0.5, min_tracking_confidence=0.5
            )
        else:
            self.mp_face_mesh = None
            self.face_mesh = None
        
        # ì‹œì„  ì¶”ì  íŒŒë¼ë¯¸í„°
        self.WIN_SEC = 2.0
        self.EMA_ALPHA = 0.3
        self.FPS_EST = 20
        self.CALIBRATION_SEC = 10
        self.MAD_MULTIPLIER = 2.5
        self.FIXSTAB_ABS_THRESH = 0.30
        self.GAZE_MAD_EPS = 1e-6
        
        # ì‹œì„  ì¶”ì  ë³€ìˆ˜ë“¤
        self.gaze_buffer = deque(maxlen=int(self.WIN_SEC * self.FPS_EST))
        self.gaze_ema_x = EMA(self.EMA_ALPHA)
        self.gaze_ema_y = EMA(self.EMA_ALPHA)
        self.area_calibration_buffer = []
        self.calibration_done = False
        self.area_median = 0
        self.area_mad = 0
        
        # ğŸ§  ì¸ì§€ ë¶€í•˜ ì¶”ì  (ë² ì´ìŠ¤ë¼ì¸ ë°©ì‹)
        self.cognitive_load_history = deque(maxlen=20)
        self.pupil_baseline = 0.0
        self.calibration_complete = False
        self.pupil_baseline_buffer = []  # 30ì´ˆê°„ ë² ì´ìŠ¤ë¼ì¸ ë°ì´í„° ìˆ˜ì§‘
        
        # ì‹œìŠ¤í…œ ì¹´ìš´í„°
        self.counter = 0
        self.frame_counter = 0
        self.evaluation_frequency = 4
        self.blink_count = 0
        self.was_blinking = False
        self.start_time = time.time()
        
        # ğŸ§  í˜ì‹ ì  ê³„ì¸µì  MCTS ì‹œìŠ¤í…œ
        self.hierarchical_mcts = HierarchicalMCTSSystem()
        
        # ğŸ¨ 4ê°œ ì‹œê°í™” ìœˆë„ìš°
        self.tree_window = HierarchicalTreeWindow()
        self.adaptation_window = CognitiveAdaptationWindow()
        self.analytics_window = PerformanceAnalyticsWindow()
        self.perception_window = PerceptionVisualizationWindow()
        # ì‹œê°í™” ë¦¬í”„ë ˆì‹œ ê°„ê²© ë° ìºì‹œ í”„ë ˆì„
        self.refresh_interval = 3
        self._last_tree_frame = None
        self._last_adaptation_frame = None
        self._last_analytics_frame = None
        self._last_perception_frame = None
        
        # CSV ë¡œê¹…
        self.setup_csv_logging()
        
        # ê°ì • ì¶”ë¡  ë¹„ë™ê¸° ì²˜ë¦¬ êµ¬ì„±
        self.emotion_infer_queue = queue.Queue(maxsize=1)
        self._emotion_thread_stop = threading.Event()
        # ì´ˆê¸° ê²°ê³¼(ì¤‘ë¦½)
        self.emotion_result = ('neutral', [0.0] * len(self.emotions))
        # ë°±ê·¸ë¼ìš´ë“œ ìŠ¤ë ˆë“œ ì‹œì‘
        self._emotion_thread = threading.Thread(target=self._emotion_worker, daemon=True)
        self._emotion_thread.start()
        
        # ë©”ëª¨ë¦¬ ìµœì í™”: í”„ë ˆì„ ë²„í¼ ì¬ì‚¬ìš©
        self._frame_buffer = None
        self._csv_batch = []
        self._csv_batch_size = 10
        
        print("ğŸ‰ Revolutionary Hierarchical MCTS System Ready!")
        print("ğŸŒŸ This is cutting-edge AI research in action!")
    
    def setup_csv_logging(self):
        """CSV ë¡œê¹… ì„¤ì •"""
        # Project-relative data directory inside workspace
        workspace_root = Path(__file__).resolve().parent
        base_dir = (workspace_root / 'data')
        base_dir.mkdir(parents=True, exist_ok=True)
        
        current_time = datetime.now().strftime('%Y%m%d_%H%M%S')
        output_dir = base_dir / current_time
        output_dir.mkdir(parents=True, exist_ok=True)
        
        self.csv_filename = output_dir / f"{self.user_name}_multi-adaptive_log.csv"
        
        # í™•ì¥ëœ ì»¬ëŸ¼ë“¤
        base_columns = ['Frame', 'Time(s)'] + self.emotions + [
            'Emotion', 'Left_Pupil_X', 'Left_Pupil_Y', 'Right_Pupil_X', 'Right_Pupil_Y', 
            'Blink', 'Blink_Count', 'FixStab', 'FixFlag'
        ]
        
        hierarchical_columns = [
            'Meta_Strategy', 'Cognitive_Adaptation', 'Agent_Combination', 'Execution_Strategy',
            'Decision_Quality', 'Decision_Time', 'Confidence', 'Tree_Depth',
            'Cognitive_Load_Level', 'Mental_Effort_Score', 'Pupil_Dilation_Rate'
        ]
        
        # GPT-4 í˜‘ì—… ê´€ë ¨ ì»¬ëŸ¼ (ë…¼ë¬¸ êµ¬í˜„)
        llm_columns = [
            'GEMMAS_IDS', 'GEMMAS_UPR', 'LLM_Feedback'
        ]
        
        with open(self.csv_filename, mode='w', newline='') as file:
            writer = csv.writer(file)
            writer.writerow(base_columns + hierarchical_columns + llm_columns)
    
    def setup_camera(self):
        """ì¹´ë©”ë¼ ì„¤ì • - íœ´ëŒ€í° ì¹´ë©”ë¼ ì‚¬ìš© (final_sac.py ìŠ¤íƒ€ì¼)"""
        print("Setting up camera for hierarchical system...")
        
        # íœ´ëŒ€í° ì¹´ë©”ë¼ ì‚¬ìš© (final_sac.py ìŠ¤íƒ€ì¼)
        preferred_indices = [1, 0, 2, 3]
        video_capture = None
        for idx in preferred_indices:
            try:
                print(f"   Trying camera index {idx} (AVFOUNDATION)")
                cap = cv2.VideoCapture(idx, cv2.CAP_AVFOUNDATION)
                cap.set(cv2.CAP_PROP_FPS, 20)
                cap.set(cv2.CAP_PROP_BUFFERSIZE, 1)  # ì§€ì—° ê°ì†Œ
                if cap.isOpened():
                    video_capture = cap
                    break
                else:
                    cap.release()
            except Exception:
                pass
        if video_capture is None:
            # Fallback to default backend
            for idx in preferred_indices:
                try:
                    print(f"   Trying camera index {idx} (default backend)")
                    cap = cv2.VideoCapture(idx)
                    cap.set(cv2.CAP_PROP_FPS, 20)
                    cap.set(cv2.CAP_PROP_BUFFERSIZE, 1)  # ì§€ì—° ê°ì†Œ
                    if cap.isOpened():
                        video_capture = cap
                        break
                    else:
                        cap.release()
                except Exception:
                    pass
        if video_capture is None:
            raise RuntimeError("Cannot open any available camera")
            
        width = int(video_capture.get(cv2.CAP_PROP_FRAME_WIDTH))
        height = int(video_capture.get(cv2.CAP_PROP_FRAME_HEIGHT))
        fps = video_capture.get(cv2.CAP_PROP_FPS)
        
        print(f"Phone Camera ready: {width}x{height} @ {fps}fps")
        
        return video_capture
    
    def _emotion_worker(self):
        """ë°±ê·¸ë¼ìš´ë“œ ê°ì • ì¶”ë¡  ì›Œì»¤"""
        while not self._emotion_thread_stop.is_set():
            try:
                face_crop = self.emotion_infer_queue.get(timeout=0.1)
            except Exception:
                continue
            try:
                label, probs = self.get_max_emotion(face_crop)
                probs = sanitize_probs(probs, len(self.emotions), self.emotions.index('neutral'))
                self.emotion_result = (label, probs)
            except Exception:
                # ì‹¤íŒ¨ ì‹œ ì´ì „ ê²°ê³¼ ìœ ì§€
                pass
    
    def get_max_emotion(self, face_image):
        """final_sac.pyì™€ ë™ì¼í•œ ê°ì • ë¶„ì„"""
        try:
            # final_sac.pyì™€ ë™ì¼í•œ ë°©ì‹
            pil_crop_img = Image.fromarray(face_image)
            rounded_scores = self.detect_emotion(pil_crop_img)
            max_index = np.argmax(rounded_scores)
            return self.emotions[max_index], rounded_scores
        except Exception as e:
            print(f"âŒ ê°ì • ë¶„ì„ ì˜¤ë¥˜: {e}")
            return 'neutral', [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0]
    
    def detect_emotion(self, video_frame):
        """final_sac.pyì˜ detect_emotion í•¨ìˆ˜"""
        vid_fr_tensor = transform(video_frame).unsqueeze(0).to(self.device)
        with torch.no_grad():
            outputs = self.model.model(vid_fr_tensor)  # self.model.modelìœ¼ë¡œ ì‹¤ì œ ëª¨ë¸ í˜¸ì¶œ
            probabilities = F.softmax(outputs, dim=1)
        scores = probabilities.cpu().numpy().flatten()
        return [round(score, 2) for score in scores]
    
    def extract_cognitive_load_data(self, face_landmarks, frame_shape):
        """ì¸ì§€ ë¶€í•˜ ë°ì´í„° ì¶”ì¶œ (ê¸°ì¡´ perception_bridgeì™€ ë™ì¼í•œ ë¡œì§)"""
        
        if not face_landmarks:
            return {
                'cognitive_load_level': 'unknown',
                'mental_effort_score': 0.5,
                'pupil_dilation_rate': 0.0,
                'avg_pupil_diameter': 0.0,
                'baseline_diameter': self.pupil_baseline,
                'confidence': 0.0
            }
        
        # ë™ê³µ í¬ê¸° ì¶”ì¶œ (MediaPipe ê¸°ë°˜ ì‹¤ì œ ê³„ì‚°)
        h, w = frame_shape[:2]
        
        # ì¢Œì•ˆ/ìš°ì•ˆ ë™ê³µ ì¶”ì • (MediaPipe ëœë“œë§ˆí¬ ê¸°ë°˜)
        left_eye_indices = [468, 469, 470, 471, 472]  # ì¢Œì•ˆ ì•„ì´ë¦¬ìŠ¤
        right_eye_indices = [473, 474, 475, 476, 477] # ìš°ì•ˆ ì•„ì´ë¦¬ìŠ¤
        
        left_diameter = self._calculate_pupil_diameter(face_landmarks, left_eye_indices, w, h)
        right_diameter = self._calculate_pupil_diameter(face_landmarks, right_eye_indices, w, h)
        
        if left_diameter is None or right_diameter is None:
            return {
                'cognitive_load_level': 'unknown',
                'mental_effort_score': 0.5,
                'pupil_dilation_rate': 0.0,
                'avg_pupil_diameter': 0.0,
                'baseline_diameter': self.pupil_baseline,
                'confidence': 0.0
            }
        
        avg_diameter = (left_diameter + right_diameter) / 2.0
        
        # ë² ì´ìŠ¤ë¼ì¸ ìˆ˜ì§‘ (ì´ˆê¸° 30ì´ˆ)
        elapsed_time = time.time() - self.start_time
        baseline_period = 30.0  # 30ì´ˆ ë² ì´ìŠ¤ë¼ì¸
        
        # ë² ì´ìŠ¤ë¼ì¸ ë°ì´í„° ìˆ˜ì§‘
        if elapsed_time <= baseline_period:
            self.pupil_baseline_buffer.append(avg_diameter)
        
        # ë² ì´ìŠ¤ë¼ì¸ ê³„ì‚° (30ì´ˆ í›„ ë˜ëŠ” ì¶©ë¶„í•œ ë°ì´í„°ê°€ ìˆì„ ë•Œ)
        if (elapsed_time > baseline_period or len(self.pupil_baseline_buffer) >= 50) and not self.calibration_complete:
            if self.pupil_baseline_buffer:
                self.pupil_baseline = np.median(self.pupil_baseline_buffer)  # ì¤‘ê°„ê°’ ì‚¬ìš© (ì´ìƒì¹˜ ì œê±°)
                self.calibration_complete = True
                print(f"Cognitive Load Baseline established: {self.pupil_baseline:.4f} (from {len(self.pupil_baseline_buffer)} samples)")
        
        # ì¸ì§€ ë¶€í•˜ ê³„ì‚° (0ì´ˆë¶€í„° ì¦‰ì‹œ ì‹œì‘)
        if self.calibration_complete and self.pupil_baseline > 0:
            # ë² ì´ìŠ¤ë¼ì¸ ê¸°ë°˜ ê³„ì‚° (30ì´ˆ í›„)
            baseline_ratio = avg_diameter / self.pupil_baseline
            
            # í™•ì¥ë¥  ê³„ì‚° (ì´ì „ ê°’ê³¼ ë¹„êµ)
            if self.cognitive_load_history:
                prev_diameter = self.cognitive_load_history[-1]['avg_pupil_diameter']
                dilation_rate = (avg_diameter - prev_diameter) / prev_diameter if prev_diameter > 0 else 0.0
            else:
                dilation_rate = 0.0
            
            # ì¸ì§€ ë¶€í•˜ ë ˆë²¨ ê²°ì •
            if baseline_ratio > 1.2:
                cognitive_level = 'high'
            elif baseline_ratio < 0.9:
                cognitive_level = 'low'
            else:
                cognitive_level = 'medium'
            
            # ì •ì‹ ì  ë…¸ë ¥ ì ìˆ˜ (0-1)
            mental_effort = min(1.0, max(0.0, (baseline_ratio - 0.8) / 0.6))
            confidence = 0.8
        else:
            # ë² ì´ìŠ¤ë¼ì¸ ìˆ˜ì§‘ ì¤‘ - ì ˆëŒ€ê°’ ê¸°ë°˜ ì„ì‹œ ê³„ì‚°
            if len(self.pupil_baseline_buffer) > 0:
                # í˜„ì¬ê¹Œì§€ì˜ ì¤‘ê°„ê°’ì„ ì„ì‹œ ë² ì´ìŠ¤ë¼ì¸ìœ¼ë¡œ ì‚¬ìš©
                temp_baseline = np.median(self.pupil_baseline_buffer)
                baseline_ratio = avg_diameter / temp_baseline if temp_baseline > 0 else 1.0
                
                if baseline_ratio > 1.15:  # ì„ì‹œ ì„ê³„ê°’ (ë” ê´€ëŒ€í•˜ê²Œ)
                    cognitive_level = 'high'
                elif baseline_ratio < 0.95:
                    cognitive_level = 'low'
                else:
                    cognitive_level = 'medium'
                
                mental_effort = min(1.0, max(0.0, (baseline_ratio - 0.8) / 0.6))
                confidence = 0.5  # ì¤‘ê°„ ì‹ ë¢°ë„
            else:
                # ì™„ì „ ì´ˆê¸°ê°’
                cognitive_level = 'medium'
                mental_effort = 0.5
                confidence = 0.3
            
            # í™•ì¥ë¥  ê³„ì‚°
            if self.cognitive_load_history:
                prev_diameter = self.cognitive_load_history[-1]['avg_pupil_diameter']
                dilation_rate = (avg_diameter - prev_diameter) / prev_diameter if prev_diameter > 0 else 0.0
            else:
                dilation_rate = 0.0
        
        cognitive_data = {
            'cognitive_load_level': cognitive_level,
            'mental_effort_score': mental_effort,
            'pupil_dilation_rate': dilation_rate,
            'avg_pupil_diameter': avg_diameter,
            'baseline_diameter': self.pupil_baseline,
            'confidence': confidence
        }
        
        # íˆìŠ¤í† ë¦¬ ì €ì¥
        self.cognitive_load_history.append(cognitive_data)
        
        return cognitive_data
    
    def _calculate_pupil_diameter(self, landmarks, indices, width, height):
        """ë™ê³µ ì§ê²½ ì¶”ì •"""
        
        if len(indices) < 5:
            return None
            
        points = []
        for idx in indices:
            if idx < len(landmarks.landmark):
                x = landmarks.landmark[idx].x * width
                y = landmarks.landmark[idx].y * height
                points.append((x, y))
        
        if len(points) < 3:
            return None
        
        # ë™ê³µ ì˜ì—­ì˜ ë°”ìš´ë”© ë°•ìŠ¤ë¡œ ì§€ë¦„ ì¶”ì •
        points = np.array(points)
        min_x, max_x = np.min(points[:, 0]), np.max(points[:, 0])
        min_y, max_y = np.min(points[:, 1]), np.max(points[:, 1])
        
        diameter = max(max_x - min_x, max_y - min_y)
        
        # ì •ê·œí™” (ì´ë¯¸ì§€ í¬ê¸° ëŒ€ë¹„)
        normalized_diameter = diameter / min(width, height)
        
        return normalized_diameter
    
    def generate_agent_discussions(self, emotion, cognitive_data, fix_stab, fix_flag):
        """ğŸ¤– ì—ì´ì „íŠ¸ë“¤ì´ í† ë¡ í•˜ëŠ” ë‚´ìš© ìƒì„±"""
        discussions = []
        
        # Meta Agent í† ë¡ 
        if emotion == 'neutral':
            discussions.append("Meta Agent: ì‚¬ìš©ìê°€ ì¤‘ë¦½ ìƒíƒœì…ë‹ˆë‹¤. í‘œì¤€ ì „ëµì„ ìœ ì§€í•˜ê² ìŠµë‹ˆë‹¤.")
        elif emotion in ['happy', 'surprise']:
            discussions.append("Meta Agent: ê¸ì •ì  ê°ì • ê°ì§€! íƒìƒ‰ ë²”ìœ„ë¥¼ í™•ëŒ€í•˜ê² ìŠµë‹ˆë‹¤.")
        elif emotion in ['sad', 'anger', 'fear']:
            discussions.append("Meta Agent: ë¶€ì •ì  ê°ì • ê°ì§€! ì‹ ì¤‘í•œ ì ‘ê·¼ì´ í•„ìš”í•©ë‹ˆë‹¤.")
        
        # Cognitive Agent í† ë¡ 
        cognitive_load = cognitive_data.get('cognitive_load_level', 'medium')
        if cognitive_load == 'low':
            discussions.append("Cognitive Agent: ì¸ì§€ ë¶€í•˜ê°€ ë‚®ìŠµë‹ˆë‹¤. ë³µì¡í•œ ì‘ì—…ì„ ì œì•ˆí•©ë‹ˆë‹¤.")
        elif cognitive_load == 'high':
            discussions.append("Cognitive Agent: ì¸ì§€ ë¶€í•˜ê°€ ë†’ìŠµë‹ˆë‹¤. ë‹¨ìˆœí™”ëœ ì ‘ê·¼ì´ í•„ìš”í•©ë‹ˆë‹¤.")
        else:
            discussions.append("Cognitive Agent: ì ì ˆí•œ ì¸ì§€ ë¶€í•˜ ìƒíƒœì…ë‹ˆë‹¤. í˜„ì¬ ì „ëµì„ ìœ ì§€í•©ë‹ˆë‹¤.")
        
        # Perception Agent í† ë¡ 
        if fix_flag == 1:
            discussions.append("Perception Agent: ì‚¬ìš©ìê°€ ì‚°ë§Œí•´ ë³´ì…ë‹ˆë‹¤. ì£¼ì˜ë¥¼ ì§‘ì¤‘ì‹œì¼œì•¼ í•©ë‹ˆë‹¤.")
        else:
            discussions.append("Perception Agent: ì§‘ì¤‘ë„ê°€ ì–‘í˜¸í•©ë‹ˆë‹¤. í˜„ì¬ ì‘ì—…ì„ ê³„ì†í•©ë‹ˆë‹¤.")
        
        # Combination Agent í† ë¡ 
        if fix_stab is not None:
            if fix_stab > 0.7:
                discussions.append("Combination Agent: ì•ˆì •ì ì¸ ì‹œì„  íŒ¨í„´ì…ë‹ˆë‹¤. ì •ë°€í•œ ì‘ì—…ì´ ê°€ëŠ¥í•©ë‹ˆë‹¤.")
            elif fix_stab < 0.3:
                discussions.append("Combination Agent: ë¶ˆì•ˆì •í•œ ì‹œì„  íŒ¨í„´ì…ë‹ˆë‹¤. ë‹¨ìˆœí•œ ì‘ì—…ìœ¼ë¡œ ì „í™˜í•©ë‹ˆë‹¤.")
            else:
                discussions.append("Combination Agent: ë³´í†µ ìˆ˜ì¤€ì˜ ì‹œì„  ì•ˆì •ì„±ì…ë‹ˆë‹¤. ê· í˜•ì¡íŒ ì ‘ê·¼ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.")
        
        # Execution Agent í† ë¡ 
        mental_effort = cognitive_data.get('mental_effort_score', 0.5)
        if mental_effort > 0.7:
            discussions.append("Execution Agent: ë†’ì€ ì •ì‹ ì  ë…¸ë ¥ì´ ê°ì§€ë©ë‹ˆë‹¤. ì‘ì—… ê°•ë„ë¥¼ ì¡°ì ˆí•©ë‹ˆë‹¤.")
        elif mental_effort < 0.3:
            discussions.append("Execution Agent: ë‚®ì€ ì •ì‹ ì  ë…¸ë ¥ì…ë‹ˆë‹¤. ë” ë„ì „ì ì¸ ì‘ì—…ì„ ì œì•ˆí•©ë‹ˆë‹¤.")
        else:
            discussions.append("Execution Agent: ì ì ˆí•œ ì •ì‹ ì  ë…¸ë ¥ ìˆ˜ì¤€ì…ë‹ˆë‹¤. í˜„ì¬ ì‘ì—…ì„ ìœ ì§€í•©ë‹ˆë‹¤.")
        
        # ì‹œìŠ¤í…œ ìƒíƒœ í† ë¡ 
        pupil_rate = cognitive_data.get('pupil_dilation_rate', 0.0)
        if pupil_rate > 0.1:
            discussions.append("System: ë™ê³µ í™•ì¥ì´ ê°ì§€ë˜ì—ˆìŠµë‹ˆë‹¤. ì¸ì§€ ë¶€í•˜ê°€ ì¦ê°€í•˜ê³  ìˆìŠµë‹ˆë‹¤.")
        elif pupil_rate < -0.1:
            discussions.append("System: ë™ê³µ ìˆ˜ì¶•ì´ ê°ì§€ë˜ì—ˆìŠµë‹ˆë‹¤. ì¸ì§€ ë¶€í•˜ê°€ ê°ì†Œí•˜ê³  ìˆìŠµë‹ˆë‹¤.")
        else:
            discussions.append("System: ë™ê³µ í¬ê¸°ê°€ ì•ˆì •ì ì…ë‹ˆë‹¤. í˜„ì¬ ìƒíƒœë¥¼ ìœ ì§€í•©ë‹ˆë‹¤.")
        
        return discussions
    
    def process_camera_frame(self, frame):
        """ğŸ§  ì¹´ë©”ë¼ í”„ë ˆì„ ì²˜ë¦¬ - ê³„ì¸µì  MCTS í†µí•©"""
        
        # ì…ë ¥ í”„ë ˆì„ ë‹¤ìš´ìŠ¤ì¼€ì¼(ì²˜ë¦¬ ë¶€í•˜ ì ˆê°)
        try:
            h, w = frame.shape[:2]
            max_w, max_h = 960, 540
            scale = min(1.0, max_w / max(w, 1), max_h / max(h, 1))
            if scale < 1.0:
                new_w = int(w * scale)
                new_h = int(h * scale)
                frame = cv2.resize(frame, (new_w, new_h))
        except Exception:
            pass

        # ê¸°ë³¸ê°’ë“¤
        max_emotion = 'neutral'
        scores = [0.0] * len(self.emotions)
        fix_stab = 0.5
        fix_flag = 0
        face_detected = False
        left_pupil = (0, 0)
        right_pupil = (0, 0)
        
        # ì¸ì§€ ë¶€í•˜ ê¸°ë³¸ ë°ì´í„°
        cognitive_data = {
            'cognitive_load_level': 'medium',
            'mental_effort_score': 0.5,
            'pupil_dilation_rate': 0.0,
            'avg_pupil_diameter': 0.0,
            'baseline_diameter': self.pupil_baseline,
            'confidence': 0.0
        }
        
        # MediaPipe ì²˜ë¦¬ (ê°€ëŠ¥í•œ ê²½ìš°)
        if MEDIAPIPE_AVAILABLE and self.face_mesh is not None:
            frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
            results = self.face_mesh.process(frame_rgb)
            
            if results.multi_face_landmarks:
                face_detected = True
                
                for face_landmarks in results.multi_face_landmarks:
                    h, w, _ = frame.shape
                
                # ğŸ‘ï¸ ì‹œì„  ì¶”ì  ë° ëˆˆ ê°ì§€
                left_eye_idxs = [362, 385, 387, 263, 373, 380]
                right_eye_idxs = [33, 160, 158, 133, 153, 144]
                left_eye_points = [(int(face_landmarks.landmark[i].x * w), int(face_landmarks.landmark[i].y * h)) for i in left_eye_idxs]
                right_eye_points = [(int(face_landmarks.landmark[i].x * w), int(face_landmarks.landmark[i].y * h)) for i in right_eye_idxs]
                
                # ëˆˆ í¬ì¸íŠ¸ í‘œì‹œ
                for pt in left_eye_points + right_eye_points:
                    cv2.circle(frame, pt, 2, (0, 255, 255), -1)
                
                # ê¹œë°•ì„ ê°ì§€
                left_ear = calculate_ear(left_eye_points)
                right_ear = calculate_ear(right_eye_points)
                avg_ear = (left_ear + right_ear) / 2.0
                is_blinking = avg_ear < 0.2
                
                if not self.was_blinking and is_blinking:
                    self.blink_count += 1
                    
                self.was_blinking = is_blinking
                left_pupil = left_eye_points[0]
                right_pupil = right_eye_points[0]
                
                # ===== final_sac.pyì˜ ì‹œì„ ì¶”ì  ë¡œì§ =====
                # í™ì±„ í¬ì¸íŠ¸ (MediaPipe 468-477)
                left_iris_indices = list(range(468, 473))  # 468-472
                right_iris_indices = list(range(473, 478))  # 473-477

                # í™ì±„ ì¤‘ì‹¬ê³¼ ë°˜ê²½ ê³„ì‚°
                left_iris_cx, left_iris_cy, left_iris_r = iris_center_radius(
                    face_landmarks.landmark, left_iris_indices, w, h)
                right_iris_cx, right_iris_cy, right_iris_r = iris_center_radius(
                    face_landmarks.landmark, right_iris_indices, w, h)

                # IPD ê³„ì‚° ë° ì‹œì„  ì¢Œí‘œ
                if left_iris_cx is not None and right_iris_cx is not None:
                    # IPD (Inter-Pupillary Distance)
                    ipd = np.sqrt((left_iris_cx - right_iris_cx)**2 + (left_iris_cy - right_iris_cy)**2)
                    
                    # ì‹œì„  ì¤‘ì 
                    mx = (left_iris_cx + right_iris_cx) / 2
                    my = (left_iris_cy + right_iris_cy) / 2
                    
                    # IPD ì •ê·œí™”
                    if ipd > 0:
                        gaze_x_raw = mx / ipd
                        gaze_y_raw = my / ipd
                        
                        # EMA í‰í™œí™”
                        gaze_x = self.gaze_ema_x.update(gaze_x_raw)
                        gaze_y = self.gaze_ema_y.update(gaze_y_raw)
                        
                        # ì‹œì„  ë²„í¼ì— ì¶”ê°€
                        self.gaze_buffer.append((gaze_x, gaze_y))

                # ===== Fixation Stability ê³„ì‚° =====
                elapsed_time = time.time() - self.start_time
                
                # ë² ì´ìŠ¤ë¼ì¸ ìˆ˜ì§‘ (ì´ˆê¸° 30ì´ˆ)
                baseline_period = 30.0  # 30ì´ˆ ë² ì´ìŠ¤ë¼ì¸
                if elapsed_time <= baseline_period:
                    if len(self.gaze_buffer) >= int(self.WIN_SEC * self.FPS_EST * 0.5):
                        area, _ = calculate_fixation_stability(self.gaze_buffer)
                        if area is not None:
                            self.area_calibration_buffer.append(area)
                
                # ë² ì´ìŠ¤ë¼ì¸ í†µê³„ ì—…ë°ì´íŠ¸ (30ì´ˆ í›„ì—ë„ ê³„ì† ì—…ë°ì´íŠ¸)
                if elapsed_time > baseline_period and self.area_calibration_buffer and not self.calibration_done:
                    self.area_median = np.median(self.area_calibration_buffer)
                    self.area_mad = calculate_mad(np.array(self.area_calibration_buffer))
                    # ì•ˆì „ í•˜í•œ ì ìš© (MAD=0 ë°©ì§€)
                    if self.area_mad < self.GAZE_MAD_EPS:
                        self.area_mad = self.GAZE_MAD_EPS
                    self.calibration_done = True
                    print(f"Gaze Baseline established (30s): Median={self.area_median:.4f}, MAD={self.area_mad:.4f}")

                # Fixation ê³„ì‚° (0ì´ˆë¶€í„° ì¦‰ì‹œ ì‹œì‘)
                if len(self.gaze_buffer) >= int(self.WIN_SEC * self.FPS_EST * 0.5):
                    fix_area, fix_stab = calculate_fixation_stability(self.gaze_buffer)
                    
                    if fix_area is not None and fix_stab is not None:
                        # ì‚°ë§Œ íŒì • (FixFlag)
                        if self.calibration_done and self.area_median > 0:
                            # ë² ì´ìŠ¤ë¼ì¸ ê¸°ë°˜ íŒì • (30ì´ˆ í›„)
                            unstable_thresh = self.area_median + self.MAD_MULTIPLIER * max(self.area_mad, self.GAZE_MAD_EPS)
                            fix_flag = 1 if fix_area > unstable_thresh else 0
                        else:
                            # ì ˆëŒ€ ì„ê³„ê°’ ì‚¬ìš© (ë² ì´ìŠ¤ë¼ì¸ ìˆ˜ì§‘ ì¤‘ ë˜ëŠ” ë°ì´í„° ë¶€ì¡±)
                            fix_flag = 1 if fix_stab <= self.FIXSTAB_ABS_THRESH else 0
                
                # ğŸ§  ì¸ì§€ ë¶€í•˜ ë°ì´í„° ì¶”ì¶œ
                cognitive_data = self.extract_cognitive_load_data(face_landmarks, frame.shape)
                
                # ğŸ˜Š ê°ì • ë¶„ì„ (ì§ì ‘ ì²˜ë¦¬ + ìŠ¤ë¬´ë”©)
                if self.counter == 0:
                    bbox_x = int(min([lm.x for lm in face_landmarks.landmark]) * w)
                    bbox_y = int(min([lm.y for lm in face_landmarks.landmark]) * h)
                    bbox_w = int((max([lm.x for lm in face_landmarks.landmark]) - min([lm.x for lm in face_landmarks.landmark])) * w)
                    bbox_h = int((max([lm.y for lm in face_landmarks.landmark]) - min([lm.y for lm in face_landmarks.landmark])) * h)
                    
                    face_crop = frame[bbox_y:bbox_y + bbox_h, bbox_x:bbox_x + bbox_w]
                    if face_crop.size > 0:
                        # ì§ì ‘ ê°ì •ë¶„ë¥˜ (final_sac.py ë°©ì‹)
                        max_emotion, raw_scores = self.get_max_emotion(face_crop)
                        # ìŠ¤ë¬´ë”© ì ìš©
                        smoothed = self.emotion_smoother.update(raw_scores)
                        # Normalize to sum 1 to avoid drift
                        denom = float(np.sum(smoothed)) if np.sum(smoothed) > 1e-8 else 1.0
                        scores = (smoothed / denom).tolist()
                        
                        # ë¹„ë™ê¸° íì—ë„ ì œì¶œ (ë°±ê·¸ë¼ìš´ë“œ ì²˜ë¦¬ìš©)
                        try:
                            while not self.emotion_infer_queue.empty():
                                _ = self.emotion_infer_queue.get_nowait()
                            self.emotion_infer_queue.put_nowait(face_crop)
                        except Exception:
                            pass
                
                # ì–¼êµ´ ë°•ìŠ¤ í‘œì‹œ
                bbox_x = int(min([lm.x for lm in face_landmarks.landmark]) * w)
                bbox_y = int(min([lm.y for lm in face_landmarks.landmark]) * h)
                bbox_w = int((max([lm.x for lm in face_landmarks.landmark]) - min([lm.x for lm in face_landmarks.landmark])) * w)
                bbox_h = int((max([lm.y for lm in face_landmarks.landmark]) - min([lm.y for lm in face_landmarks.landmark])) * h)
                
                # Enhanced face visualization
                cv2.rectangle(frame, (bbox_x, bbox_y), (bbox_x + bbox_w, bbox_y + bbox_h), (0, 255, 0), 3)
                cv2.putText(frame, f"EMOTION: {max_emotion.upper()}", (bbox_x, bbox_y - 15), 
                           cv2.FONT_HERSHEY_SIMPLEX, 1.0, (0, 255, 0), 2)
                
                # Emotion confidence bar
                emotion_confidence = max(scores) if scores else 0.5
                bar_width = int(bbox_w * emotion_confidence)
                cv2.rectangle(frame, (bbox_x, bbox_y - 35), (bbox_x + bar_width, bbox_y - 25), (0, 255, 0), -1)
                cv2.rectangle(frame, (bbox_x, bbox_y - 35), (bbox_x + bbox_w, bbox_y - 25), (255, 255, 255), 1)
                
                # Pupil centers visualization
                if left_pupil != (0, 0) and right_pupil != (0, 0):
                    cv2.circle(frame, left_pupil, 5, (255, 0, 255), -1)  # Left pupil - Magenta
                    cv2.circle(frame, right_pupil, 5, (255, 0, 255), -1)  # Right pupil - Magenta
                    cv2.putText(frame, "L", (left_pupil[0] - 10, left_pupil[1] - 10), 
                               cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 0, 255), 1)
                    cv2.putText(frame, "R", (right_pupil[0] - 10, right_pupil[1] - 10), 
                               cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 0, 255), 1)
                
                # Gaze direction indicator
                if len(self.gaze_buffer) > 5:
                    recent_gaze = list(self.gaze_buffer)[-5:]
                    avg_gaze_x = np.mean([g[0] for g in recent_gaze])
                    avg_gaze_y = np.mean([g[1] for g in recent_gaze])
                    
                    # Gaze direction line (í™”ì‚´í‘œ ì œê±°)
                    gaze_center_x = bbox_x + bbox_w // 2
                    gaze_center_y = bbox_y + bbox_h // 2
                    gaze_end_x = int(gaze_center_x + (avg_gaze_x - 0.5) * 100)
                    gaze_end_y = int(gaze_center_y + (avg_gaze_y - 0.5) * 100)
                    
                    # Removed: visual gaze line and label to avoid sky-blue arrow/line on screen
                    # cv2.line(frame, (gaze_center_x, gaze_center_y), (gaze_end_x, gaze_end_y), (255, 255, 0), 3)
                    # cv2.putText(frame, "GAZE", (gaze_end_x + 5, gaze_end_y), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 0), 1)
        
        # ğŸ§  ì‚¬ìš©ì ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
        user_context = {
            'emotion': max_emotion,
            'attention': clamp_float(fix_stab if fix_stab is not None else 0.5, 0.0, 1.0, 0.5),
            'cognitive_load_level': cognitive_data['cognitive_load_level'],
            'mental_effort_score': clamp_float(cognitive_data['mental_effort_score'], 0.0, 1.0, 0.5),
            'pupil_dilation_rate': clamp_float(cognitive_data['pupil_dilation_rate'], -1.0, 1.0, 0.0),
            'avg_pupil_diameter': clamp_float(cognitive_data['avg_pupil_diameter'], 0.0, 1.0, 0.0),
            'baseline_diameter': clamp_float(cognitive_data['baseline_diameter'], 0.0, 1.0, 0.0),
            'confidence': clamp_float(cognitive_data['confidence'], 0.0, 1.0, 0.0),
            # ë² ì´ìŠ¤ë¼ì¸ ë°©ì‹: calibration_in_progress í”Œë˜ê·¸ ì œê±° (0ì´ˆë¶€í„° ì¦‰ì‹œ ë¶„ì„)
            # ë³´ìƒ ë³´ì™„ìš© í•„ë“œ
            'face_detected': bool(face_detected),
            'emotion_probabilities': sanitize_probs(scores, len(self.emotions), self.emotions.index('neutral'))  # R_emoì—ì„œ ì‚¬ìš©
        }
        
        # ğŸš€ ê³„ì¸µì  MCTS ì˜ì‚¬ê²°ì •
        # ì‹œìŠ¤í…œ ëª¨ë“œì— ë”°ë¼ ë‹¤ë¥¸ ì˜ì‚¬ê²°ì •
        if self.system_mode == "proposed":
            hierarchical_decision = self.hierarchical_mcts.hierarchical_decision_making(user_context, face_detected)
        else:
            # ë² ì´ìŠ¤ë¼ì¸ ì‹œìŠ¤í…œ ì‚¬ìš©
            baseline_system = self.baseline_systems[self.system_mode]
            hierarchical_decision = baseline_system.search(user_context)
        
        # Enhanced camera window information display
        elapsed_time_display = round(time.time() - self.start_time, 2)
        
        # Background panel for better readability
        cv2.rectangle(frame, (5, 5), (600, 250), (0, 0, 0), -1)
        cv2.rectangle(frame, (5, 5), (600, 250), (100, 100, 100), 2)
        
        # ì•ˆì „í•œ ë³€ìˆ˜ ì²˜ë¦¬
        safe_fix_stab = fix_stab if fix_stab is not None else 0.5
        safe_left_pupil = f"({left_pupil[0]:.0f}, {left_pupil[1]:.0f})" if left_pupil != (0, 0) else "(0, 0)"
        safe_right_pupil = f"({right_pupil[0]:.0f}, {right_pupil[1]:.0f})" if right_pupil != (0, 0) else "(0, 0)"
        safe_cognitive_level = cognitive_data.get('cognitive_load_level', 'medium')
        safe_effort = cognitive_data.get('mental_effort_score', 0.5)
        safe_pupil_size = cognitive_data.get('avg_pupil_diameter', 0.0)
        safe_pupil_rate = cognitive_data.get('pupil_dilation_rate', 0.0)
        safe_meta_strategy = getattr(hierarchical_decision, 'meta_strategy', 'adaptive')
        safe_adaptation = getattr(hierarchical_decision, 'cognitive_adaptation', 'standard')
        safe_quality = getattr(hierarchical_decision, 'quality_score', 0.5)
        
        info_texts = [
            f"â±ï¸ Time: {elapsed_time_display}s | ğŸ‘¤ User: {self.user_name}",
            f"ğŸ˜Š Emotion: {max_emotion.upper()} | ğŸ¯ Focus: {safe_fix_stab:.3f}",
            f"ğŸ‘ï¸ Pupil L: {safe_left_pupil} | R: {safe_right_pupil}",
            f"ğŸ§  Cognitive: {safe_cognitive_level} | Effort: {safe_effort:.2f}",
            f"ğŸ“Š Pupil Size: {safe_pupil_size:.2f}mm | Rate: {safe_pupil_rate:.3f}",
            f"ğŸ”„ Distracted: {'YES' if fix_flag else 'NO'} | ğŸ‘ï¸ Blinks: {self.blink_count}",
            f"âš™ï¸ Calibration: {'DONE' if self.calibration_done else 'IN PROGRESS'}",
            f"ğŸ¯ Meta Strategy: {safe_meta_strategy}",
            f"ğŸ§  Adaptation: {safe_adaptation}",
            f"ğŸ“Š Quality Score: {safe_quality:.3f}",
            "",
            "ğŸ” See Hierarchical MCTS Hub for detailed analysis!"
        ]
        
        for i, text in enumerate(info_texts):
            if text:
                if "Hierarchical MCTS Hub" in text:
                    color = (0, 255, 255)  # Cyan
                elif "Pupil" in text or "Cognitive" in text:
                    color = (255, 0, 255)  # Magenta
                elif "Emotion" in text:
                    color = (0, 255, 0)  # Green
                elif "Meta Strategy" in text or "Adaptation" in text or "Quality" in text:
                    color = (255, 215, 0)  # Gold
                else:
                    color = (255, 255, 255)  # White
                
                # í•œêµ­ì–´ ë Œë”ëŸ¬ ì‚¬ìš©
                frame = put_korean_text(frame, text, (10, 25 + i*20), 
                                      font_size=16, color=color, bg_color=(0, 0, 0), padding=2)
        
        # ğŸ¤– ì—ì´ì „íŠ¸ í† ë¡  ë‚´ìš© ìƒì„±
        agent_discussions = self.generate_agent_discussions(max_emotion, cognitive_data, fix_stab, fix_flag)
        
        # ì—ì´ì „íŠ¸ í† ë¡  ë‚´ìš©ì„ í™”ë©´ì— í‘œì‹œ
        discussion_y_start = frame.shape[0] - 200
        for i, discussion in enumerate(agent_discussions[:8]):  # ìµœëŒ€ 8ê°œë§Œ í‘œì‹œ
            color = (255, 255, 0) if "Meta" in discussion else (0, 255, 255) if "Cognitive" in discussion else (255, 0, 255) if "Perception" in discussion else (0, 255, 0)
            frame = put_korean_text(frame, discussion, (10, discussion_y_start + i*22), 
                                   font_size=16, color=color, bg_color=(20, 20, 20), padding=3)
        
        return frame, {
            'emotion': max_emotion,
            'scores': scores,
            'fix_stab': fix_stab,
            'fix_flag': fix_flag,
            'face_detected': face_detected,
            'left_pupil': left_pupil,
            'right_pupil': right_pupil,
            'hierarchical_decision': hierarchical_decision,
            'user_context': user_context,
            'cognitive_data': cognitive_data,
            'agent_discussions': agent_discussions
        }
    
    def create_combined_visualization(self, tree_frame, adaptation_frame, analytics_frame, perception_frame):
        """4ê°œ ì‹œê°í™”ë¥¼ í•˜ë‚˜ì˜ ìœˆë„ìš°ë¡œ í†µí•©"""
        
        # í†µí•© ìº”ë²„ìŠ¤ í¬ê¸° (2x2 ê·¸ë¦¬ë“œ)
        canvas_width = 1800
        canvas_height = 1200
        combined_canvas = np.full((canvas_height, canvas_width, 3), (20, 20, 30), dtype=np.uint8)
        
        # ê° í”„ë ˆì„ í¬ê¸° ì¡°ì •
        cell_width = canvas_width // 2
        cell_height = canvas_height // 2
        
        # 1ï¸âƒ£ Decision Tree (ì¢Œìƒë‹¨)
        tree_resized = cv2.resize(tree_frame, (cell_width, cell_height))
        combined_canvas[0:cell_height, 0:cell_width] = tree_resized
        
        # 2ï¸âƒ£ Cognitive Adaptation (ìš°ìƒë‹¨)  
        adaptation_resized = cv2.resize(adaptation_frame, (cell_width, cell_height))
        combined_canvas[0:cell_height, cell_width:canvas_width] = adaptation_resized
        
        # 3ï¸âƒ£ Performance Analytics (ì¢Œí•˜ë‹¨)
        analytics_resized = cv2.resize(analytics_frame, (cell_width, cell_height))
        combined_canvas[cell_height:canvas_height, 0:cell_width] = analytics_resized
        
        # 4ï¸âƒ£ Perception Analysis (ìš°í•˜ë‹¨)
        perception_resized = cv2.resize(perception_frame, (cell_width, cell_height))
        combined_canvas[cell_height:canvas_height, cell_width:canvas_width] = perception_resized
        
        # êµ¬ë¶„ì„  ì¶”ê°€
        cv2.line(combined_canvas, (cell_width, 0), (cell_width, canvas_height), (100, 100, 100), 2)
        cv2.line(combined_canvas, (0, cell_height), (canvas_width, cell_height), (100, 100, 100), 2)
        
        # ì¤‘ì•™ ì œëª©
        cv2.putText(combined_canvas, "HIERARCHICAL MCTS INTEGRATED HUB", 
                   (canvas_width//2 - 200, 30), 
                   cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 255, 255), 2)
        
        return combined_canvas
    
    def run(self):
        """ğŸš€ ë©”ì¸ 4-ìœˆë„ìš° ì‹¤í–‰ ë£¨í”„"""
        try:
            cap = self.setup_camera()
            
            print("\nğŸŒŸ REVOLUTIONARY HIERARCHICAL MCTS SYSTEM RUNNING...")
            print("ğŸ–¼ï¸ Windows (2-Window Layout):")
            print("   1. ğŸ“¹ Emotion Estimation & Eye Tracker: Real-time face & cognitive tracking")
            print("   2. ğŸ§  Hierarchical MCTS Hub: Integrated 4-panel visualization")
            print("      - ğŸŒ² Decision Tree (Top-Left)")
            print("      - ğŸ§  Cognitive Adaptation (Top-Right)")
            print("      - ğŸ“Š Performance Analytics (Bottom-Left)")
            print("      - ğŸ‘ï¸ Perception Analysis (Bottom-Right)")
            print("ğŸ® Controls: Q = Quit")
            print("=" * 70)
            
            while True:
                success, frame = cap.read()
                if not success:
                    print("Failed to read frame")
                    break
                
                # ğŸ§  ì¹´ë©”ë¼ í”„ë ˆì„ ì²˜ë¦¬ ë° ê³„ì¸µì  ì˜ì‚¬ê²°ì •
                camera_frame, frame_data = self.process_camera_frame(frame)
                
                # ğŸ¨ 3ê°œ ì‹œê°í™” ìœˆë„ìš° ìƒì„±
                hierarchical_decision = frame_data['hierarchical_decision']
                user_context = frame_data['user_context']
                
                # ì‹œê°í™” í”„ë ˆì„ ìƒì„±(ë¦¬í”„ë ˆì‹œ ê°„ê²© ì ìš©)
                if (self.frame_counter % self.refresh_interval == 0 or
                    self._last_tree_frame is None):
                    # 1ï¸âƒ£ ê³„ì¸µì  ì˜ì‚¬ê²°ì • íŠ¸ë¦¬
                    self._last_tree_frame = self.tree_window.create_tree_window(
                        hierarchical_decision,
                        self.hierarchical_mcts.get_current_performance_summary()
                    )
                    # 2ï¸âƒ£ ì¸ì§€ ì ì‘ ì‹œê°í™”
                    self._last_adaptation_frame = self.adaptation_window.create_adaptation_window(
                        user_context,
                        hierarchical_decision,
                        self.hierarchical_mcts.adaptation_history
                    )
                    # 3ï¸âƒ£ ì„±ëŠ¥ ë¶„ì„ ëŒ€ì‹œë³´ë“œ
                    self._last_analytics_frame = self.analytics_window.create_analytics_window(
                        self.hierarchical_mcts,
                        hierarchical_decision
                    )
                    # 4ï¸âƒ£ ì‹œì„ ì¶”ì  ë° ê°ì •ë¶„ë¥˜ ì‹œê°í™”
                    self._last_perception_frame = self.perception_window.create_perception_window(
                        frame_data,
                        frame_data['cognitive_data']
                    )
                tree_frame = self._last_tree_frame
                adaptation_frame = self._last_adaptation_frame
                analytics_frame = self._last_analytics_frame
                perception_frame = self._last_perception_frame
                
                # 5ï¸âƒ£ í†µí•© ì‹œê°í™” ìœˆë„ìš° ìƒì„± (4ê°œ ì‹œê°í™”ë¥¼ í•˜ë‚˜ë¡œ)
                combined_visualization_frame = self.create_combined_visualization(
                    tree_frame, adaptation_frame, analytics_frame, perception_frame
                )
                
                # ğŸ–¥ï¸ 2ê°œ ìœˆë„ìš°ë¡œ ë¶„ë¦¬ (final_sac.py ìŠ¤íƒ€ì¼ + MCTS Hub)
                cv2.imshow("Emotion Estimation & Eye Tracker", camera_frame)
                cv2.imshow("ğŸ§  Hierarchical MCTS Hub", combined_visualization_frame)
                
                # ğŸ“Š CSV ë¡œê¹… (ë°°ì¹˜ ì²˜ë¦¬)
                if self.frame_counter % 30 == 0:
                    elapsed_time = time.time() - self.start_time
                    
                    base_data = [
                        self.frame_counter, elapsed_time
                    ] + sanitize_probs(frame_data['scores'], len(self.emotions), self.emotions.index('neutral')) + [
                        frame_data['emotion'],
                        clamp_float(frame_data['left_pupil'][0] if frame_data['left_pupil'] else 0.0, -1e6, 1e6, 0.0),
                        clamp_float(frame_data['left_pupil'][1] if frame_data['left_pupil'] else 0.0, -1e6, 1e6, 0.0),
                        clamp_float(frame_data['right_pupil'][0] if frame_data['right_pupil'] else 0.0, -1e6, 1e6, 0.0),
                        clamp_float(frame_data['right_pupil'][1] if frame_data['right_pupil'] else 0.0, -1e6, 1e6, 0.0),
                        int(bool(self.was_blinking)), int(self.blink_count),
                        clamp_float(frame_data['fix_stab'] if frame_data['fix_stab'] is not None else 0.5, 0.0, 1.0, 0.5),
                        int(bool(frame_data['fix_flag']))
                    ]
                    
                    # ë³´ìƒ êµ¬ì„±ìš”ì†Œ(ë ˆë²¨2) ë¡œê¹… í™•ì¥
                    comb_mcts = getattr(self.hierarchical_mcts, 'combination_mcts', None)
                    reward_comp = getattr(comb_mcts, 'last_reward_components', {}) if comb_mcts is not None else {}
                    hierarchical_data = [
                        hierarchical_decision.meta_strategy,
                        hierarchical_decision.cognitive_adaptation,
                        f"{hierarchical_decision.combination_choice[0]}+{hierarchical_decision.combination_choice[1]}+{hierarchical_decision.combination_choice[2]}",
                        hierarchical_decision.execution_strategy,
                        hierarchical_decision.quality_score,
                        hierarchical_decision.decision_time,
                        hierarchical_decision.confidence,
                        hierarchical_decision.tree_depth,
                        frame_data['cognitive_data']['cognitive_load_level'],
                        frame_data['cognitive_data']['mental_effort_score'],
                        frame_data['cognitive_data']['pupil_dilation_rate'],
                        # ì¶”ê°€: ë³´ìƒ ë¶„í•´
                        reward_comp.get('neurips_2023', ''),
                        reward_comp.get('ijcai_2025', ''),
                        # GPT-4 í˜‘ì—… ê´€ë ¨ (ë…¼ë¬¸ êµ¬í˜„)
                        hierarchical_decision.ids,
                        hierarchical_decision.upr,
                        hierarchical_decision.llm_feedback[:100] if hierarchical_decision.llm_feedback else '',
                        reward_comp.get('uncertainty', ''),
                        reward_comp.get('safety', ''),
                        reward_comp.get('final', ''),
                        reward_comp.get('elapsed_ms', '')
                    ]
                    
                    # ë°°ì¹˜ì— ì¶”ê°€
                    self._csv_batch.append(base_data + hierarchical_data)
                    
                    # ë°°ì¹˜ í¬ê¸° ë„ë‹¬ ì‹œ íŒŒì¼ ì“°ê¸°
                    if len(self._csv_batch) >= self._csv_batch_size:
                        try:
                            with open(self.csv_filename, mode='a', newline='') as file:
                                writer = csv.writer(file)
                                writer.writerows(self._csv_batch)
                            self._csv_batch.clear()
                        except:
                            pass
                
                # í‚¤ ì…ë ¥
                key = cv2.waitKey(1) & 0xFF
                if key == ord("q"):
                    break
                
                self.counter += 1
                self.frame_counter += 1
                if self.counter == self.evaluation_frequency:
                    self.counter = 0
                
                # ë©”ëª¨ë¦¬ ì •ë¦¬ (100í”„ë ˆì„ë§ˆë‹¤)
                if self.frame_counter % 100 == 0:
                    gc.collect()
            
        except Exception as e:
            print(f"System error: {e}")
            import traceback
            traceback.print_exc()
            
        finally:
            # ë‚¨ì€ CSV ë°°ì¹˜ ì“°ê¸°
            try:
                if hasattr(self, '_csv_batch') and self._csv_batch:
                    with open(self.csv_filename, mode='a', newline='') as file:
                        writer = csv.writer(file)
                        writer.writerows(self._csv_batch)
            except:
                pass
            
            if 'cap' in locals():
                cap.release()
            # Properly close MediaPipe resources if available
            try:
                if hasattr(self, 'face_mesh') and self.face_mesh is not None:
                    self.face_mesh.close()
            except Exception:
                pass
            # Emotion worker ì¢…ë£Œ
            try:
                if hasattr(self, '_emotion_thread_stop'):
                    self._emotion_thread_stop.set()
                if hasattr(self, '_emotion_thread') and self._emotion_thread is not None:
                    self._emotion_thread.join(timeout=1.0)
            except Exception:
                pass
            cv2.destroyAllWindows()
            
            # ğŸ“Š ìµœì¢… í†µê³„
            runtime = time.time() - self.start_time
            performance_summary = self.hierarchical_mcts.get_current_performance_summary()
            
            print(f"\nğŸ‰ HIERARCHICAL MCTS SYSTEM TERMINATED")
            print("=" * 60)
            print(f"ğŸ“Š FINAL STATISTICS:")
            print(f"   â±ï¸ Runtime: {runtime:.1f}s")
            print(f"   ğŸ¬ Frames Processed: {self.frame_counter}")
            print(f"   ğŸ§  Total Decisions: {performance_summary.get('total_decisions', 0)}")
            print(f"   ğŸ† Average Quality: {performance_summary.get('avg_quality', 0.0):.3f}")
            print(f"   âš¡ Average Response Time: {performance_summary.get('avg_time', 0.0):.3f}s")
            print(f"   ğŸ¯ Average Confidence: {performance_summary.get('avg_confidence', 0.0):.3f}")
            print(f"   ğŸ’¾ Data saved: {self.csv_filename}")
            print("=" * 60)
            print("ğŸŒŸ Thank you for testing this revolutionary AI system!")

# ==================== Main Execution ====================
def main():
    print("ğŸ”¶âœ¨ CONDITION: MULTI-ADAPTIVE (PROPOSED SYSTEM WITH GPT-4)")
    print("=" * 80)
    print("ğŸ”¶âœ¨ EXPERIMENTAL CONDITION:")
    print("   ğŸ“ Agent Type: MULTI (GPT-4 Planner, Critic, Executor)")
    print("   ğŸ“ Adaptation: ADAPTIVE (Full user adaptation)")
    print("   ğŸ“ Strategy: 4-Level Hierarchical MCTS")
    print("=" * 80)
    print("ğŸŒŸ REVOLUTIONARY FEATURES:")
    print("   ğŸ¯ 4-Level Hierarchical MCTS (Meta â†’ Cognitive â†’ Combination â†’ Execution)")
    print("   ğŸ¤– GPT-4 Based Multi-Agent Collaboration")
    print("   ğŸ§  Real-time Cognitive Load Adaptation via Pupil Tracking")
    print("   ğŸ‘ï¸ Advanced MediaPipe-based Gaze & Emotion Analysis")
    print("   ğŸ­ Adaptive Agent Personality (BaseTemplate + PersonalityTag)")
    print("   ğŸ“Š GEMMAS Framework (IDS, UPR)")
    print("   ğŸ† Paper Reward Function (R* = wâ‚R_emo + wâ‚‚R_eff + wâ‚ƒR_unc + wâ‚„R_safe)")
    print("   ğŸ”¬ Research-Grade Data Collection & Analysis")
    print("=" * 80)
    
    # GPT-4 í†µí•© ì—¬ë¶€ í™•ì¸
    try:
        import os
        api_key = os.getenv("OPENAI_API_KEY")
        if api_key:
            print("âœ… OpenAI API Key detected - GPT-4 agents will be used")
            use_gpt4 = True
        else:
            print("âš ï¸  No OpenAI API Key - Running without GPT-4 agents")
            use_gpt4 = False
    except:
        use_gpt4 = False
    
    try:
        user_name = input("Enter your name: ")
    except EOFError:
        user_name = "MultiAdaptive_User"
    
    print(f"\nâœ… Running MULTI-ADAPTIVE (PROPOSED) condition")
    print(f"   User: {user_name}")
    print(f"   GPT-4 Agents: {'ENABLED' if use_gpt4 else 'DISABLED'}\n")
    
    # Multi-Adaptive ëª¨ë“œ (ì œì•ˆ ì‹œìŠ¤í…œ)
    system_mode = "proposed"
    
    try:
        system = HierarchicalMCTSIntegratedSystem(user_name, system_mode)
        
        # GPT-4 ì—ì´ì „íŠ¸ í†µí•© (ì„ íƒì )
        if use_gpt4:
            try:
                print("ğŸš€ Integrating GPT-4 Multi-Agent System...")
                from integration_wrapper import integrate_with_maca_system
                integrate_with_maca_system(system, api_key=api_key)
                system.use_llm_agents = True
                print("âœ… GPT-4 agents integrated successfully!")
            except Exception as e:
                print(f"âš ï¸  GPT-4 integration failed: {e}")
                print("   Continuing with standard MCTS only...")
                system.use_llm_agents = False
        else:
            system.use_llm_agents = False
        
        system.run()
        
    except KeyboardInterrupt:
        print("\nSystem interrupted by user")
    except Exception as e:
        print(f"\nSystem error: {e}")
        import traceback
        traceback.print_exc()
    finally:
        print("ğŸ­ Hierarchical MCTS system shutdown complete")
        print("ğŸš€ Thank you for experiencing the future of AI!")

if __name__ == "__main__":
    main()

